[
  {
    "idx": 1,
    "question": {
      "kor": "한 회사에서 300개 이상의 글로벌 웹사이트와 애플리케이션을 호스팅합니다. 이 회사는 매일 이상의 클릭스트림 30TB 데이터를 분석할 플랫폼이 필요합니다.\n솔루션 설계자는 클릭 스트림 데이터를 전송하고 처리하기 위해 무엇을 해야 합니까?",
      "eng": "A company hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day.\nWhat should a solutions architect do to transmit and process the clickstream data?"
    },
    "choices": {
      "kor": {
        "A": "데이터를 Amazon S3 버킷에 보관하고 데이터로 Amazon EMR 클러스터를 실행하여 분석을 생성하도록 AWS Data Pipeline을 설계합니다.",
        "B": "Amazon EC2 인스턴스의 Auto Scaling 그룹을 생성하여 데이터를 처리하고 Amazon Redshift가 분석에 사용할 수 있도록 Amazon S3 데이터 레이크로 보냅니다.",
        "C": "데이터를 Amazon CloudFront에 캐시합니다. 데이터를 Amazon S3 버킷에 저장합니다. 객체가 S3 버킷에 추가될 때. AWS Lambda 함수를 실행하여 분석을 위해 데이터를 처리합니다.",
        "D": "Amazon Kinesis Data Streams에서 데이터를 수집합니다. Amazon Kinesis Data Firehose를 사용하여 데이터를 Amazon S3 데이터 레이크로 전송합니다. 분석을 위해 Amazon Redshift에 데이터를 로드합니다."
      },
      "eng": {
        "A": "Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster with the data to generate analytics.",
        "B": "Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 data lake for Amazon Redshift to use for analysis.",
        "C": "Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is added to the S3 bucket. run an AWS Lambda function to process the data for analysis.",
        "D": "Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis."
      }
    },
    "category": [
      "Data Streaming"
    ],
    "subcategory": [
      "Kinesis Data Streams",
      "Kinesis Data Firehose",
      "clickstream data"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85793-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ]
  },
  {
    "idx": 2,
    "question": {
      "kor": "한 로봇 회사가 의료 수술을 위한 솔루션을 설계하고 있습니다. 로봇은 고급 센서, 카메라 및 AI 알고리즘을 사용하여 환경을 인식하고 수술을 완료합니다.\n회사에는 백엔드 서비스와의 원활한 통신을 보장할 AWS 클라우드의 공용 로드 밸런서가 필요합니다. 로드 밸런서는 쿼리 문자열을 기반으로 트래픽을 다른 대상 그룹으로 라우팅할 수 있어야 합니다. 트래픽도 암호화되어야 합니다.\n어떤 솔루션이 이러한 요구 사항을 충족합니까?",
      "eng": "A robotics company is designing a solution for medical surgery. The robots will use advanced sensors, cameras, and AI algorithms to perceive their environment and to complete surgeries.\nThe company needs a public load balancer in the AWS Cloud that will ensure seamless communication with backend services. The load balancer must be capable of routing traffic based on the query strings to different target groups. The traffic must also be encrypted.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "ACM(AWS Certificate Manager)에서 첨부된 인증서와 함께 Application Load Balancer를 사용합니다. 쿼리 매개변수 기반 라우팅을 사용합니다.",
        "B": "Network Load Balancer를 사용하십시오. AWS Identity and Access Management(IAM)에서 생성된 인증서를 가져옵니다. 인증서를 로드 밸런서에 연결합니다. 쿼리 매개변수 기반 라우팅을 사용합니다.",
        "C": "게이트웨이 로드 밸런서를 사용합니다. AWS Identity and Access Management(IAM)에서 생성된 인증서를 가져옵니다. 인증서를 로드 밸런서에 연결합니다. HTTP 경로 기반 라우팅을 사용합니다.",
        "D": "ACM(AWS Certificate Manager)에서 첨부된 인증서와 함께 Network Load Balancer를 사용하십시오. 쿼리 매개변수 기반 라우팅을 사용합니다."
      },
      "eng": {
        "A": "Use an Application Load Balancer with a certificate attached from AWS Certificate Manager (ACM). Use query parameter-based routing.",
        "B": "Use a Network Load Balancer. Import a generated certificate in AWS Identity and Access Management (IAM). Attach the certificate to the load balancer. Use query parameter-based routing.",
        "C": "Use a Gateway Load Balancer. Import a generated certificate in AWS Identity and Access Management (IAM). Attach the certificate to the load balancer. Use HTTP path-based routing.",
        "D": "Use a Network Load Balancer with a certificate attached from AWS Certificate Manager (ACM). Use query parameter-based routing."
      }
    },
    "category": [
      "Elastic Load Balancer"
    ],
    "subcategory": [
      "routing methods",
      "encryption in flight"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/136955-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 3,
    "question": {
      "kor": "솔루션 설계자는 Amazon EC2 인스턴스를 호스팅하는 VPC 네트워크를 보호해야 합니다. EC2 인스턴스는 매우 민감한 데이터를 포함하고 프라이빗 서브넷에서 실행됩니다. 회사 정책에 따라 VPC에서 실행되는 EC2 인스턴스는 타사 URL을 사용하는 소프트웨어 제품 업데이트를 위해 인터넷에서 승인된 타사 소프트웨어 리포지토리에만 액세스할 수 있습니다. 다른 인터넷 트래픽은 차단되어야 합니다.\n어떤 솔루션이 이러한 요구 사항을 충족합니까?",
      "eng": "A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet traffic must be blocked.\nWhich solution meets these requirements?"
    },
    "choices": {
      "kor": {
        "A": "아웃바운드 트래픽을 AWS 네트워크 방화벽 방화벽으로 라우팅하도록 프라이빗 서브넷의 라우팅 테이블을 업데이트합니다. 도메인 목록 규칙 그룹을 구성합니다.",
        "B": "AWS WAF 웹 ACL을 설정합니다. 소스 및 대상 IP 주소 범위 집합을 기반으로 트래픽 요청을 필터링하는 사용자 지정 규칙 집합을 만듭니다.",
        "C": "엄격한 인바운드 보안 그룹 규칙을 구현합니다. URL을 지정하여 인터넷에서 승인된 소프트웨어 리포지토리에 대한 트래픽만 허용하는 아웃바운드 규칙을 구성합니다.",
        "D": "EC2 인스턴스 앞에 Application Load Balancer(ALB)를 구성합니다. 모든 아웃바운드 트래픽을 ALB로 보냅니다. 인터넷에 대한 아웃바운드 액세스를 위해 ALB의 대상 그룹에서 URL 기반 규칙 리스너를 사용합니다."
      },
      "eng": {
        "A": "Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups.",
        "B": "Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on source and destination IP address range sets.",
        "C": "Implement strict inbound security group rules. Configure an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs.",
        "D": "Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound traffic to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet."
      }
    },
    "category": [
      "Security"
    ],
    "subcategory": [
      "VPC security",
      "Network Firewall firewall"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/99795-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 4,
    "question": {
      "kor": "한 회사는 최근 프라이빗 서브넷의 Amazon EC2에서 Linux 기반 애플리케이션 인스턴스를 시작했고 VPC의 퍼블릭 서브넷의 Amazon EC2 인스턴스에서 Linux 기반 배스천 호스트를 시작했습니다. 솔루션 설계자는 회사의 인터넷 연결을 통해 온프레미스 네트워크에서 배스천 호스트 및 애플리케이션 서버에 연결해야 합니다. 솔루션 설계자는 모든 EC2 인스턴스의 보안 그룹이 해당 액세스를 허용하는지 확인해야 합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까? (두 가지를 선택하세요.)",
      "eng": "A company recently launched Linux-based application instances on Amazon EC2 in a private subnet and launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet of a VPC. A solutions architect needs to connect from the on-premises network, through the company's internet connection, to the bastion host, and to the application servers. The solutions architect must make sure that the security groups of all the EC2 instances will allow that access.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "배스천 호스트의 현재 보안 그룹을 애플리케이션 인스턴스로부터의 인바운드 액세스만 허용하는 보안 그룹으로 교체하십시오.",
        "B": "배스천 호스트의 현재 보안 그룹을 회사 내부 IP 범위의 인바운드 액세스만 허용하는 보안 그룹으로 교체합니다.",
        "C": "배스천 호스트의 현재 보안 그룹을 회사의 외부 IP 범위로부터의 인바운드 액세스만 허용하는 보안 그룹으로 교체합니다.",
        "D": "애플리케이션 인스턴스의 현재 보안 그룹을 배스천 호스트의 사설 IP 주소에서만 인바운드 SSH 액세스를 허용하는 보안 그룹으로 바꿉니다.",
        "E": "애플리케이션 인스턴스의 현재 보안 그룹을 배스천 호스트의 퍼블릭 IP 주소에서만 인바운드 SSH 액세스를 허용하는 보안 그룹으로 바꿉니다."
      },
      "eng": {
        "A": "Replace the current security group of the bastion host with one that only allows inbound access from the application instances.",
        "B": "Replace the current security group of the bastion host with one that only allows inbound access from the internal IP range for the company.",
        "C": "Replace the current security group of the bastion host with one that only allows inbound access from the external IP range for the company.",
        "D": "Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.",
        "E": "Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host."
      }
    },
    "category": [
      "EC2"
    ],
    "subcategory": [
      "Bastion host",
      "Security group"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85613-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C",
      "D"
    ]
  },
  {
    "idx": 5,
    "question": {
      "kor": "회사에는 1,000개의 Amazon EC2 Linux 인스턴스에서 실행되는 프로덕션 워크로드가 있습니다. 워크로드는 타사 소프트웨어로 구동됩니다. 회사는 중요한 보안 취약성을 수정하기 위해 가능한 한 빨리 모든 EC2 인스턴스에서 타사 소프트웨어를 패치해야 합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by third-party software. The company needs to patch the third-party software on all EC2 instances as quickly as possible to remediate a critical security vulnerability.\nWhat should a solutions architect do to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "모든 EC2 인스턴스에 패치를 적용할 AWS Lambda 함수를 생성합니다.",
        "B": "모든 EC2 인스턴스에 패치를 적용하도록 AWS Systems Manager Patch Manager를 구성합니다.",
        "C": "모든 인스턴스에 EC2 패치를 적용하도록 AWS Systems Manager 유지 관리 기간을 예약합니다.",
        "D": "AWS Systems Manager Run Command를 사용하여 패치를 모든 EC2 인스턴스에 적용하는 사용자 지정 명령을 실행합니다."
      },
      "eng": {
        "A": "Create an AWS Lambda function to apply the patch to all EC2 instances.",
        "B": "Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.",
        "C": "Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.",
        "D": "Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances."
      }
    },
    "category": [
      "AWS Systems Manager"
    ],
    "subcategory": [
      "Patch Manager"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85026-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ]
  },
  {
    "idx": 6,
    "question": {
      "kor": "회사는 NFS를 사용하여 온프레미스 네트워크 연결 스토리지에 대용량 비디오 파일을 저장합니다. 각 비디오 파일의 크기는 1MB에서 500GB까지입니다. 총 스토리지는 70TB이며 더 이상 증가하지 않습니다. 회사는 비디오 파일을 Amazon S3로 마이그레이션하기로 결정합니다. 회사는 최소한의 네트워크 대역폭을 사용하면서 가능한 한 빨리 비디오 파일을 마이그레이션해야 합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as soon as possible while using the least possible network bandwidth.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "S3 버킷을 생성합니다. S3 버킷에 쓸 수 있는 권한이 있는 IAM 역할을 생성합니다. AWS CLI를 사용하여 모든 파일을 S3 버킷에 로컬로 복사합니다.",
        "B": "AWS Snowball Edge 작업을 생성합니다. 온프레미스에서 Snowball Edge 디바이스를 받습니다. Snowball Edge 클라이언트를 사용하여 데이터를 디바이스로 전송합니다. AWS가 데이터를 Amazon S3로 가져올 수 있도록 장치를 반환하십시오.",
        "C": "온프레미스에 S3 파일 게이트웨이를 배포합니다. S3 파일 게이트웨이에 연결할 공용 서비스 엔드포인트를 생성합니다. S3 버킷을 생성합니다. S3 File Gateway에서 새 NFS 파일 공유를 생성합니다. 새 파일 공유가 S3 버킷을 가리키도록 합니다. 기존 NFS 파일 공유에서 S3 파일 게이트웨이로 데이터를 전송합니다.",
        "D": "온프레미스 네트워크와 AWS 간에 AWS Direct Connect 연결을 설정합니다. 온프레미스에 S3 파일 게이트웨이를 배포합니다. S3 파일 게이트웨이에 연결할 퍼블릭 가상 인터페이스(VIF)를 생성합니다. S3 버킷을 생성합니다. S3 File Gateway에서 새 NFS 파일 공유를 생성합니다. 새 파일 공유가 S3 버킷을 가리키도록 합니다. 기존 NFS 파일 공유에서 S3 파일 게이트웨이로 데이터를 전송합니다."
      },
      "eng": {
        "A": "Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the AWS CLI to copy all files locally to the S3 bucket.",
        "B": "Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device so that AWS can import the data into Amazon S3.",
        "C": "Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.",
        "D": "Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3 File Gateway on premises. Create a public virtual interface (VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway."
      }
    },
    "category": [
      "Migration"
    ],
    "subcategory": [
      "Snowball",
      "Large-scale data transfer"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/84875-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ]
  },
  {
    "idx": 7,
    "question": {
      "kor": "한 회사가 AWS에서 여러 Windows 워크로드를 실행합니다. 회사 직원은 두 개의 Amazon EC2 인스턴스에서 호스팅되는 Windows 파일 공유를 사용합니다. 파일 공유는 서로 간에 데이터를 동기화하고 복제본을 유지합니다. 회사는 사용자가 현재 파일에 액세스하는 방식을 보존하는 가용성이 높고 내구성이 뛰어난 스토리지 솔루션을 원합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A company runs multiple Windows workloads on AWS. The company's employees use Windows file shares that are hosted on two Amazon EC2 instances.\nThe file shares synchronize data between themselves and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves how users currently access the files.\nWhat should a solutions architect do to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "모든 데이터를 Amazon S3로 마이그레이션합니다. 사용자가 파일에 액세스할 수 있도록 IAM 인증을 설정합니다.",
        "B": "Amazon S3 파일 게이트웨이를 설정합니다. 기존 EC2 인스턴스에 S3 File Gateway를 탑재합니다.",
        "C": "다중 AZ 구성을 사용하여 파일 공유 환경을 Windows 파일 서버용 Amazon FSx로 확장합니다. 모든 데이터를 FSx for Windows File Server로 마이그레이션합니다.",
        "D": "다중 AZ 구성을 사용하여 파일 공유 환경을 Amazon Elastic File System(Amazon EFS)으로 확장합니다. 모든 데이터를 Amazon EFS로 마이그레이션합니다."
      },
      "eng": {
        "A": "Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.",
        "B": "Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 instances.",
        "C": "Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.",
        "D": "Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS."
      }
    },
    "category": [
      "File Storage"
    ],
    "subcategory": [
      "FSx for Windows File Server",
      "Windows file sharing"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85574-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 8,
    "question": {
      "kor": "회사는 최근 청구서에서 Amazon EC2 비용이 증가했음을 확인했습니다. 청구 팀은 몇 가지 EC2 인스턴스에 대한 인스턴스 유형의 원치 않는 수직 확장을 발견했습니다. 솔루션 아키텍트는 최근 2개월간의 EC2 비용을 비교하는 그래프를 생성하고 심층 분석을 수행하여 수직 확장의 근본 원인을 식별해야 합니다.\n솔루션 설계자는 최소한의 운영 오버헤드로 정보를 어떻게 생성해야 합니까?",
      "eng": "A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team notices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the vertical scaling.\nHow should the solutions architect generate the information with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "AWS 예산을 사용하여 예산 보고서를 생성하고 인스턴스 유형에 따라 EC2 비용을 비교합니다.",
        "B": "비용 탐색기의 세분화된 필터링 기능을 사용하여 인스턴스 유형에 따라 EC2 비용을 심층 분석합니다.",
        "C": "AWS Billing and Cost Management 대시보드의 그래프를 사용하여 지난 2개월 동안 인스턴스 유형을 기준으로 EC2 비용을 비교합니다.",
        "D": "AWS 비용 및 사용 보고서를 사용하여 보고서를 생성하고 Amazon S3 버킷으로 보냅니다. Amazon S3와 함께 Amazon QuickSight를 소스로 사용하여 인스턴스 유형을 기반으로 대화형 그래프를 생성합니다."
      },
      "eng": {
        "A": "Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.",
        "B": "Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.",
        "C": "Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.",
        "D": "Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate an interactive graph based on instance types."
      }
    },
    "category": [
      "Cost Management"
    ],
    "subcategory": [
      "Cost Explorer"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85038-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ]
  },
  {
    "idx": 9,
    "question": {
      "kor": "회사는 1주일 동안 지속되는 예정된 이벤트를 위해 특정 AWS 리전에 있는 3개의 특정 가용 영역에서 보장된 Amazon EC2 용량이 필요합니다.\n회사는 EC2 용량을 보장하기 위해 무엇을 해야 합니까?",
      "eng": "A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region for an upcoming event that will last 1 week.\nWhat should the company do to guarantee the EC2 capacity?"
    },
    "choices": {
      "kor": {
        "A": "필요한 리전을 지정하는 예약 인스턴스를 구매합니다.",
        "B": "필요한 리전을 지정하는 온디맨드 용량 예약을 생성합니다.",
        "C": "필요한 리전과 3개의 가용 영역을 지정하는 예약 인스턴스를 구입합니다.",
        "D": "필요한 리전 및 세 개의 가용 영역을 지정하는 온디맨드 용량 예약을 생성합니다."
      },
      "eng": {
        "A": "Purchase Reserved Instances that specify the Region needed.",
        "B": "Create an On-Demand Capacity Reservation that specifies the Region needed.",
        "C": "Purchase Reserved Instances that specify the Region and three Availability Zones needed.",
        "D": "Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed."
      }
    },
    "category": [
      "EC2"
    ],
    "subcategory": [
      "On-Demand Capacity Reservation"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85529-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ]
  },
  {
    "idx": 10,
    "question": {
      "kor": "회사는 Amazon DynamoDB를 사용하여 고객 정보를 저장하는 쇼핑 애플리케이션을 실행합니다. 데이터 손상의 경우 솔루션 설계자는 15분의 RPO(복구 지점 목표)와 1시간의 RTO(복구 시간 목표)를 충족하는 솔루션을 설계해야 합니다.\n이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 권장해야 합니까?",
      "eng": "A company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour.\nWhat should the solutions architect recommend to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "DynamoDB 전역 테이블을 구성합니다. RPO 복구의 경우 애플리케이션이 다른 AWS 리전을 가리키도록 합니다.",
        "B": "DynamoDB 특정 시점으로 복구를 구성합니다. RPO 복구를 위해 원하는 시점으로 복원합니다.",
        "C": "매일 DynamoDB 데이터를 Amazon S3 Glacier로 내보냅니다. RPO 복구를 위해 S3 Glacier에서 DynamoDB로 데이터를 가져옵니다.",
        "D": "15분마다 DynamoDB 테이블에 대한 Amazon Elastic Block Store(Amazon EBS) 스냅샷을 예약합니다. RPO 복구의 경우 EBS 스냅샷을 사용하여 DynamoDB 테이블을 복원합니다."
      },
      "eng": {
        "A": "Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS Region.",
        "B": "Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.",
        "C": "Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data from S3 Glacier to DynamoDB.",
        "D": "Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table by using the EBS snapshot."
      }
    },
    "category": [
      "DynamoDB"
    ],
    "subcategory": [
      "recovery point objective",
      "recovery time objective"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85603-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ]
  },
  {
    "idx": 11,
    "question": {
      "kor": "회사에는 중요한 데이터가 포함된 Amazon S3 버킷이 있습니다. 회사는 우발적인 삭제로부터 데이터를 보호해야 합니다.\n이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 단계 조합을 수행해야 합니까? (두 가지를 선택하세요.)",
      "eng": "A company has an Amazon S3 bucket that contains critical data. The company must protect the data from accidental deletion.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "S3 버킷에서 버전 관리를 활성화합니다.",
        "B": "S3 버킷에서 MFA 삭제를 활성화합니다.",
        "C": "S3 버킷에 버킷 정책을 생성합니다.",
        "D": "S3 버킷에서 기본 암호화를 활성화합니다.",
        "E": "S3 버킷의 객체에 대한 수명 주기 정책을 생성합니다."
      },
      "eng": {
        "A": "Enable versioning on the S3 bucket.",
        "B": "Enable MFA Delete on the S3 bucket.",
        "C": "Create a bucket policy on the S3 bucket.",
        "D": "Enable default encryption on the S3 bucket.",
        "E": "Create a lifecycle policy for the objects in the S3 bucket."
      }
    },
    "category": [
      "S3"
    ],
    "subcategory": [
      "MFA Delete",
      "Versioning"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/84750-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A",
      "B"
    ]
  },
  {
    "idx": 12,
    "question": {
      "kor": "회사는 Amazon S3에 데이터를 저장하고 데이터가 변경되지 않도록 해야 합니다. 회사는 Amazon S3에 업로드된 새 객체가 회사에서 객체를 수정하기로 결정할 때까지 불특정한 시간 동안 변경 불가능한 상태로 유지되기를 원합니다. 회사 AWS 계정의 특정 사용자만 개체를 삭제할 수 있습니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A company needs to store data in Amazon S3 and must prevent the data from being changed. The company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company decides to modify the objects. Only specific users in the company's AWS account can have the ability 10 delete the objects.\nWhat should a solutions architect do to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "S3 Glacier 볼트를 생성합니다. 개체에 WORM(Write-Once, Read-Many) 볼트 잠금 정책을 적용합니다.",
        "B": "S3 객체 잠금이 활성화된 S3 버킷을 생성합니다. 버전 관리를 활성화합니다. 보존 기간을 100년으로 설정합니다. 거버넌스 모드를 새 객체에 대한 S3 버킷의 기본 보관 모드로 사용합니다.",
        "C": "S3 버킷을 생성합니다. AWS CloudTrail을 사용하여 객체를 수정하는 모든 S3 API 이벤트를 추적합니다. 알림을 받으면 회사가 가지고 있는 모든 백업 버전에서 수정된 개체를 복원합니다.",
        "D": "S3 객체 잠금이 활성화된 S3 버킷을 생성합니다. 버전 관리를 활성화합니다. 객체에 법적 보존을 추가합니다. 객체를 삭제해야 하는 사용자의 IAM 정책에 s3:PutObjectLegalHold 권",
        "E": "한을 추가합니다."
      },
      "eng": {
        "A": "Create an S3 Glacier vault. Apply a write-once, read-many (WORM) vault lock policy to the objects.",
        "B": "Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 years. Use governance mode as the S3 bucket’s default retention mode for new objects.",
        "C": "Create an S3 bucket. Use AWS CloudTrail to track any S3 API events that modify the objects. Upon notification, restore the modified objects from any backup versions that the company has.",
        "D": "Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Add a legal hold to the objects. Add the s3:PutObjectLegalHold permission to the IAM policies of users who need to delete the objects."
      }
    },
    "category": [
      "S3"
    ],
    "subcategory": [
      "Object Lock",
      "legal hold",
      "Glacier vault"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85634-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ]
  },
  {
    "idx": 13,
    "question": {
      "kor": "회사는 Amazon S3에서 정적 웹 사이트를 호스팅하고 DNS에 Amazon Route 53을 사용하고 있습니다. 웹 사이트는 전 세계적으로 수요가 증가하고 있습니다. 회사는 웹사이트에 접속하는 사용자의 대기 시간을 줄여야 합니다.\n이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?",
      "eng": "A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. The website is experiencing increased demand from around the world. The company must decrease latency for users who access the website.\nWhich solution meets these requirements MOST cost-effectively?"
    },
    "choices": {
      "kor": {
        "A": "웹사이트가 포함된 S3 버킷을 모든 AWS 리전에 복제합니다. Route 53 지리적 위치 라우팅 항목을 추가합니다.",
        "B": "AWS Global Accelerator에서 액셀러레이터를 프로비저닝합니다. 제공된 IP 주소를 S3 버킷과 연결합니다. 가속기의 IP 주소를 가리키도록 Route 53 항목을 편집합니다.",
        "C": "S3 버킷 앞에 Amazon CloudFront 배포를 추가합니다. CloudFront 배포를 가리키도록 Route 53 항목을 편집합니다.",
        "D": "버킷에서 S3 Transfer Acceleration을 활성화합니다. 새 엔드포인트를 가리키도록 Route 53 항목을 편집합니다."
      },
      "eng": {
        "A": "Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation routing entries.",
        "B": "Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3 bucket. Edit the Route 53 entries to point to the IP addresses of the accelerators.",
        "C": "Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to the CloudFront distribution.",
        "D": "Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85238-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 14,
    "question": {
      "kor": "회사는 Amazon API Gateway API에 의해 호출되는 AWS Lambda 함수에서 애플리케이션을 호스팅합니다. Lambda 함수는 고객 데이터를 Amazon Aurora MySQL 데이터베이스에 저장합니다. 회사에서 데이터베이스를 업그레이드할 때마다 Lambda 함수는 업그레이드가 완료될 때까지 데이터베이스 연결을 설정하지 못합니다. 그 결과 일부 이벤트에 대해 고객 데이터가 기록되지 않습니다.\n솔루션 설계자는 데이터베이스 업그레이드 중에 생성된 고객 데이터를 저장하는 솔루션을 설계해야 합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company hosts an application on AWS Lambda functions that are invoked by an Amazon API Gateway API. The Lambda functions save customer data to an Amazon Aurora MySQL database. Whenever the company upgrades the database, the Lambda functions fail to establish database connections until the upgrade is complete. The result is that customer data is not recorded for some of the event.\nA solutions architect needs to design a solution that stores customer data that is created during database upgrades.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Lambda 함수와 데이터베이스 사이에 위치하도록 Amazon RDS 프록시를 프로비저닝합니다. RDS 프록시에 연결하도록 Lambda 함수를 구성합니다.",
        "B": "Lambda 함수의 실행 시간을 최대로 늘립니다. 데이터베이스에 고객 데이터를 저장하는 코드에서 재시도 메커니즘을 만듭니다.",
        "C": "고객 데이터를 Lambda 로컬 스토리지에 유지합니다. 고객 데이터를 데이터베이스에 저장하기 위해 로컬 스토리지를 스캔하도록 새로운 Lambda 함수를 구성합니다.",
        "D": "고객 데이터를 Amazon Simple Queue Service(Amazon SQS) FIFO 대기열에 저장합니다. 대기열을 폴링하고 고객 데이터를 데이터베이스에 저장하는 새 Lambda 함수를 생성합니다."
      },
      "eng": {
        "A": "Provision an Amazon RDS proxy to sit between the Lambda functions and the database. Configure the Lambda functions to connect to the RDS proxy.",
        "B": "Increase the run time of the Lambda functions to the maximum. Create a retry mechanism in the code that stores the customer data in the database.",
        "C": "Persist the customer data to Lambda local storage. Configure new Lambda functions to scan the local storage to save the customer data to the database.",
        "D": "Store the customer data in an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Create a new Lambda function that polls the queue and stores the customer data in the database."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85319-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ]
  },
  {
    "idx": 15,
    "question": {
      "kor": "회사에 수신 메시지를 수집하는 애플리케이션이 있습니다. 그러면 수십 개의 다른 애플리케이션과 마이크로서비스가 이러한 메시지를 빠르게 사용합니다. 메시지 수는 매우 다양하며 때로는 초당 100,000개로 갑자기 증가하기도 합니다. 회사는 솔루션을 분리하고 확장성을 높이고자 합니다.\n어떤 솔루션이 이러한 요구 사항을 충족합니까?",
      "eng": "A company has an application that ingests incoming messages. Dozens of other applications and microservices then quickly consume these messages. The number of messages varies drastically and sometimes increases suddenly to 100,000 each second. The company wants to decouple the solution and increase scalability.\nWhich solution meets these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis Data Analytics에 대한 메시지를 유지합니다. 메시지를 읽고 처리하도록 소비자 애플리케이션을 구성합니다.",
        "B": "Auto Scaling 그룹의 Amazon EC2 인스턴스에 수집 애플리케이션을 배포하여 CPU 지표를 기반으로 EC2 인스턴스 수를 확장합니다.",
        "C": "단일 샤드로 Amazon Kinesis Data Streams에 메시지를 씁니다. AWS Lambda 함수를 사용하여 메시지를 사전 처리하고 Amazon DynamoDB에 저장합니다. 메시지를 처리하기 위해 DynamoDB에서 읽을 소비자 애플리케이션을 구성합니다.",
        "D": "여러 Amazon Simple Queue Service(Amazon SQS) 구독이 있는 Amazon Simple Notification Service(Amazon SNS) 주제에 메시지를 게시합니다. 대기열의 메시지를 처리하도록 소비자 애플리케이션을 구성합니다."
      },
      "eng": {
        "A": "Persist the messages to Amazon Kinesis Data Analytics. Configure the consumer applications to read and process the messages.",
        "B": "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale the number of EC2 instances based on CPU metrics.",
        "C": "Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to preprocess messages and store them in Amazon DynamoDB. Configure the consumer applications to read from DynamoDB to process the messages.",
        "D": "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SQS) subscriptions. Configure the consumer applications to process the messages from the queues."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/84721-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ]
  },
  {
    "idx": 16,
    "question": {
      "kor": "개발 팀은 성능 개선 도우미가 활성화된 MySQL DB 인스턴스용 범용 Amazon RDS에서 매월 리소스 집약적인 테스트를 실행합니다. 테스트는 한 달에 한 번 48시간 동안 진행되며 데이터베이스를 사용하는 유일한 프로세스입니다. 팀은 DB 인스턴스의 컴퓨팅 및 메모리 속성을 줄이지 않고 테스트 실행 비용을 줄이려고 합니다.\n이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?",
      "eng": "A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled.\nThe testing lasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and memory attributes of the DB instance.\nWhich solution meets these requirements MOST cost-effectively?"
    },
    "choices": {
      "kor": {
        "A": "테스트가 완료되면 DB 인스턴스를 중지합니다. 필요한 경우 DB 인스턴스를 다시 시작합니다.",
        "B": "DB 인스턴스와 함께 Auto Scaling 정책을 사용하여 테스트가 완료되면 자동으로 조정합니다.",
        "C": "테스트가 완료되면 스냅샷을 생성합니다. 필요한 경우 DB 인스턴스를 종료하고 스냅샷을 복원합니다.",
        "D": "테스트가 완료되면 DB 인스턴스를 저용량 인스턴스로 수정한다. 필요한 경우 DB 인스턴스를 다시 수정합니다."
      },
      "eng": {
        "A": "Stop the DB instance when tests are completed. Restart the DB instance when required.",
        "B": "Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.",
        "C": "Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.",
        "D": "Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again when required."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85030-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 17,
    "question": {
      "kor": "회사에서 새 애플리케이션을 출시하고 Amazon CloudWatch 대시보드에 애플리케이션 지표를 표시합니다. 회사의 제품 관리자는 이 대시보드에 정기적으로 액세스해야 합니다. 제품 관리자에게 AWS 계정이 없습니다. 솔루션 설계자는 최소 권한 원칙에 따라 제품 관리자에게 액세스 권한을 제공해야 합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company is launching a new application and will display application metrics on an Amazon CloudWatch dashboard. The company's product manager needs to access this dashboard periodically. The product manager does not have an AWS account. A solutions architect must provide access to the product manager by following the principle of least privilege.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "CloudWatch 콘솔에서 대시보드를 공유합니다. 제품 관리자의 이메일 주소를 입력하고 공유 단계를 완료합니다. 제품 관리자에게 대시보드에 대한 공유 가능한 링크를 제공합니다.",
        "B": "제품 관리자 전용 IAM 사용자를 생성합니다. CloudWatchReadOnlyAccess AWS 관리형 정책을 사용자에게 연결합니다. 새 로그인 자격 증명을 제품 관리자와 공유하십시오. 올바른 대시보드의 브라우저 URL을 제품 관리자와 공유하십시오.",
        "C": "회사 직원을 위한 IAM 사용자를 생성합니다. ViewOnlyAccess AWS 관리형 정책을 IAM 사용자에게 연결합니다. 새 로그인 자격 증명을 제품 관리자와 공유하십시오. 제품 관리자에게 CloudWatch 콘솔로 이동하여 대시보드 섹션에서 이름으로 대시보드를 찾도록 요청하십시오.",
        "D": "퍼블릭 서브넷에 배스천 서버를 배포합니다. 제품 관리자가 대시보드에 액세스해야 하는 경우 서버를 시작하고 RDP 자격 증명을 공유합니다. 배스천 서버에서 브라우저가 대시보드를 볼 수 있는 적절한 권한이 있는 캐시된 AWS 자격 증명으로 대시보드 URL을 열도록 구성되어 있는지 확인합니다."
      },
      "eng": {
        "A": "Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.",
        "B": "Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the product manager. Share the browser URL of the correct dashboard with the product manager.",
        "C": "Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the IAM user. Share the new login credentials with the product manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section.",
        "D": "Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85227-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 18,
    "question": {
      "kor": "한 회사가 최근 애플리케이션 마이그레이션 이니셔티브를 지원하기 위해 AWS MSP(Managed Service Provider) 파트너와 계약을 체결했습니다. 솔루션 설계자는 기존 AWS 계정의 Amazon Machine Image(AMI)를 MSP 파트너의 AWS 계정과 공유해야 합니다. AMI는 Amazon Elastic Block Store(Amazon EBS)에서 지원하며 AWS Key Management Service(AWS KMS) 고객 관리형 키를 사용하여 EBS 볼륨 스냅샷을 암호화합니다.\n솔루션 설계자가 AMI를 MSP 파트너의 AWS 계정과 공유할 수 있는 가장 안전한 방법은 무엇입니까?",
      "eng": "A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an application migration initiative. A solutions architect needs ta share an Amazon Machine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The AMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt EBS volume snapshots.\nWhat is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's AWS account?"
    },
    "choices": {
      "kor": {
        "A": "암호화된 AMI와 스냅샷을 공개적으로 사용 가능하게 만드십시오. MSP 파트너의 AWS 계정이 키를 사용할 수 있도록 키 정책을 수정합니다.",
        "B": "AMI의 launchPermission 속성을 수정합니다. MSP 파트너의 AWS 계정과만 AMI를 공유합니다. MSP 파트너의 AWS 계정이 키를 사용할 수 있도록 키 정책을 수정합니다.",
        "C": "AMI의 launchPermission 속성을 수정합니다. MSP 파트너의 AWS 계정과만 AMI를 공유합니다. 암호화를 위해 MSP 파트너가 소유한 새 KMS 키를 신뢰하도록 키 정책을 수정합니다.",
        "D": "소스 계정에서 MSP 파트너의 AWS 계정에 있는 Amazon S3 버킷으로 AMI를 내보내고 MSP 파트너가 소유한 새 KMS 키로 S3 버킷을 암호화합니다. MSP 파트너의 AWS 계정에서 AMI를 복사하고 시작합니다."
      },
      "eng": {
        "A": "Make the encrypted AMI and snapshots publicly available. Modify the key policy to allow the MSP Partner's AWS account to use the key.",
        "B": "Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to allow the MSP Partner's AWS account to use the key.",
        "C": "Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to trust a new KMS key that is owned by the MSP Partner for encryption.",
        "D": "Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS account, Encrypt the S3 bucket with a new KMS key that is owned by the MSP Partner. Copy and launch the AMI in the MSP Partner's AWS account."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85606-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ]
  },
  {
    "idx": 19,
    "question": {
      "kor": "회사는 웹 사이트에서 항목의 검색 가능한 저장소를 유지 관리합니다. 데이터는 1,000만 개 이상의 행을 포함하는 Amazon RDS for MySQL 데이터베이스 테이블에 저장됩니다. 데이터베이스에는 2TB의 범용 SSD 스토리지가 있습니다. 회사 웹 사이트를 통해 매일 이 데이터에 대한 수백만 건의 업데이트가 있습니다.\n회사는 일부 삽입 작업이 10초 이상 걸리는 것을 발견했습니다. 회사는 데이터베이스 스토리지 성능이 문제라고 판단했습니다.\n이 성능 문제를 해결하는 솔루션은 무엇입니까?",
      "eng": "A company maintains a searchable repository of items on its website. The data is stored in an Amazon RDS for MySQL database table that contains more than 10 million rows. The database has 2 TB of General Purpose SSD storage. There are millions of updates against this data every day through the company's website.\nThe company has noticed that some insert operations are taking 10 seconds or longer. The company has determined that the database storage performance is the problem.\nWhich solution addresses this performance issue?"
    },
    "choices": {
      "kor": {
        "A": "스토리지 유형을 프로비저닝된 IOPS SSD로 변경합니다.",
        "B": "DB 인스턴스를 메모리 최적화 인스턴스 클래스로 변경합니다.",
        "C": "DB 인스턴스를 버스트 가능한 성능 인스턴스 클래스로 변경합니다.",
        "D": "MySQL 기본 비동기 복제로 다중 AZ RDS 읽기 복제본을 활성화합니다."
      },
      "eng": {
        "A": "Change the storage type to Provisioned IOPS SSD.",
        "B": "Change the DB instance to a memory optimized instance class.",
        "C": "Change the DB instance to a burstable performance instance class.",
        "D": "Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/84748-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 20,
    "question": {
      "kor": "회사에서 UDP 연결을 사용하는 VoIP(Voice over Internet Protocol) 서비스를 제공합니다. 이 서비스는 Auto Scaling 그룹에서 실행되는 Amazon EC2 인스턴스로 구성됩니다. 이 회사는 여러 AWS 지역에 배포했습니다.\n회사는 지연 시간이 가장 짧은 리전으로 사용자를 라우팅해야 합니다. 회사는 또한 리전 간에 자동화된 장애 조치가 필요합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service consists of Amazon EC2 instances that run in an Auto Scaling group. The company has deployments across multiple AWS Regions.\nThe company needs to route users to the Region with the lowest latency. The company also needs automated failover between Regions.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "NLB(Network Load Balancer) 및 연결된 대상 그룹을 배포합니다. 대상 그룹을 Auto Scaling 그룹과 연결합니다. 각 리전에서 NLB를 AWS Global Accelerator 엔드포인트로 사용합니다.",
        "B": "ALB(Application Load Balancer) 및 연결된 대상 그룹을 배포합니다. 대상 그룹을 Auto Scaling 그룹과 연결합니다. 각 리전에서 ALB를 AWS Global Accelerator 엔드포인트로 사용합니다.",
        "C": "NLB(Network Load Balancer) 및 연결된 대상 그룹을 배포합니다. 대상 그룹을 Auto Scaling 그룹과 연결합니다. 각 NLB의 별칭을 가리키는 Amazon Route 53 지연 시간 레코드를 생성합니다. 지연 시간 레코드를 오리진으로 사용하는 Amazon CloudFront 배포를 생성합니다.",
        "D": "ALB(Application Load Balancer) 및 연결된 대상 그룹을 배포합니다. 대상 그룹을 Auto Scaling 그룹과 연결합니다. 각 ALB의 별칭을 가리키는 Amazon Route 53 가중치 레코드를 생성합니다. 가중 레코드를 오리진으로 사용하는 Amazon CloudFront 배포를 배포합니다."
      },
      "eng": {
        "A": "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region.",
        "B": "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Use the ALB as an AWS Global Accelerator endpoint in each Region.",
        "C": "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB. Create an Amazon CloudFront distribution that uses the latency record as an origin.",
        "D": "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each ALB. Deploy an Amazon CloudFront distribution that uses the weighted record as an origin."
      }
    },
    "category": [
      "Elastic Load Balancer",
      "Global Accelerator"
    ],
    "subcategory": [
      "protocol",
      "multi region routing",
      "latency based routing",
      "automatic failover"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85029-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 21,
    "question": {
      "kor": "회사에는 매일 총 1TB의 상태 알림을 생성하는 수천 개의 에지 장치가 있습니다. 각 알림의 크기는 약 2KB입니다. 솔루션 설계자는 향후 분석을 위해 경고를 수집하고 저장하는 솔루션을 구현해야 합니다.\n회사는 고가용성 솔루션을 원합니다. 그러나 회사는 비용을 최소화해야 하며 추가 인프라를 관리하기를 원하지 않습니다. 또한 회사는 즉각적인 분석을 위해 14일간의 데이터를 유지하고 14일 보다 오래된 모든 데이터를 보관하기를 원합니다.\n이러한 요구 사항을 충족하는 운영상 가장 효율적인 솔루션은 무엇입니까?",
      "eng": "A company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis.\nThe company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure.\nAdditionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days.\nWhat is the MOST operationally efficient solution that meets these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis Data Firehose 전송 스트림을 생성하여 알림을 수집합니다. Amazon S3 버킷에 알림을 전달하도록 Kinesis Data Firehose 스트림을 구성합니다. 14일 후에 데이터를 Amazon S3 Glacier로 전환하도록 S3 수명 주기 구성을 설정합니다.",
        "B": "두 가용 영역에서 Amazon EC2 인스턴스를 시작하고 이를 Elastic Load Balancer 뒤에 배치하여 알림을 수집합니다. Amazon S3 버킷에 알림을 저장할 EC2 인스턴스에서 스크립트를 생성합니다. 14일 후에 데이터를 Amazon S3 Glacier로 전환하도록 S3 수명 주기 구성을 설정합니다.",
        "C": "Amazon Kinesis Data Firehose 전송 스트림을 생성하여 알림을 수집합니다. Amazon OpenSearch Service(Amazon Elasticsearch Service) 클러스터에 알림을 전달하도록 Kinesis Data Firehose 스트림을 구성합니다. 매일 수동 스냅샷을 생성하고 14일보다 오래된 클러스터에서 데이터를 삭제하도록 Amazon OpenSearch Service(Amazon Elasticsearch Service) 클러스터를 설정합니다.",
        "D": "알림을 수집할 Amazon Simple Queue Service(Amazon SQS) 표준 대기열을 생성하고 메시지 보존 기간을 14일로 설정합니다. 소비자가 SQS 대기열을 폴링하고 메시지 수명을 확인하고 필요에 따라 메시지 데이터를 분석하도록 구성합니다. 메시지가 14일이 지난 경우 소비자는 메시지를 Amazon S3 버킷에 복사하고 SQS 대기열에서 메시지를 삭제해야 합니다."
      },
      "eng": {
        "A": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.",
        "B": "Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.",
        "C": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.",
        "D": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts, and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85204-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 23,
    "question": {
      "kor": "회사의 동적 웹 사이트는 미국의 온프레미스 서버를 사용하여 호스팅됩니다. 이 회사는 유럽에서 제품을 출시하고 있으며 새로운 유럽 사용자를 위해 사이트 로드 시간을 최적화하려고 합니다.\n사이트의 백엔드는 미국에 남아 있어야 합니다. 이 제품은 며칠 안에 출시되며 즉각적인 솔루션이 필요합니다.\n솔루션 설계자는 무엇을 추천해야 합니까?",
      "eng": "A company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed.\nWhat should the solutions architect recommend?"
    },
    "choices": {
      "kor": {
        "A": "us-east-1에서 Amazon EC2 인스턴스를 시작하고 사이트를 여기로 마이그레이션합니다.",
        "B": "웹사이트를 Amazon S3로 이동합니다. 리전 간 교차 리전 복제를 사용합니다.",
        "C": "온프레미스 서버를 가리키는 사용자 지정 오리진과 함께 Amazon CloudFront를 사용합니다.",
        "D": "온프레미스 서버를 가리키는 Amazon Route 53 지리 근접 라우팅 정책을 사용합니다."
      },
      "eng": {
        "A": "Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.",
        "B": "Move the website to Amazon S3. Use Cross-Region Replication between Regions.",
        "C": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
        "D": "Use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers."
      }
    },
    "category": [
      "CloudFront"
    ],
    "subcategory": [
      "custom origin",
      "hybrid cloud"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85902-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 24,
    "question": {
      "kor": "회사에는 VPC의 Amazon EC2 인스턴스에서 실행되는 애플리케이션이 있습니다. 애플리케이션 중 하나는 Amazon S3 API를 호출하여 객체를 저장하고 읽어야 합니다. 회사의 보안 정책은 애플리케이션에서 인터넷에 연결된 모든 트래픽을 제한합니다.\n이러한 요구 사항을 충족하고 보안을 유지하는 조치는 무엇입니까?",
      "eng": "A company has application running on Amazon EC2 instances in a VPC. One of the applications needs to call an Amazon S3 API to store and read objects.\nThe company's security policies restrict any internet-bound traffic from the applications.\nWhich action will fulfill these requirements and maintain security?"
    },
    "choices": {
      "kor": {
        "A": "S3 인터페이스 엔드포인트를 구성합니다.",
        "B": "S3 게이트웨이 엔드포인트를 구성합니다.",
        "C": "프라이빗 서브넷에 S3 버킷을 생성합니다.",
        "D": "EC2 인스턴스와 동일한 지역에 S3 버킷을 생성합니다."
      },
      "eng": {
        "A": "Configure an S3 interface endpoint.",
        "B": "Configure an S3 gateway endpoint.",
        "C": "Create an S3 bucket in a private subnet.",
        "D": "Create an S3 bucket in the same Region as the EC2 instance."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/21775-exam-aws-certified-solutions-architect-associate-saa-c02/",
    "answer": [
      "B"
    ]
  },
  {
    "idx": 25,
    "question": {
      "kor": "회사는 월 단위로 통화 기록 파일을 저장합니다 사용자는 통화 후 1년 이내에 임의로 파일에 액세스하지만 1년 후에는 드물게 파일에 액세스합니다. 이 회사는 사용자에게 1년 미만의 파일을 가능한 한 빨리 쿼리하고 검색할 수 있는 기능을 제공하여 솔루션을 최적화하려고 합니다. 이전 파일 검색 지연은 허용됩니다.\n이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?",
      "eng": "A company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call, but users access the files infrequently after 1 year. The company wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older files is acceptable.\nWhich solution will meet these requirements MOST cost-effectively?"
    },
    "choices": {
      "kor": {
        "A": "Amazon S3 Glacier Instant Retrieval에 태그가 있는 개별 파일을 저장합니다. 태그를 쿼리하여 S3 Glacier Instant Retrieval에서 파일을 검색합니다.",
        "B": "Amazon S3 Intelligent-Tiering에 개별 파일을 저장합니다. S3 수명 주기 정책을 사용하여 1년 후 파일을 S3 Glacier Flexible Retrieval로 이동합니다. Amazon Athena를 사용하여 Amazon S3에 있는 파일을 쿼리하고 검색합니다. S3 Glacier Select를 사용하여 S3 Glacier에 있는 파일을 쿼리하고 검색합니다.",
        "C": "Amazon S3 Standard 스토리지에 태그가 있는 개별 파일을 저장합니다. Amazon S3 Standard 스토리지의 각 아카이브에 대한 검색 메타데이터를 저장합니다. S3 수명 주기 정책을 사용하여 1년 후 파일을 S3 Glacier Instant Retrieval로 이동합니다. Amazon S3에서 메타데이터를 검색하여 파일을 쿼리하고 검색합니다.",
        "D": "Amazon S3 Standard 스토리지에 개별 파일을 저장합니다. S3 수명 주기 정책을 사용하여 1년 후 파일을 S3 Glacier Deep Archive로 이동합니다. Amazon RDS에 검색 메타데이터를 저장합니다. Amazon RDS에서 파일을 쿼리합니다. S3 Glacier Deep Archive에서 파일을 검색합니다."
      },
      "eng": {
        "A": "Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the files from S3 Glacier Instant Retrieval.",
        "B": "Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.",
        "C": "Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Instant Retrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.",
        "D": "Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Deep Archive after 1 year. Store search metadata in Amazon RDS. Query the files from Amazon RDS. Retrieve the files from S3 Glacier Deep Archive."
      }
    },
    "category": [
      "S3"
    ],
    "subcategory": [
      "lifecycle policies"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85211-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ]
  },
  {
    "idx": 26,
    "question": {
      "kor": "한 회사에서 사용자가 작은 파일을 Amazon S3에 업로드하는 애플리케이션을 설계하고 있습니다. 사용자가 파일을 업로드한 후 파일은 나중에 분석할 수 있도록 데이터를 변환하고 데이터를 JSON 형식으로 저장하는 일회성 간단한 처리가 필요합니다.\n각 파일은 업로드된 후 가능한 한 빨리 처리되어야 합니다. 수요는 다양할 것입니다. 어떤 날에는 사용자가 많은 수의 파일을 업로드합니다. 다른 날에는 사용자가 몇 개의 파일을 업로드하거나 파일을 전혀 업로드하지 않습니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company is designing an application where users upload small files into Amazon S3. After a user uploads a file, the file requires one-time simple processing to transform the data and save the data in JSON format for later analysis.\nEach file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users will upload a high number of files. On other days, users will upload a few files or no files.\nWhich solution meets these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "Amazon S3에서 텍스트 파일을 읽도록 Amazon EMR을 구성합니다. 처리 스크립트를 실행하여 데이터를 변환합니다. 결과 JSON 파일을 Amazon Aurora DB 클러스터에 저장합니다.",
        "B": "이벤트 알림을 Amazon Simple Queue Service(Amazon SQS) 대기열로 보내도록 Amazon S3를 구성합니다. Amazon EC2 인스턴스를 사용하여 대기열에서 읽고 데이터를 처리합니다. 결과 JSON 파일을 Amazon DynamoDB에 저장합니다.",
        "C": "이벤트 알림을 Amazon Simple Queue Service(Amazon SQS) 대기열로 보내도록 Amazon S3를 구성합니다. AWS Lambda 함수를 사용하여 대기열에서 읽고 데이터를 처리합니다. 결과 JSON 파일을 Amazon DynamoDB에 저장합니다.",
        "D": "새 파일이 업로드되면 Amazon Kinesis Data Streams로 이벤트를 보내도록 Amazon EventBridge(Amazon CloudWatch Events)를 구성합니다. AWS Lambda 함수를 사용하여 스트림에서 이벤트를 소비하고 데이터를 처리합니다. 결과 JSON 파일을 Amazon Aurora DB 클러스터에 저장합니다."
      },
      "eng": {
        "A": "Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the data. Store the resulting JSON file in an Amazon Aurora DB cluster.",
        "B": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.",
        "C": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.",
        "D": "Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new file is uploaded. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON file in an Amazon Aurora DB cluster."
      }
    },
    "category": [
      "S3"
    ],
    "subcategory": [
      "event notification",
      "random demand",
      "operational overhead"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/86676-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 27,
    "question": {
      "kor": "솔루션 아키텍트는 AWS에 배포되는 새로운 애플리케이션을 위한 클라우드 아키텍처를 설계하고 있습니다. 프로세스는 처리할 작업 수에 따라 필요에 따라 애플리케이션 노드를 추가 및 제거하는 동안 병렬로 실행되어야 합니다. 프로세서 애플리케이션은 상태 비저장입니다. 솔루션 설계자는 애플리케이션이 느슨하게 결합되고 작업 항목이 지속적으로 저장되는지 확인해야 합니다.\n솔루션 설계자는 어떤 디자인을 사용해야 합니까?",
      "eng": "A solutions architect is designing the cloud architecture for a new application being deployed on AWS. The process should run in parallel while adding and removing application nodes as needed based on the number of jobs to be processed. The processor application is stateless. The solutions architect must ensure that the application is loosely coupled and the job items are durably stored.\nWhich design should the solutions architect use?"
    },
    "choices": {
      "kor": {
        "A": "처리해야 할 작업을 보낼 Amazon SNS 주제를 생성합니다. 프로세서 애플리케이션으로 구성된 Amazon 머신 이미지(AMI)를 생성합니다. AMI를 사용하는 시작 구성을 생성합니다. 시작 구성을 사용하여 Auto Scaling 그룹을 생성합니다. CPU 사용량에 따라 노드를 추가 및 제거하도록 Auto Scaling 그룹에 대한 조정 정책을 설정합니다.",
        "B": "처리해야 하는 작업을 보관할 Amazon SQS 대기열을 생성합니다. 프로세서 애플리케이션으로 구성된 Amazon 머신 이미지(AMI)를 생성합니다. AMI를 사용하는 시작 구성을 생성합니다. 시작 구성을 사용하여 Auto Scaling 그룹을 생성합니다. 네트워크 사용량에 따라 노드를 추가 및 제거하도록 Auto Scaling 그룹에 대한 조정 정책을 설정합니다.",
        "C": "처리해야 할 작업을 보관할 Amazon SQS 대기열을 생성합니다. 프로세서 애플리케이션으로 구성된 Amazon 머신 이미지(AMI)를 생성합니다. AMI를 사용하는 시작 템플릿을 생성합니다. 시작 템플릿을 사용하여 Auto Scaling 그룹을 생성합니다. SQS 대기열의 항목 수에 따라 노드를 추가 및 제거하도록 Auto Scaling 그룹에 대한 조정 정책을 설정합니다.",
        "D": "처리해야 할 작업을 보낼 Amazon SNS 주제를 생성합니다. 프로세서 애플리케이션으로 구성된 Amazon 머신 이미지(AMI)를 생성합니다. AMI를 사용하는 시작 템플릿을 생성합니다. 시작 템플릿을 사용하여 Auto Scaling 그룹을 생성합니다. SNS 주제에 게시된 메시지 수에 따라 노드를 추가 및 제거하도록 Auto Scaling 그룹에 대한 조정 정책을 설정합니다."
      },
      "eng": {
        "A": "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage.",
        "B": "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network usage.",
        "C": "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue.",
        "D": "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [
      "scaling policy",
      "AMI"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/86621-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 28,
    "question": {
      "kor": "회사는 AWS 클라우드에서 호스팅되는 미디어 애플리케이션을 위한 공유 스토리지 솔루션을 구현하고 있습니다. 회사는 SMB 클라이언트를 사용하여 데이터에 액세스할 수 있는 기능이 필요 합니다. 솔루션은 완전히 관리되어야 합니다.\n어떤 AWS 솔루션이 이러한 요구 사항을 충족합니까?",
      "eng": "A company is implementing a shared storage solution for a media application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed.\nWhich AWS solution meets these requirements?"
    },
    "choices": {
      "kor": {
        "A": "AWS Storage Gateway 볼륨 게이트웨이를 생성합니다. 필요한 클라이언트 프로토콜을 사용하는 파일 공유를 만듭니다. 응용 프로그램 서버를 파일 공유에 연결합니다.",
        "B": "AWS Storage Gateway 테이프 게이트웨이를 생성합니다. Amazon S3를 사용하도록 테이프를 구성합니다. 애플리케이션 서버를 테이프 게이트웨이에 연결합니다.",
        "C": "Amazon EC2 Windows 인스턴스를 생성합니다. 인스턴스에 Windows 파일 공유 역할을 설치하고 구성합니다. 응용 프로그램 서버를 파일 공유에 연결합니다.",
        "D": "Windows 파일 서버 파일 시스템용 Amazon FSx를 생성합니다. 원본 서버에 파일 시스템을 연결합니다. 애플리케이션 서버를 파일 시스템에 연결하십시오."
      },
      "eng": {
        "A": "Create an AWS Storage Gateway volume gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.",
        "B": "Create an AWS Storage Gateway tape gateway. Configure tapes to use Amazon S3. Connect the application server to the tape gateway.",
        "C": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.",
        "D": "Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/95006-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ]
  },
  {
    "idx": 29,
    "question": {
      "kor": "회사에는 사용자가 웹 인터페이스 또는 모바일 앱을 통해 문서를 업로드하는 프로덕션 웹 애플리케이션이 있습니다. 새로운 규제 요구 사항에 따라. 새 문서는 저장된 후에 수정하거나 삭제할 수 없습니다.\n솔루션 설계자는 이 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A company has a production web application in which users upload documents through a web interface or a mobile app. According to a new regulatory requirement. new documents cannot be modified or deleted after they are stored.\nWhat should a solutions architect do to meet this requirement?"
    },
    "choices": {
      "kor": {
        "A": "S3 버전 관리 및 S3 객체 잠금이 활성화된 Amazon S3 버킷에 업로드된 문서를 저장합니다.",
        "B": "업로드된 문서를 Amazon S3 버킷에 저장합니다. 문서를 주기적으로 보관하도록 S3 수명 주기 정책을 구성합니다.",
        "C": "업로드된 문서를 S3 버전 관리가 활성화된 Amazon S3 버킷에 저장합니다. 모든 액세스를 읽기 전용으로 제한하도록 ACL을 구성합니다.",
        "D": "업로드된 문서를 Amazon Elastic File System(Amazon EFS) 볼륨에 저장합니다. 읽기 전용 모드로 볼륨을 마운트하여 데이터에 액세스하십시오."
      },
      "eng": {
        "A": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled.",
        "B": "Store the uploaded documents in an Amazon S3 bucket. Configure an S3 Lifecycle policy to archive the documents periodically.",
        "C": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled. Configure an ACL to restrict all access to read-only.",
        "D": "Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume. Access the data by mounting the volume in read-only mode."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85751-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 30,
    "question": {
      "kor": "솔루션 설계자는 회사의 온프레미스 인프라를 AWS로 확장하기 위해 새로운 하이브리드 아키텍처를 설계하고 있습니다. 이 회사는 AWS 리전에 대해 지속적으로 짧은 지연 시간을 갖는 고가용성 연결이 필요합니다. 회사는 비용을 최소화해야 하며 기본 연결이 실패할 경우 더 느린 트래픽을 수용할 의향이 있습니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.\nWhat should the solutions architect do to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "리전에 대한 AWS Direct Connect 연결을 프로비저닝합니다. 기본 Direct Connect 연결이 실패할 경우 VPN 연결을 백업으로 프로비저닝합니다.",
        "B": "사설 연결을 위해 지역에 대한 VPN 터널 연결을 프로비저닝합니다. 기본 VPN 연결이 실패할 경우 비공개 연결 및 백업을 위한 두 번째 VPN 터널을 프로비저닝합니다.",
        "C": "리전에 대한 AWS Direct Connect 연결을 프로비저닝합니다. 기본 Direct Connect 연결이 실패할 경우 백업과 동일한 리전에 대한 두 번째 Direct Connect 연결을 프로비저닝합니다.",
        "D": "리전에 대한 AWS Direct Connect 연결을 프로비저닝합니다. AWS CLI에서 Direct Connect 장애 조치 속성을 사용하여 기본 Direct Connect 연결이 실패할 경우 백업 연결을 자동으로 생성합니다."
      },
      "eng": {
        "A": "Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.",
        "B": "Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPN connection fails.",
        "C": "Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.",
        "D": "Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85593-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 31,
    "question": {
      "kor": "회사에서 AWS 인프라에 대한 월별 유지 관리를 수행합니다. 이러한 유지 관리 활동 중에 회사는 여러 AWS 리전에서 Amazon RDS for MySQL 데이터베이스에 대한 자격 증명을 교체해야 합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the company needs to rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "자격 증명을 AWS Secrets Manager에 비밀로 저장합니다. 필요한 리전에 대해 다중 리전 비밀 복제를 사용합니다. 일정에 따라 비밀을 교체하도록 Secrets Manager를 구성합니다.",
        "B": "보안 문자열 파라미터를 생성하여 AWS Systems Manager에 자격 증명을 비밀로 저장합니다. 필요한 리전에 대해 다중 리전 비밀 복제를 사용합니다. 일정에 따라 암호를 교체하도록 Systems Manager를 구성합니다.",
        "C": "서버 측 암호화(SSE)가 활성화된 Amazon S3 버킷에 자격 증명을 저장합니다. Amazon EventBridge(Amazon CloudWatch Events)를 사용하여 AWS Lambda 함수를 호출하여 자격 증명을 교체합니다.",
        "D": "AWS Key Management Service(AWS KMS) 다중 리전 고객 관리 키를 사용하여 자격 증명을 비밀로 암호화합니다. Amazon DynamoDB 전역 테이블에 암호를 저장합니다. AWS Lambda 함수를 사용하여 DynamoDB에서 비밀을 검색합니다. RDS API를 사용하여 암호를 교체합니다."
      },
      "eng": {
        "A": "Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the required Regions. Configure Secrets Manager to rotate the secrets on a schedule.",
        "B": "Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter. Use multi-Region secret replication for the required Regions. Configure Systems Manager to rotate the secrets on a schedule.",
        "C": "Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke an AWS Lambda function to rotate the credentials.",
        "D": "Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Store the secrets in an Amazon DynamoDB global table. Use an AWS Lambda function to retrieve the secrets from DynamoDB. Use the RDS API to rotate the secrets."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/84728-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 32,
    "question": {
      "kor": "회사에서 온프레미스 데이터 센터를 AWS로 마이그레이션하려고 합니다. 데이터 센터는 NFS 기반 파일 시스템에 데이터를 저장하는 SFTP 서버를 호스팅합니다. 서버에는 전송해야 하는 200GB의 데이터가 있습니다. 서버는 Amazon Elastic File System(Amazon EFS) 파일 시스템을 사용하는 Amazon EC2 인스턴스에서 호스팅되어야 합니다.\n이 작업을 자동화하기 위해 솔루션 설계자가 수행해야 하는 단계 조합은 무엇입니까? (두 가지를 선택하세요.)",
      "eng": "A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP server that stores its data on an NFS-based file system.\nThe server holds 200 GB of data that needs to be transferred. The server must be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS) file system.\nWhich combination of steps should a solutions architect take to automate this task? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "EFS 파일 시스템과 동일한 가용 영역에서 EC2 인스턴스를 시작합니다.",
        "B": "온프레미스 데이터 센터에 AWS DataSync 에이전트를 설치합니다.",
        "C": "데이터를 위해 EC2 인스턴스에 보조 Amazon Elastic Block Store(Amazon EBS) 볼륨을 생성합니다.",
        "D": "운영 체제 복사 명령을 수동으로 사용하여 데이터를 EC2 인스턴스로 푸시합니다.",
        "E": "AWS DataSync를 사용하여 온프레미스 SFTP 서버에 적합한 위치 구성을 생성합니다."
      },
      "eng": {
        "A": "Launch the EC2 instance into the same Availability Zone as the EFS file system.",
        "B": "Install an AWS DataSync agent in the on-premises data center.",
        "C": "Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the data.",
        "D": "Manually use an operating system copy command to push the data to the EC2 instance.",
        "E": "Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server."
      }
    },
    "category": [
      "Migration"
    ],
    "subcategory": [
      "DataSync",
      "S3"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85814-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B",
      "E"
    ]
  },
  {
    "idx": 33,
    "question": {
      "kor": "회사의 컨테이너화된 애플리케이션이 Amazon EC2 인스턴스에서 실행됩니다. 애플리케이션은 다른 비즈니스 애플리케이션과 통신하기 전에 보안 인증서를 다운로드해야 합니다. 회사는 거의 실시간으로 인증서를 암호화하고 해독할 수 있는 매우 안전한 솔루션을 원합니다. 또한 솔루션은 데이터가 암호화된 후 고가용성 스토리지에 데이터를 저장해야 합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company's containerized application runs on an Amazon EC2 instance. The application needs to download security certificates before it can communicate with other business applications. The company wants a highly secure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in highly available storage after the data is encrypted.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "암호화된 인증서에 대한 AWS Secrets Manager 비밀을 생성합니다. 필요에 따라 인증서를 수동으로 업데이트합니다. 세분화된 IAM 액세스를 사용하여 데이터에 대한 액세스를 제어 합니다.",
        "B": "Python 암호화 라이브러리를 사용하여 암호화 작업을 수신하고 수행하는 AWS Lambda 함수를 생성합니다. Amazon S3 버킷에 함수를 저장합니다.",
        "C": "AWS Key Management Service(AWS KMS) 고객 관리형 키를 생성합니다. EC2 역할이 암호화 작업에 KMS 키를 사용하도록 허용합니다. 암호화된 데이터를 Amazon S3에 저장합니다.",
        "D": "AWS Key Management Service(AWS KMS) 고객 관리형 키를 생성합니다. EC2 역할이 암호화 작업에 KMS 키를 사용하도록 허용합니다. 암호화된 데이터를 Amazon",
        "E": "Elastic Block Store(Amazon EBS) 볼륨에 저장합니다."
      },
      "eng": {
        "A": "Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as needed. Control access to the data by using finegrained IAM access.",
        "B": "Create an AWS Lambda function that uses the Python cryptography library to receive and perform encryption operations. Store the function in an Amazon S3 bucket.",
        "C": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon S3.",
        "D": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block Store (Amazon EBS) volumes."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85186-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 34,
    "question": {
      "kor": "애플리케이션 개발 팀은 큰 이미지를 더 작은 압축 이미지로 변환하는 마이크로서비스를 설계하고 있습니다 사용자가 웹 인터페이스를 통해 이미지를 업로드할 때 마이크로 서비스는 이미지를 Amazon S3 버킷에 저장하고 AWS Lambda 함수로 이미지를 처리 및 압축하고 이미지를 압축된 형태로 다른 S3 버킷에 저장해야 합니다.\n솔루션 설계자는 이미지를 자동으로 처리하기 위해 내구성 있고 상태 비저장 구성 요소를 사용하는 솔루션을 설계해야 합니다.\n이러한 요구 사항을 충족하는 작업 조합은 무엇입니까? (두 가지를 선택하세요.)",
      "eng": "An application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the web interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in its compressed form in a different S3 bucket.\nA solutions architect needs to design a solution that uses durable, stateless components to process the images automatically.\nWhich combination of actions will meet these requirements? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "Amazon Simple Queue Service(Amazon SQS) 대기열을 생성합니다. 이미지가 S3 버킷에 업로드될 때 SQS 대기열에 알림을 보내도록 S3 버킷을 구성합니다.",
        "B": "Amazon Simple Queue Service(Amazon SQS) 대기열을 호출 소스로 사용하도록 Lambda 함수를 구성합니다. SQS 메시지가 성공적으로 처리되면 대기열에서 메시지를 삭제 합니다.",
        "C": "새 업로드에 대해 S3 버킷을 모니터링하도록 Lambda 함수를 구성합니다. 업로드된 이미지가 감지되면 파일 이름을 메모리의 텍스트 파일에 쓰고 텍스트 파일을 사용하여 처리된 이미지를 추적합니다.",
        "D": "Amazon EC2 인스턴스를 시작하여 Amazon Simple Queue Service(Amazon SQS) 대기열을 모니터링합니다. 대기열에 항목이 추가되면 EC2 인스턴스의 텍스트 파일에 파일 이름을 기록하고 Lambda 함수를 호출합니다.",
        "E": "S3 버킷을 모니터링하도록 Amazon EventBridge(Amazon CloudWatch Events) 이벤트를 구성합니다. 이미지가 업로드되면 추가 처리를 위해 애플리케이션 소유자의 이메일 주소와 함께 Amazon ampl Notification Service(Amazon SNS) 주제로 알림을 보냅니다."
      },
      "eng": {
        "A": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.",
        "B": "Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.",
        "C": "Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed.",
        "D": "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.",
        "E": "Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon ample Notification Service (Amazon SNS) topic with the application owner's email address for further processing."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85033-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A",
      "B"
    ]
  },
  {
    "idx": 35,
    "question": {
      "kor": "회사는 애플리케이션에 대한 실시간 데이터 수집 아키텍처를 구성해야 합니다. 회사는 API, 데이터가 스트리밍될 때 데이터를 변환하는 프로세스, 데이터를 위한 스토리지 솔루션이 필요합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is streamed, and a storage solution for the data.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis 데이터 스트림으로 데이터를 전송하는 API를 호스팅하기 위해 Amazon EC2 인스턴스를 배포합니다. Kinesis 데이터 스트림을 데이터 소스로 사용하는 Amazon Kinesis Data Firehose 전송 스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. Kinesis Data Firehose 전송 스트림을 사용하여 데이터를 Amazon S3로 보냅니다.",
        "B": "AWS Glue로 데이터를 전송하는 API를 호스팅하기 위해 Amazon EC2 인스턴스를 배포합니다. EC2 인스턴스에서 소스/대상 확인을 중지합니다. AWS Glue를 사용하여 데이터를 변환하고 데이터를 Amazon S3로 보냅니다.",
        "C": "Amazon Kinesis 데이터 스트림으로 데이터를 전송하도록 Amazon API Gateway API를 구성합니다. Kinesis 데이터 스트림을 데이터 소스로 사용하는 Amazon Kinesis Data Firehose 전송 스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. Kinesis Data Firehose 전송 스트림을 사용하여 데이터를 Amazon S3로 보냅니다.",
        "D": "AWS Glue로 데이터를 전송하도록 Amazon API Gateway API를 구성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. AWS Glue를 사용하여 데이터를 Amazon S3로 보냅니다."
      },
      "eng": {
        "A": "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.",
        "B": "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination checking on the EC2 instance. Use AWS Glue to transform the data and to send the data to Amazon S3.",
        "C": "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.",
        "D": "Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform the data. Use AWS Glue to send the data to Amazon S3."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85740-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 36,
    "question": {
      "kor": "회사는 데이터 센터에서 SMB 파일 서버를 실행하고 있습니다. 파일 서버는 파일이 생성된 후 처음 며칠 동안 자주 액세스되는 대용량 파일을 저장합니다. 7일 후에는 파일에 거의 액세스하지 않습니다.\n총 데이터 크기는 증가하고 있으며 회사의 총 스토리지 용량에 근접합니다. 솔루션 설계자는 가장 최근에 액세스한 파일에 대한 짧은 대기 시간 액세스를 잃지 않고 회사의 사용 가능한 스토리지 공간을 늘려야 합니다. 솔루션 설계자는 향후 스토리지 문제를 방지하기 위해 파일 수명 주기 관리도 제공해야 합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed.\nThe total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "AWS DataSync를 사용하여 SMB 파일 서버에서 AWS로 7일보다 오래된 데이터를 복사합니다.",
        "B": "Amazon S3 File Gateway를 생성하여 회사의 스토리지 공간을 확장합니다. 7일 후에 데이터를 S3 Glacier Deep Archive로 전환하는 S3 수명 주기 정책을 생성합니다.",
        "C": "Amazon FSx for Windows File Server 파일 시스템을 생성하여 회사의 스토리지 공간을 확장합니다.",
        "D": "각 사용자의 컴퓨터에 유틸리티를 설치하여 Amazon S3에 액세스합니다. 7일 후에 데이터를 S3 Glacier Flexible Retrieval로 전환하는 S3 수명 주기 정책을 생성합니다."
      },
      "eng": {
        "A": "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
        "B": "Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.",
        "C": "Create an Amazon FSx for Windows File Server file system to extend the company's storage space.",
        "D": "Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/84680-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ]
  },
  {
    "idx": 37,
    "question": {
      "kor": "회사는 단일 VPC의 Amazon EC2 인스턴스에서 고가용성 이미지 처리 애플리케이션을 실행합니다. EC2 인스턴스는 여러 가용 영역에 걸쳐 여러 서브넷 내에서 실행됩니다. EC2 인스턴스는 서로 통신하지 않습니다. 그러나 EC2 인스턴스는 Amazon S3에서 이미지를 다운로드하고 단일 NAT 게이트웨이를 통해 Amazon S3에 이미지를 업로드합니다. 회사는 데이터 전송 요금에 대해 우려하고 있습니다.\n회사가 지역 데이터 전송 요금을 피하는 가장 비용 효율적인 방법은 무엇입니까?",
      "eng": "A company runs a highly available image-processing application on Amazon EC2 instances in a single VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The EC2 instances do not communicate with each other. However, the EC2 instances download images from Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The company is concerned about data transfer charges.\nWhat is the MOST cost-effective way for the company to avoid Regional data transfer charges?"
    },
    "choices": {
      "kor": {
        "A": "각 가용 영역에서 NAT 게이트웨이를 시작합니다.",
        "B": "NAT 게이트웨이를 NAT 인스턴스로 교체합니다.",
        "C": "Amazon S3용 게이트웨이 VPC 엔드포인트를 배포합니다.",
        "D": "EC2 인스턴스를 실행할 EC2 전용 호스트를 프로비저닝합니다."
      },
      "eng": {
        "A": "Launch the NAT gateway in each Availability Zone.",
        "B": "Replace the NAT gateway with a NAT instance.",
        "C": "Deploy a gateway VPC endpoint for Amazon S3.",
        "D": "Provision an EC2 Dedicated Host to run the EC2 instances."
      }
    },
    "category": [
      "VPC endpoint",
      "bucket policy"
    ],
    "subcategory": [
      "S3"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85205-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 38,
    "question": {
      "kor": "회사는 AWS에서 인프라를 실행하고 문서 관리 애플리케이션에 대해 700,000명의 등록된 사용자 기반을 가지고 있습니다. 회사는 대용량 .pdf 파일을 .jpg 이미지 파일로 변환하는 제품을 만들려고 합니다. .pdf 파일의 평균 크기는 5MB입니다. 회사는 원본 파일과 변환된 파일을 보관해야 합니다. 솔루션 설계자는 시간이 지남에 따라 빠르게 증가할 수요를 수용할 수 있는 확장\n가능한 솔루션을 설계해야 합니다.\n이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?",
      "eng": "A company runs its infrastructure on AWS and has a registered base of 700,000 users for its document management application. The company intends to create a product that converts large .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files. A solutions architect must design a scalable solution to accommodate demand that will grow rapidly over time.\nWhich solution meets these requirements MOST cost-effectively?"
    },
    "choices": {
      "kor": {
        "A": ".pdf 파일을 Amazon S3에 저장합니다. 파일을 .jpg 형식으로 변환하고 Amazon S3에 다시 저장하는 AWS Lambda 함수를 호출하도록 S3 PUT 이벤트를 구성합니다.",
        "B": ".pdf 파일을 Amazon Dynamo에 저장합니다DynamoDB Streams 기능을 사용하여 AWS Lambda 함수를 호출하여 파일을 .jpg 형식으로 변환하고 다시 DynamoDB에 저장합니다.",
        "C": "Amazon EC2 인스턴스, Amazon Elastic Block Store(Amazon EBS) 스토리지 및 Auto Scaling 그룹을 포함하는 AWS Elastic Beanstalk 애플리케이션에 .pdf 파일을 업로드합니다. EC2 인스턴스의 프로그램을 사용하여 파일을 .jpg 형식으로 변환합니다. .pdf 파일과 .jpg 파일을 EBS 스토어에 저장합니다.",
        "D": "Amazon EC2 인스턴스, Amazon Elastic File System(Amazon EFS) 스토리지 및 Auto Scaling 그룹이 포함된 AWS Elastic Beanstalk 애플리케이션에 .pdf 파일을 업로드합니다. EC2 인스턴스의 프로그램을 사용하여 파일을 .jpg 형식으로 변환합니다. .pdf 파일과 .jpg 파일을 EBS 스토어에 저장합니다."
      },
      "eng": {
        "A": "Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Amazon S3.",
        "B": "Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS Lambda function to convert the files to .jpg format and store them back in DynamoDB.",
        "C": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .jpg format. Save the .pdf files and the .jpg files in the EBS store.",
        "D": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the file to .jpg format. Save the .pdf files and the .jpg files in the EBS store."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85795-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 39,
    "question": {
      "kor": "회사는 온프레미스에서 실행되는 Windows 파일 서버에 5TB 이상의 파일 데이터를 보유하고 있습니다. 사용자와 애플리케이션은 매일 데이터와 상호 작용합니다.\n이 회사는 Windows 워크로드를 AWS로 이전하고 있습니다. 회사가 이 프로세스를 계속 진행함에 따라 회사는 최소한의 대기 시간으로 AWS 및 온프레미스 파일 스토리지에 액세스해야 합니다. 회사는 운영 오버헤드를 최소화하고 기존 파일 액세스 패턴을 크게 변경할 필요가 없는 솔루션이 필요합니다. 이 회사는 AWS에 연결하기 위해 AWS Site-to-Site VPN 연결을 사용 합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A company has more than 5 TB of file data on Windows file servers that run on premises. Users and applications interact with the data each day.\nThe company is moving its Windows workloads to AWS. As the company continues this process, the company requires access to AWS and on-premises file storage with minimum latency. The company needs a solution that minimizes operational overhead and requires no significant changes to the existing file access patterns. The company uses an AWS Site-to-Site VPN connection for connectivity to AWS.\nWhat should a solutions architect do to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "AWS에서 Windows File Server용 Amazon FSx를 배포하고 구성합니다. 온프레미스 파일 데이터를 FSx for Windows File Server로 이동합니다. AWS에서 Windows File Server용 FSx를 사용하도록 워크로드를 재구성합니다.",
        "B": "온프레미스에 Amazon S3 파일 게이트웨이를 배포하고 구성합니다. 온프레미스 파일 데이터를 S3 파일 게이트웨이로 이동합니다. S3 파일 게이트웨이를 사용하도록 온프레미스 워크로드 및 클라우드 워크로드를 재구성합니다.",
        "C": "온프레미스에 Amazon S3 파일 게이트웨이를 배포하고 구성합니다. 온프레미스 파일 데이터를 Amazon S3로 이동합니다. Amazon S3를 직접 사용하거나 S3 파일 게이트웨이를 사용하도록 워크로드를 재구성합니다. 각 워크로드의 위치에 따라 다릅니다.",
        "D": "AWS에서 Windows 파일 서버용 Amazon FSx를 배포하고 구성합니다. 온프레미스에 Amazon FSx 파일 게이트웨이를 배포하고 구성합니다. 온프레미스 파일 데이터를 FSx 파일 게이트웨이로 이동합니다. AWS에서 Windows File Server용 FSx를 사용하도록 클라우드 워크로드를 구성합니다. FSx 파일 게이트웨이를 사용하도록 온프레미스 워크로드를 구성 합니다."
      },
      "eng": {
        "A": "Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on AWS.",
        "B": "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to the S3 File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the S3 File Gateway.",
        "C": "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway. depending on each workload's location.",
        "D": "Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on premises. Move the onpremises file data to the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85173-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ]
  },
  {
    "idx": 40,
    "question": {
      "kor": "한 회사가 Amazon Route 53에 도메인 이름을 등록했습니다. 이 회사는 ca-central-1 리전의 Amazon API Gateway를 백엔드 마이크로서비스 API용 퍼블릭 인터페이스로 사용합니다. 타사 서비스는 API를 안전하게 사용합니다. 회사는 타사 서비스가 HTTPS를 사용할 수 있도록 회사의 도메인 이름과 해당 인증서로 API 게이트웨이 URL을 설계하려고 합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with the company's domain name and corresponding certificate so that the third-party services can use HTTPS.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "API Gateway에서 Name=\"Endpoint-URL\" 및 Value=\"Company Domain Name\"으로 단계 변수를 생성하여 기본 URL을 덮어씁니다. 회사 도메인 이름과 연결된 공인 인증서를 AWS Certificate Manager(ACM)로 가져옵니다.",
        "B": "회사의 도메인 이름으로 Route 53 DNS 레코드를 생성합니다. 별칭 레코드가 지역 API 게이트웨이 단계 엔드포인트를 가리킵니다. 회사 도메인 이름과 연결된 공인 인증서를 us-east-1 리전의 AWS Certificate Manager(ACM)로 가져옵니다.",
        "C": "지역 API 게이트웨이 엔드포인트를 생성합니다. API Gateway 끝점을 회사의 도메인 이름과 연결합니다. 회사 도메인 이름과 연결된 공인 인증서를 동일한 리전의 AWS Certificate Manager(ACM)로 가져옵니다. API Gateway 엔드포인트에 인증서를 연결합니다. 트래픽을 API 게이트웨이 엔드포인트로 라우팅하도록 Route 53을 구성합니다.",
        "D": "지역 API 게이트웨이 엔드포인트를 생성합니다. API Gateway 끝점을 회사의 도메인 이름과 연결합니다. 회사 도메인 이름과 연결된 공인 인증서를 us-east-1 리전의 AWS Certificate Manager(ACM)로 가져옵니다. API Gateway API에 인증서를 연결합니다. 회사의 도메인 이름으로 Route 53 DNS 레코드를 생성합니다. A 레코드가 회사의 도메인 이름을 가리킵니다."
      },
      "eng": {
        "A": "Create stage variables in API Gateway with Name=\"Endpoint-URL\" and Value=\"Company Domain Name\" to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).",
        "B": "Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.",
        "C": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.",
        "D": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85266-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 41,
    "question": {
      "kor": "회사는 AWS에서 다중 계층 애플리케이션을 호스팅합니다. 규정 준수, 거버넌스, 감사 및 보안을 위해 회사는 AWS 리소스에 대한 구성 변경을 추적하고 이러한 리소스에 대한 API 호출 기록을 기록해야 합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS resources and record a history of API calls made to these resources.\nWhat should a solutions architect do to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "AWS CloudTrail을 사용하여 구성 변경을 추적하고 AWS Config를 사용하여 API 호출을 기록합니다.",
        "B": "AWS Config를 사용하여 구성 변경을 추적하고 AWS CloudTrail을 사용하여 API 호출을 기록합니다.",
        "C": "AWS Config를 사용하여 구성 변경을 추적하고 Amazon CloudWatch를 사용하여 API 호출을 기록합니다.",
        "D": "AWS CloudTrail을 사용하여 구성 변경을 추적하고 Amazon CloudWatch를 사용하여 API 호출을 기록합니다."
      },
      "eng": {
        "A": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
        "B": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
        "C": "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.",
        "D": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85202-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ]
  },
  {
    "idx": 42,
    "question": {
      "kor": "회사는 독점 응용 프로그램의 로그 파일을 분석할 수 있는 기능이 필요합니다. 로그는 Amazon S3 버킷에 JSON 형식으로 저장됩니다. 쿼리는 간단하고 주문형으로 실행됩니다. 솔루션 설계자는 기존 아키텍처를 최소한으로 변경하여 분석을 수행해야 합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야 합니까?",
      "eng": "A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON format in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to perform the analysis with minimal changes to the existing architecture.\nWhat should the solutions architect do to meet these requirements with the LEAST amount of operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Redshift를 사용하여 모든 콘텐츠를 한 곳에 로드하고 필요에 따라 SQL 쿼리를 실행합니다.",
        "B": "Amazon CloudWatch Logs를 사용하여 로그를 저장합니다. Amazon CloudWatch 콘솔에서 필요에 따라 SQL 쿼리를 실행합니다.",
        "C": "Amazon S3와 함께 Amazon Athena를 직접 사용하여 필요에 따라 쿼리를 실행합니다.",
        "D": "AWS Glue를 사용하여 로그를 분류합니다. 필요에 따라 Amazon EMR에서 임시 Apache Spark 클러스터를 사용하여 SQL 쿼리를 실행합니다."
      },
      "eng": {
        "A": "Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.",
        "B": "Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon CloudWatch console.",
        "C": "Use Amazon Athena directly with Amazon S3 to run the queries as needed.",
        "D": "Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/84848-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 43,
    "question": {
      "kor": "회사에 매장에 마케팅 서비스를 제공하는 애플리케이션이 있습니다. 서비스는 매장 고객의 이전 구매를 기반으로 합니다. 상점은 SFTP를 통해 거래 데이터를 회사에 업로드하고 데이터를 처리 및 분석하여 새로운 마케팅 제안을 생성합니다. 일부 파일은 크기가 200GB를 초과할 수 있습니다.\n최근 회사는 일부 매장에서 포함되어서는 안 되는 개인 식별 정보(PII)가 포함된 파일을 업로드한 것을 발견했습니다. 회사는 PII가 다시 공유될 경우 관리자에게 알림을 받기를 원합니다. 회사는 또한 문제 해결을 자동화하기를 원합니다.\n최소한의 개발 노력으로 이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 해야 합니까?",
      "eng": "A company has an application that provides marketing services to stores. The services are based on previous purchases by store customers. The stores upload transaction data to the company through SFTP, and the data is processed and analyzed to generate new marketing offers. Some of the files can exceed 200 GB in size.\nRecently, the company discovered that some of the stores have uploaded files that contain personally identifiable information (PII) that should not have been included. The company wants administrators to be alerted if PII is shared again. The company also wants to automate remediation.\nWhat should a solutions architect do to meet these requirements with the LEAST development effort?"
    },
    "choices": {
      "kor": {
        "A": "Amazon S3 버킷을 안전한 전송 지점으로 사용하십시오. Amazon Inspector를 사용하여 버킷의 객체를 스캔합니다. 개체에 PII가 포함된 경우 S3 수명 주기 정책을 트리거하여 PII가 포함된 개체를 제거합니다.",
        "B": "Amazon S3 버킷을 안전한 전송 지점으로 사용하십시오. Amazon Macie를 사용하여 버킷의 객체를 스캔합니다. 객체에 PII가 포함된 경우 Amazon Simple Notification Service(Amazon SNS)를 사용하여 관리자에게 PII가 포함된 객체를 제거하라는 알림을 트리거합니다.",
        "C": "AWS Lambda 함수에서 사용자 지정 검색 알고리즘을 구현합니다. 객체가 버킷에 로드될 때 함수를 트리거합니다. 객체에 PII가 포함된 경우 Amazon Simple Notification Service(Amazon SNS)를 사용하여 관리자에게 PII가 포함된 객체를 제거하라는 알림을 트리거합니다.",
        "D": "AWS Lambda 함수에서 사용자 지정 검색 알고리즘을 구현합니다. 객체가 버킷에 로드될 때 함수를 트리거합니다. 객체에 PII가 포함된 경우 Amazon Simple Email Service(Amazon SES)를 사용하여 관리자에게 알림을 트리거하고 S3 수명 주기 정책을 트리거하여 PII가 포함된 미트를 제거합니다."
      },
      "eng": {
        "A": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, trigger an S3 Lifecycle policy to remove the objects that contain PII.",
        "B": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.",
        "C": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.",
        "D": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Email Service (Amazon SES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the meats that contain PII."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85264-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ]
  },
  {
    "idx": 44,
    "question": {
      "kor": "회사에서 새로운 비즈니스 애플리케이션을 구현하고 있습니다. 이 애플리케이션은 두 개의 Amazon EC2 인스턴스에서 실행되며 문서 저장을 위해 Amazon S3 버킷을 사용합니다. 솔루션 설계자는 EC2 인스턴스가 S3 버킷에 액세스할 수 있는지 확인해야 합니다.\n솔루션 설계자는 이 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A company is implementing a new business application. The application runs on two Amazon EC2 instances and uses an Amazon S3 bucket for document storage. A solutions architect needs to ensure that the EC2 instances can access the S3 bucket.\nWhat should the solutions architect do to meet this requirement?"
    },
    "choices": {
      "kor": {
        "A": "S3 버킷에 대한 액세스 권한을 부여하는 IAM 역할을 생성합니다. 역할을 EC2 인스턴스에 연결합니다.",
        "B": "S3 버킷에 대한 액세스 권한을 부여하는 IAM 정책을 생성합니다. 정책을 EC2 인스턴스에 연결합니다.",
        "C": "S3 버킷에 대한 액세스 권한을 부여하는 IAM 그룹을 생성합니다. 그룹을 EC2 인스턴스에 연결합니다.",
        "D": "S3 버킷에 대한 액세스 권한을 부여하는 IAM 사용자를 생성합니다. 사용자 계정을 EC2 인스턴스에 연결합니다."
      },
      "eng": {
        "A": "Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.",
        "B": "Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.",
        "C": "Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.",
        "D": "Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85032-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 45,
    "question": {
      "kor": "솔루션 설계자는 여러 서브넷을 포함하는 VPC 아키텍처를 개발하고 있습니다. 이 아키텍처는 Amazon EC2 인스턴스와 Amazon RDS DB 인스턴스를 사용하는 애플리케이션을 호스팅 합니다. 아키텍처는 2개의 가용 영역에 있는 6개의 서브넷으로 구성됩니다. 각 가용 영역에는 퍼블릭 서브넷, 프라이빗 서브넷 및 데이터베이스 전용 서브넷이 포함됩니다. 프라이빗 서브넷에서 실행되는 EC2 인스턴스만 RDS 데이터베이스에 액세스할 수 있습니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A solutions architect is developing a VPC architecture that includes multiple subnets. The architecture will host applications that use Amazon EC2 instances and Amazon RDS DB instances. The architecture consists of six subnets in two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a dedicated subnet for databases. Only EC2 instances that run in the private subnets can have access to the RDS databases.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "퍼블릭 서브넷의 CIDR 블록에 대한 경로를 제외하는 새 경로 테이블을 만듭니다. 경로 테이블을 데이터베이스 서브넷과 연결합니다.",
        "B": "퍼블릭 서브넷의 인스턴스에 할당된 보안 그룹에서 인바운드 트래픽을 거부하는 보안 그룹을 생성합니다. 보안 그룹을 DB 인스턴스에 연결합니다.",
        "C": "프라이빗 서브넷의 인스턴스에 할당된 보안 그룹에서 인바운드 트래픽을 허용하는 보안 그룹을 생성합니다. 보안 그룹을 DB 인스턴스에 연결합니다.",
        "D": "퍼블릭 서브넷과 프라이빗 서브넷 간에 새 피어링 연결을 만듭니다. 프라이빗 서브넷과 데이터베이스 서브넷 간에 다른 피어링 연결을 만듭니다."
      },
      "eng": {
        "A": "Create a new route table that excludes the route to the public subnets' CIDR blocks. Associate the route table with the database subnets.",
        "B": "Create a security group that denies inbound traffic from the security group that is assigned to instances in the public subnets. Attach the security group to the DB instances.",
        "C": "Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.",
        "D": "Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and the database subnets."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85409-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ]
  },
  {
    "idx": 46,
    "question": {
      "kor": "자전거 공유 회사는 피크 운영 시간 동안 자전거의 위치를 추적하기 위해 다계층 아키텍처를 개발하고 있습니다. 회사는 기존 분석 플랫폼에서 이러한 데이터 포인트를 사용하려고 합니다. 솔루션 설계자는 이 아키텍처를 지원하기 위해 가장 실행 가능한 다중 계층 옵션을 결정해야 합니다. 데이터 포인트는 REST API에서 액세스할 수 있어야 합니다.\n위치 데이터 저장 및 검색에 대한 이러한 요구 사항을 충족하는 작업은 무엇입니까?",
      "eng": "A bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles during peak operating hours. The company wants to use these data points in its existing analytics platform. A solutions architect must determine the most viable multi-tier option to support this architecture. The data points must be accessible from the REST API.\nWhich action meets these requirements for storing and retrieving location data?"
    },
    "choices": {
      "kor": {
        "A": "Amazon S3와 함께 Amazon Athena를 사용합니다.",
        "B": "AWS Lambda와 함께 Amazon API Gateway를 사용합니다.",
        "C": "Amazon Redshift와 함께 Amazon QuickSight를 사용합니다.",
        "D": "Amazon Kinesis Data Analytics와 함께 Amazon API Gateway를 사용합니다."
      },
      "eng": {
        "A": "Use Amazon Athena with Amazon S3.",
        "B": "Use Amazon API Gateway with AWS Lambda.",
        "C": "Use Amazon QuickSight with Amazon Redshift.",
        "D": "Use Amazon API Gateway with Amazon Kinesis Data Analytics."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85212-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ]
  },
  {
    "idx": 47,
    "question": {
      "kor": "한 회사에서 REST API로 검색할 주문 배송 통계를 제공하는 애플리케이션을 개발하고 있습니다. 회사는 배송 통계를 추출하고 데이터를 읽기 쉬운 HTML 형식으로 구성하고 매일 아침 동시에 여러 이메일 주소로 보고서를 보내려고 합니다.\n이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 단계 조합을 수행해야 합니까? (두 가지를 선택하세요.)",
      "eng": "A company is developing an application that provides order shipping statistics for retrieval by a REST API. The company wants to extract the shipping statistics, organize the data into an easy-to-read HTML format, and send the report to several email addresses at the same time every morning.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "데이터를 Amazon Kinesis Data Firehose로 보내도록 애플리케이션을 구성합니다.",
        "B": "Amazon Simple Email Service(Amazon SES)를 사용하여 데이터 형식을 지정하고 이메일로 보고서를 보냅니다.",
        "C": "데이터에 대한 애플리케이션의 API를 쿼리하기 위해 AWS Glue 작업을 호출하는 Amazon EventBridge(Amazon CloudWatch Events) 예약 이벤트를 생성합니다.",
        "D": "데이터에 대한 애플리케이션의 API를 쿼리하기 위해 AWS Lambda 함수를 호출하는 Amazon EventBridge(Amazon CloudWatch Events) 예약 이벤트를 생성합니다.",
        "E": "애플리케이션 데이터를 Amazon S3에 저장합니다. 보고서를 이메일로 보낼 S3 이벤트 대상으로 Amazon Simple Notification Service(Amazon SNS) 주제를 생성합니다."
      },
      "eng": {
        "A": "Configure the application to send the data to Amazon Kinesis Data Firehose.",
        "B": "Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.",
        "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.",
        "D": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.",
        "E": "Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85557-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B",
      "D"
    ]
  },
  {
    "idx": 48,
    "question": {
      "kor": "솔루션 아키텍트가 2계층 웹 애플리케이션을 설계하고 있습니다. 이 애플리케이션은 퍼블릭 서브넷의 Amazon EC2에서 호스팅되는 퍼블릭 웹 계층으로 구성됩니다. 데이터베이스 계층은 프라이빗 서브넷의 Amazon EC2에서 실행되는 Microsoft SQL Server로 구성됩니다. 보안은 회사의 최우선 순위입니다.\n이 상황에서 보안 그룹을 어떻게 구성해야 합니까? (두 가지를 선택하세요.)",
      "eng": "A solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets.\nThe database tier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company.\nHow should security groups be configured in this situation? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "0.0.0.0/0에서 포트 443의 인바운드 트래픽을 허용하도록 웹 계층에 대한 보안 그룹을 구성합니다.",
        "B": "0.0.0.0/0의 포트 443에서 아웃바운드 트래픽을 허용하도록 웹 계층에 대한 보안 그룹을 구성합니다.",
        "C": "웹 계층의 보안 그룹에서 포트 1433의 인바운드 트래픽을 허용하도록 데이터베이스 계층의 보안 그룹을 구성합니다.",
        "D": "웹 계층의 보안 그룹에 대한 포트 443 및 1433의 아웃바운드 트래픽을 허용하도록 데이터베이스 계층의 보안 그룹을 구성합니다.",
        "E": "웹 계층에 대한 보안 그룹의 포트 443 및 1433에서 인바운드 트래픽을 허용하도록 데이터베이스 계층에 대한 보안 그룹을 구성합니다."
      },
      "eng": {
        "A": "Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.",
        "B": "Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.",
        "C": "Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier.",
        "D": "Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier.",
        "E": "Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85346-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A",
      "C"
    ]
  },
  {
    "idx": 49,
    "question": {
      "kor": "회사는 AWS Organizations를 사용하여 여러 부서의 여러 AWS 계정을 관리합니다. 마스터 계정에는 프로젝트 보고서가 포함된 Amazon S3 버킷이 있습니다. 회사는 이 S3 버킷에 대한 액세스를 AWS Organizations의 조직 내 계정 사용자로만 제한하려고 합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains project reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.\nWhich solution meets these requirements with the LEAST amount of operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "조직 ID에 대한 참조와 함께 aws PrincipalOrgID 전역 조건 키를 S3 버킷 정책에 추가합니다.",
        "B": "각 부서에 대한 조직 단위(OU)를 만듭니다. aws:PrincipalOrgPaths 전역 조건 키를 S3 버킷 정책에 추가합니다.",
        "C": "AWS CloudTrail을 사용하여 CreateAccount, InviteAccountToOrganization, LeaveOrganization 및 RemoveAccountFromOrganization 이벤트를 모니터링합니다. 그에 따라 S3 버킷 정책을 업데이트합니다.",
        "D": "S3 버킷에 액세스해야 하는 각 사용자에게 태그를 지정합니다. aws:PrincipalTag 전역 조건 키를 S3 버킷 정책에 추가합니다."
      },
      "eng": {
        "A": "Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.",
        "B": "Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.",
        "C": "Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.",
        "D": "Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy."
      }
    },
    "category": [],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/84838-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ]
  },
  {
    "idx": 50,
    "question": {
      "kor": "솔루션 설계자는 웹 사이트를 위한 고가용성 인프라를 설계해야 합니다. 이 웹 사이트는 Amazon EC2 인스턴스에서 실행되는 Windows 웹 서버로 구동됩니다. 솔루션 설계자는 수천 개의 IP 주소에서 발생하는 대규모 DDoS 공격을 완화할 수 있는 솔루션을 구현해야 합니다. 다운타임은 웹사이트에 허용되지 않습니다.\n그러한 공격으로부터 웹 사이트를 보호하기 위해 솔루션 설계자는 어떤 조치를 취해야 합니까? (두 가지를 선택하세요.)",
      "eng": "A solutions architect must design a highly available infrastructure for a website. The website is powered by Windows web servers that run on Amazon EC2 instances. The solutions architect must implement a solution that can mitigate a large-scale DDoS attack that originates from thousands of IP addresses.\nDowntime is not acceptable for the website.\nWhich actions should the solutions architect take to protect the website from such an attack? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "AWS Shield Advanced를 사용하여 DDoS 공격을 중지하십시오.",
        "B": "공격자를 자동으로 차단하도록 Amazon GuardDuty를 구성합니다.",
        "C": "정적 및 동적 콘텐츠 모두에 대해 Amazon CloudFront를 사용하도록 웹 사이트를 구성합니다.",
        "D": "AWS Lambda 함수를 사용하여 공격자 IP 주소를 VPC 네트워크 ACL에 자동으로 추가합니다.",
        "E": "CPU 사용률이 80%로 설정된 대상 추적 조정 정책과 함께 Auto Scaling 그룹에서 EC2 스팟 인스턴스를 사용합니다."
      },
      "eng": {
        "A": "Use AWS Shield Advanced to stop the DDoS attack.",
        "B": "Configure Amazon GuardDuty to automatically block the attackers.",
        "C": "Configure the website to use Amazon CloudFront for both static and dynamic content.",
        "D": "Use an AWS Lambda function to automatically add attacker IP addresses to VPC network ACLs.",
        "E": "Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set to 80% CPU utilization."
      }
    },
    "category": [
      "DDoS"
    ],
    "subcategory": [
      "Shield",
      "Shield Advanced",
      "CloudFront"
    ],
    "rote_memorization": true,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85342-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A",
      "C"
    ]
  }
]