[
  {
    "idx": 80,
    "question": {
      "kor": "회사는 여러 Amazon EC 인스턴스에서 애플리케이션을 호스팅합니다. 애플리케이션은 Amazon SQS  대기열의 메시지를 처리하고 Amazon RDS 테이블에 쓰고 대기열에서 메시지를 삭제합니다. 때때로 중복 레코드가 RDS 테이블에서 발견됩니다. SQS 대기열에는 중복 메시지가 없습니다.\n솔루션 설계자는 메시지가 한 번만 처리되도록 하려면 어떻게 해야 합니까?",
      "eng": "A company hosts an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table, and deletes the message from the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages.\nWhat should a solutions architect do to ensure messages are being processed once only?"
    },
    "choices": {
      "kor": {
        "A": "CreateQueue API 호출을 사용하여 새 대기열을 만듭니다.",
        "B": "AddPermission API 호출을 사용하여 적절한 권한을 추가합니다.",
        "C": "ReceiveMessage API 호출을 사용하여 적절한 대기 시간을 설정합니다.",
        "D": "ChangeMessageVisibility API 호출을 사용하여 가시성 제한 시간을 늘립니다."
      },
      "eng": {
        "A": "Use the CreateQueue API call to create a new queue.",
        "B": "Use the AddPermission API call to add appropriate permissions.",
        "C": "Use the ReceiveMessage API call to set an appropriate wait time.",
        "D": "Use the ChangeMessageVisibility API call to increase the visibility timeout."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [
      "Visibility timeout"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85583-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 111,
    "question": {
      "kor": "한 회사가 AWS에서 전자상거래 웹 애플리케이션을 구축하고 있습니다. 애플리케이션은 새 주문에 대한 정보를 Amazon API Gateway REST API로 전송하여 처리합니다. 회사는 주문이 접수된 순서대로 처리되기를 원합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company is building an ecommerce web application on AWS. The application sends information about new orders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are processed in the order that they are received.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "애플리케이션이 주문을 받으면 API Gateway 통합을 사용하여 Amazon Simple Notification Service(Amazon SNS) 주제에 메시지를 게시합니다. 처리를 수행할 주제에 대한 AWS Lambda 함수를 구독합니다.",
        "B": "애플리케이션이 주문을 받으면 API Gateway 통합을 사용하여 Amazon Simple Queue Service(Amazon SQS) FIFO 대기열에 메시지를 보냅니다. 처리를 위해 AWS Lambda 함수를 호출하도록 SQS FIFO 대기열을 구성합니다.",
        "C": "애플리케이션이 주문을 처리하는 동안 API 게이트웨이 권한 부여자를 사용하여 모든 요청을 차단합니다.",
        "D": "애플리케이션이 주문을 받으면 API Gateway 통합을 사용하여 Amazon Simple Queue Service(Amazon SQS) 표준 대기열로 메시지를 보냅니다. 처리를 위해 AWS Lambda 함수를 호출하도록 SQS 표준 대기열을 구성합니다."
      },
      "eng": {
        "A": "Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to the topic to perform processing.",
        "B": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.",
        "C": "Use an API Gateway authorizer to block any requests while the application processes an order.",
        "D": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the application receives an order. Configure the SQS standard queue to invoke an AWS Lambda function for processing."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [
      "FIFO"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/84681-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 27,
    "question": {
      "kor": "솔루션 아키텍트는 AWS에 배포되는 새로운 애플리케이션을 위한 클라우드 아키텍처를 설계하고 있습니다. 프로세스는 처리할 작업 수에 따라 필요에 따라 애플리케이션 노드를 추가 및 제거하는 동안 병렬로 실행되어야 합니다. 프로세서 애플리케이션은 상태 비저장입니다. 솔루션 설계자는 애플리케이션이 느슨하게 결합되고 작업 항목이 지속적으로 저장되는지 확인해야 합니다.\n솔루션 설계자는 어떤 디자인을 사용해야 합니까?",
      "eng": "A solutions architect is designing the cloud architecture for a new application being deployed on AWS. The process should run in parallel while adding and removing application nodes as needed based on the number of jobs to be processed. The processor application is stateless. The solutions architect must ensure that the application is loosely coupled and the job items are durably stored.\nWhich design should the solutions architect use?"
    },
    "choices": {
      "kor": {
        "A": "처리해야 할 작업을 보낼 Amazon SNS 주제를 생성합니다. 프로세서 애플리케이션으로 구성된 Amazon 머신 이미지(AMI)를 생성합니다. AMI를 사용하는 시작 구성을 생성합니다. 시작 구성을 사용하여 Auto Scaling 그룹을 생성합니다. CPU 사용량에 따라 노드를 추가 및 제거하도록 Auto Scaling 그룹에 대한 조정 정책을 설정합니다.",
        "B": "처리해야 하는 작업을 보관할 Amazon SQS 대기열을 생성합니다. 프로세서 애플리케이션으로 구성된 Amazon 머신 이미지(AMI)를 생성합니다. AMI를 사용하는 시작 구성을 생성합니다. 시작 구성을 사용하여 Auto Scaling 그룹을 생성합니다. 네트워크 사용량에 따라 노드를 추가 및 제거하도록 Auto Scaling 그룹에 대한 조정 정책을 설정합니다.",
        "C": "처리해야 할 작업을 보관할 Amazon SQS 대기열을 생성합니다. 프로세서 애플리케이션으로 구성된 Amazon 머신 이미지(AMI)를 생성합니다. AMI를 사용하는 시작 템플릿을 생성합니다. 시작 템플릿을 사용하여 Auto Scaling 그룹을 생성합니다. SQS 대기열의 항목 수에 따라 노드를 추가 및 제거하도록 Auto Scaling 그룹에 대한 조정 정책을 설정합니다.",
        "D": "처리해야 할 작업을 보낼 Amazon SNS 주제를 생성합니다. 프로세서 애플리케이션으로 구성된 Amazon 머신 이미지(AMI)를 생성합니다. AMI를 사용하는 시작 템플릿을 생성합니다. 시작 템플릿을 사용하여 Auto Scaling 그룹을 생성합니다. SNS 주제에 게시된 메시지 수에 따라 노드를 추가 및 제거하도록 Auto Scaling 그룹에 대한 조정 정책을 설정합니다."
      },
      "eng": {
        "A": "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage.",
        "B": "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network usage.",
        "C": "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue.",
        "D": "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [
      "AMI",
      "scaling policy"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/86621-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 83,
    "question": {
      "kor": "회사에서 분산 애플리케이션을 AWS로 마이그레이션하고 있습니다. 이 애플리케이션은 다양한 워크로드를 처리합니다. 레거시 플랫폼은 여러 컴퓨팅 노드에서 작업을 조정하는 기본 서버로 구성됩니다. 회사는 복원력과 확장성을 최대화하는 솔루션으로 애플리케이션을 현대화하려고 합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 아키텍처를 어떻게 설계해야 합니까?",
      "eng": "A company is migrating a distributed application to AWS. The application serves variable workloads. The legacy platform consists of a primary server that coordinates jobs across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resiliency and scalability.\nHow should a solutions architect design the architecture to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "작업의 대상으로 Amazon Simple Queue Service(Amazon SQS) 대기열을 구성합니다. Auto Scaling 그룹에서 관리되는 Amazon EC2 인스턴스로 컴퓨팅 노드를 구현합니다. 예약된 조정을 사용하도록 EC2 Auto Scaling을 구성합니다.",
        "B": "작업의 대상으로 Amazon Simple Queue Service(Amazon SQS) 대기열을 구성합니다. Auto Scaling 그룹에서 관리되는 Amazon EC2 인스턴스로 컴퓨팅 노드를 구현합니다. 대기열 크기에 따라 EC2 Auto Scaling을 구성합니다.",
        "C": "Auto Scaling 그룹에서 관리되는 Amazon EC2 인스턴스로 기본 서버 및 컴퓨팅 노드를 구현합니다. 작업의 대상으로 AWS CloudTrail을 구성합니다. 기본 서버의 로드를 기반으로 EC2 Auto Scaling을 구성합니다.",
        "D": "Auto Scaling 그룹에서 관리되는 Amazon EC2 인스턴스로 기본 서버 및 컴퓨팅 노드를 구현합니다. 작업의 대상으로 Amazon EventBridge(Amazon CloudWatch Events)를 구성합니다. 컴퓨팅 노드의 로드를 기반으로 EC2 Auto Scaling을 구성합니다."
      },
      "eng": {
        "A": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.",
        "B": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue.",
        "C": "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure AWS CloudTrail as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the primary server.",
        "D": "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure Amazon EventBridge (Amazon CloudWatch Events) as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/84679-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 120,
    "question": {
      "kor": "회사에는 Amazon EC2 인스턴스에서 실행되는 레거시 데이터 처리 애플리케이션이 있습니다. 데이터는 순차적으로 처리되지만 결과의 순서는 중요하지 않습니다. 응용 프로그램은 모놀리식 아키텍처를 사용합니다. 회사에서 수요 증가에 맞춰 애플리케이션을 확장할 수 있는 유일한 방법은 인스턴스 크기를 늘리는 것입니다.\n이 회사의 개발자는 Amazon Elastic Container Service(Amazon ECS)에서 마이크로서비스 아키텍처를 사용하도록 애플리케이션을 다시 작성하기로 결정했습니다.\n솔루션 설계자는 마이크로서비스 간의 통신을 위해 무엇을 권장해야 합니까?",
      "eng": "A company has a legacy data processing application that runs on Amazon EC2 instances. Data is processed sequentially, but the order of results does not matter. The application uses a monolithic architecture. The only way that the company can scale the application to meet increased demand is to increase the size of the instances.\nThe company’s developers have decided to rewrite the application to use a microservices architecture on Amazon Elastic Container Service (Amazon ECS).\nWhat should a solutions architect recommend for communication between the microservices?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Simple Queue Service(Amazon SQS) 대기열을 생성합니다. 데이터 생산자에 코드를 추가하고 데이터를 대기열로 보냅니다. 데이터 소비자에 코드를 추가하여 대기열의 데이터를 처리합니다.",
        "B": "Amazon Simple Notification Service(Amazon SNS) 주제를 생성합니다. 데이터 생산자에 코드를 추가하고 주제에 알림을 게시합니다. 데이터 소비자에 코드를 추가하여 주제를 구독합니다.",
        "C": "메시지를 전달할 AWS Lambda 함수를 생성합니다. 데이터 생산자에 코드를 추가하여 데이터 객체로 Lambda 함수를 호출합니다. 데이터 소비자에 코드를 추가하여 Lambda 함수에서 전달되는 데이터 객체를 수신합니다.",
        "D": "Amazon DynamoDB 테이블을 생성합니다. DynamoDB 스트림을 활성화합니다. 데이터 생산자에 코드를 추가하여 테이블에 데이터를 삽입합니다. 데이터 소비자에 코드를 추가하여 DynamoDB Streams API를 사용하여 새 테이블 항목을 감지하고 데이터를 검색합니다."
      },
      "eng": {
        "A": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send data to the queue. Add code to the data consumers to process data from the queue.",
        "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Add code to the data producers, and publish notifications to the topic. Add code to the data consumers to subscribe to the topic.",
        "C": "Create an AWS Lambda function to pass messages. Add code to the data producers to call the Lambda function with a data object. Add code to the data consumers to receive a data object that is passed from the Lambda function.",
        "D": "Create an Amazon DynamoDB table. Enable DynamoDB Streams. Add code to the data producers to insert data into the table. Add code to the data consumers to use the DynamoDB Streams API to detect new table entries and retrieve the data."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [
      "microservices architecture"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/87647-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 122,
    "question": {
      "kor": "회사는 사용자 요청을 수집하고 요청 유형에 따라 처리를 위해 적절한 마이크로 서비스에 요청을 발송하는 데 사용되는 비동기 API를 소유하고 있습니다. 이 회사는 Amazon API Gateway를 사용하여 API 프런트 엔드를 배포하고 Amazon DynamoDB를 호출하여 사용자 요청을 처리 마이크로서비스로 보내기 전에 저장하는 AWS Lambda 함수를 사용하고 있습니다.\n회사는 예산이 허용하는 한 많은 DynamoDB 처리량을 프로비저닝했지만 회사는 여전히 가용성 문제를 겪고 있으며 사용자 요청이 손실되고 있습니다.\n솔루션 설계자는 기존 사용자에게 영향을 주지 않고 이 문제를 해결하기 위해 무엇을 해야 합니까?",
      "eng": "A company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway to deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests before dispatching them to the processing microservices.\nThe company provisioned as much DynamoDB throughput as its budget allows, but the company is still experiencing availability issues and is losing user requests.\nWhat should a solutions architect do to address this issue without impacting existing users?"
    },
    "choices": {
      "kor": {
        "A": "API 게이트웨이에서 서버 측 조절 제한을 사용하여 조절을 추가합니다.",
        "B": "DynamoDB Accelerator(DAX) 및 Lambda를 사용하여 DynamoDB에 대한 쓰기를 버퍼링합니다.",
        "C": "사용자 요청이 있는 테이블에 대해 DynamoDB에서 보조 인덱스를 생성합니다.",
        "D": "Amazon Simple Queue Service(Amazon SQS) 대기열과 Lambda를 사용하여 DynamoDB에 대한 쓰기를 버퍼링합니다."
      },
      "eng": {
        "A": "Add throttling on the API Gateway with server-side throttling limits.",
        "B": "Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.",
        "C": "Create a secondary index in DynamoDB for the table with the user requests.",
        "D": "Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [
      "asynchronous",
      "microservice"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/89087-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 134,
    "question": {
      "kor": "회사에는 데이터베이스에 주문을 작성하고 지불을 처리하기 위해 서비스를 호출하는 전자 상거래 체크아웃 워크플로우가 있습니다. 사용자는 체크아웃 프로세스 중에 시간 초과를 경험하고 있습니다. 사용자가 체크아웃 양식을 다시 제출하면 동일한 원하는 거래에 대해 여러 고유 주문이 생성됩니다.\n여러 주문 생성을 방지하기 위해 솔루션 설계자는 이 워크플로우를 어떻게 리팩터링해야 합니까?",
      "eng": "A company has an ecommerce checkout workflow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction.\nHow should a solutions architect refactor this workflow to prevent the creation of multiple orders?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis Data Firehose로 주문 메시지를 보내도록 웹 애플리케이션을 구성합니다. Kinesis Data Firehose에서 메시지를 검색하고 주문을 처리하도록 결제 서비스를 설정합니다.",
        "B": "로깅된 애플리케이션 경로 요청을 기반으로 AWS Lambda 함수를 호출하기 위해 AWS CloudTrail에서 규칙을 생성합니다. Lambda를 사용하여 데이터베이스를 쿼리하고 결제 서비스를 호출하고 주문 정보를 전달합니다.",
        "C": "데이터베이스에 주문을 저장합니다. 주문 번호가 포함된 메시지를 Amazon Simple Notification Service(Amazon SNS)로 보냅니다. Amazon SNS를 폴링하고 메시지를 검색하고 주문을 처리하도록 결제 서비스를 설정합니다.",
        "D": "데이터베이스에 주문을 저장합니다. 주문 번호가 포함된 메시지를 Amazon Simple Queue Service(Amazon SQS) FIFO 대기열로 보냅니다. 메시지를 검색하고 주문을 처리하도록 결제 서비스를 설정합니다. 대기열에서 메시지를 삭제합니다."
      },
      "eng": {
        "A": "Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.",
        "B": "Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.",
        "C": "Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.",
        "D": "Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/95026-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 189,
    "question": {
      "kor": "회사에는 이벤트 데이터를 생성하는 서비스가 있습니다. 회사는 AWS를 사용하여 수신된 이벤트 데이터를 처리하려고 합니다. 데이터는 처리 전반에 걸쳐 유지되어야 하는 특정 순서로 기록됩니다. 회사는 운영 오버헤드를 최소화하는 솔루션을 구현하려고 합니다.\n솔루션 설계자는 이를 어떻게 달성해야 합니까?",
      "eng": "A company has a service that produces event data. The company wants to use AWS to process the event data as it is received. The data is written in a specific order that must be maintained throughout processing. The company wants to implement a solution that minimizes operational overhead.\nHow should a solutions architect accomplish this?"
    },
    "choices": {
      "kor": {
        "A": "메시지를 보관할 Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 생성합니다. 대기열의 메시지를 처리하도록 AWS Lambda 함수를 설정합니다.",
        "B": "처리할 페이로드가 포함된 알림을 전달할 Amazon Simple Notification Service(Amazon SNS) 주제를 생성합니다. AWS Lambda 함수를 구독자로 구성합니다.",
        "C": "메시지를 보관할 Amazon Simple Queue Service(Amazon SQS) 표준 대기열을 생성합니다. 대기열의 메시지를 독립적으로 처리하도록 AWS Lambda 함수를 설정합니다.",
        "D": "처리할 페이로드가 포함된 알림을 전달할 Amazon Simple Notification Service(Amazon SNS) 주제를 생성합니다. Amazon Simple Queue Service(Amazon SQS) 대기",
        "E": "열을 구독자로 구성합니다."
      },
      "eng": {
        "A": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages. Set up an AWS Lambda function to process messages from the queue.",
        "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an AWS Lambda function as a subscriber.",
        "C": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages. Set up an AWS Lambda function to process messages from the queue independently.",
        "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/86784-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 192,
    "question": {
      "kor": "회사에는 다음 구성 요소를 포함하는 데이터 수집 워크플로가 있습니다.\n새로운 데이터 전송에 대한 알림을 수신하는 Amazon Simple Notification Service(Amazon SNS) 주제 \n데이터를 처리하고 저장하는 AWS Lambda 함수 \n네트워크 연결 문제로 인해 수집 워크플로가 때때로 실패함. 장애가 발생하면 회사에서 수동으로 작업을 다시 실행하지 않는 한 해당 데이터가 수집되지 않습니다.\n모든 알림이 최종적으로 처리되도록 하려면 솔루션 설계자가 무엇을 해야 합니까?",
      "eng": "A company has a data ingestion workflow that includes the following components:\nAn Amazon Simple Notification Service (Amazon SNS) topic that receives notifications about new data deliveries\nAn AWS Lambda function that processes and stores the data\nThe ingestion workflow occasionally fails because of network connectivity issues. When failure occurs, the corresponding data is not ingested unless the company manually reruns the job.\nWhat should a solutions architect do to ensure that all notifications are eventually processed?"
    },
    "choices": {
      "kor": {
        "A": "여러 가용 영역에 걸쳐 배포할 Lambda 함수를 구성합니다.",
        "B": "Lambda 함수의 구성을 수정하여 함수에 대한 CPU 및 메모리 할당을 늘립니다.",
        "C": "재시도 횟수와 재시도 사이의 대기 시간을 모두 늘리도록 SNS 주제의 재시도 전략을 구성합니다.",
        "D": "Amazon Simple Queue Service(Amazon SQS) 대기열을 장애 시 대상으로 구성합니다. 대기열의 메시지를 처리하도록 Lambda 함수를 수정합니다."
      },
      "eng": {
        "A": "Configure the Lambda function for deployment across multiple Availability Zones.",
        "B": "Modify the Lambda function's configuration to increase the CPU and memory allocations for the function.",
        "C": "Configure the SNS topic’s retry strategy to increase both the number of retries and the wait time between retries.",
        "D": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Modify the Lambda function to process messages in the queue."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [
      "Lambda"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85424-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 204,
    "question": {
      "kor": "회사에는 여러 모니터링 장치에서 실시간 데이터를 수신하는 API가 있습니다. API는 나중에 분석할 수 있도록 이 데이터를 Amazon RDS DB 인스턴스에 저장합니다. 모니터링 장치가 API로 보내는 데이터의 양은 변동합니다. 트래픽이 많은 기간 동안 API는 종종 시간 초과 오류를 반환합니다. 로그를 검사한 후 회사는 데이터베이스가 API에서 오는 쓰기 트래픽 볼륨을 처리할 수 없음을 확인합니다. 솔루션 설계자는 데이터베이스에 대한 연결 수를 최소화하고 트래픽이 많은 기간 동안 데이터가 손실되지 않도록 해야 합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company has an API that receives real-time data from a fleet of monitoring devices. The API stores this data in an Amazon RDS DB instance for later analysis. The amount of data that the monitoring devices send to the API fluctuates. During periods of heavy traffic, the API often returns timeout errors.\nAfter an inspection of the logs, the company determines that the database is not capable of processing the volume of write traffic that comes from the API. A solutions architect must minimize the number of connections to the database and must ensure that data is not lost during periods of heavy traffic.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "사용 가능한 메모리가 더 많은 인스턴스 유형으로 DB 인스턴스의 크기를 늘리십시오.",
        "B": "DB 인스턴스를 다중 AZ DB 인스턴스로 수정합니다. 모든 활성 RDS DB 인스턴스에 쓰도록 애플리케이션을 구성합니다.",
        "C": "수신 데이터를 Amazon Simple Queue Service(Amazon SQS) 대기열에 쓰도록 API를 수정합니다. Amazon SQS가 호출하는 AWS Lambda 함수를 사용하여 대기열에서 데이터베이스로 데이터를 씁니다.",
        "D": "수신 데이터를 Amazon Simple Notification Service(Amazon SNS) 주제에 쓰도록 API를 수정합니다. Amazon SNS가 호출하는 AWS Lambda 함수를 사용하여 주제에서 데이터베이스로 데이터를 씁니다."
      },
      "eng": {
        "A": "Increase the size of the DB instance to an instance type that has more available memory.",
        "B": "Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all active RDS DB instances.",
        "C": "Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database.",
        "D": "Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an AWS Lambda function that Amazon SNS invokes to write data from the topic to the database."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [
      "Lambda"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/95318-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 190,
    "question": {
      "kor": "회사에는 처리할 페이로드가 포함된 메시지를 보내는 발신자 애플리케이션과 페이로드가 포함된 메시지를 수신하기 위한 처리 애플리케이션의 두 가지 애플리케이션이 있습니다. 회사는 두 애플리케이션 간의 메시지를 처리하기 위해 AWS 서비스를 구현하려고 합니다. 발신자 애플리케이션은 매시간 약 1,000개의 메시지를 보낼 수 있습니다. 메시지를 처리하는 데 최대 2일이 걸릴 수 있습니다. 메시지를 처리하지 못한 경우 나머지 메시지 처리에 영향을 주지 않도록 보관해야 합니다.\n어떤 솔루션이 이러한 요구 사항을 충족하고 운영상 가장 효율적입니까?",
      "eng": "A company has two applications: a sender application that sends messages with payloads to be processed and a processing application intended to receive the messages with payloads. The company wants to implement an AWS service to handle messages between the two applications. The sender application can send about 1,000 messages each hour. The messages may take up to 2 days to be processed: If the messages fail to process, they must be retained so that they do not impact the processing of any remaining messages.\nWhich solution meets these requirements and is the MOST operationally efficient?"
    },
    "choices": {
      "kor": {
        "A": "Redis 데이터베이스를 실행하는 Amazon EC2 인스턴스를 설정합니다. 인스턴스를 사용하도록 두 애플리케이션을 모두 구성합니다. 메시지를 각각 저장, 처리 및 삭제합니다.",
        "B": "Amazon Kinesis 데이터 스트림을 사용하여 발신자 애플리케이션에서 메시지를 수신합니다. 처리 애플리케이션을 Kinesis Client Library(KCL)와 통합합니다.",
        "C": "발신자 및 프로세서 애플리케이션을 Amazon Simple Queue Service(Amazon SQS) 대기열과 통합합니다. 처리에 실패한 메시지를 수집하도록 배달 못한 편지 대기열을 구성합니다.",
        "D": "처리할 알림을 수신하려면 처리 애플리케이션을 Amazon Simple Notification Service(Amazon SNS) 주제에 구독합니다. 발신자 애플리케이션을 통합하여 SNS 주제에 씁니다."
      },
      "eng": {
        "A": "Set up an Amazon EC2 instance running a Redis database. Configure both applications to use the instance. Store, process, and delete the messages, respectively.",
        "B": "Use an Amazon Kinesis data stream to receive the messages from the sender application. Integrate the processing application with the Kinesis Client Library (KCL).",
        "C": "Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue. Configure a dead-letter queue to collect the messages that failed to process.",
        "D": "Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications to process. Integrate the sender application to write to the SNS topic."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [
      "DLQ"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/87523-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 219,
    "question": {
      "kor": "회사의 주문 시스템은 클라이언트의 요청을 Amazon EC2 인스턴스로 보냅니다. EC2 인스턴스는 주문을 처리한 다음 Amazon RDS의 데이터베이스에 주문을 저장합니다. 사용자는 시스\n템이 실패하면 주문을 다시 처리해야 한다고 보고합니다. 회사는 시스템 중단이 발생할 경우 주문을 자동으로 처리할 수 있는 탄력적인 솔루션을 원합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A company’s order system sends requests from clients to Amazon EC2 instances. The EC2 instances process the orders and then store the orders in a database on Amazon RDS. Users report that they must reprocess orders when the system fails. The company wants a resilient solution that can process orders automatically if a system outage occurs.\nWhat should a solutions architect do to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "EC2 인스턴스를 Auto Scaling 그룹으로 이동합니다. Amazon Elastic Container Service(Amazon ECS) 작업을 대상으로 하는 Amazon EventBridge(Amazon",
        "B": "CloudWatch Events) 규칙을 생성합니다.",
        "C": "Application Load Balancer(ALB) 뒤에 있는 Auto Scaling 그룹으로 EC2 인스턴스를 이동합니다. ALB 엔드포인트에 메시지를 보내도록 주문 시스템을 업데이트합니다.",
        "D": "EC2 인스턴스를 Auto Scaling 그룹으로 이동합니다. Amazon Simple Queue Service(Amazon SQS) 대기열로 메시지를 보내도록 주문 시스템을 구성합니다. 대기열의 메시지를 사용하도록 EC2 인스턴스를 구성합니다.",
        "E": "Amazon Simple Notification Service(Amazon SNS) 주제를 생성합니다. AWS Lambda 함수를 생성하고 함수를 SNS 주제에 구독합니다. SNS 주제에 메시지를 보내도록 주",
        "F": "문 시스템을 구성합니다. AWS Systems Manager Run Command를 사용하여 메시지를 처리하도록 EC2 인스턴스에 명령을 보냅니다."
      },
      "eng": {
        "A": "Move the EC2 instances into an Auto Scaling group. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to target an Amazon Elastic Container Service (Amazon ECS) task.",
        "B": "Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB). Update the order system to send messages to the ALB endpoint.",
        "C": "Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.",
        "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function, and subscribe the function to the SNS topic. Configure the order system to send messages to the SNS topic. Send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/89138-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 230,
    "question": {
      "kor": "솔루션 설계자는 회사를 위한 다중 계층 애플리케이션을 설계하고 있습니다. 애플리케이션 사용자는 모바일 장치에서 이미지를 업로드합니다. 애플리케이션은 각 이미지의 썸네일을 생성하고 이미지가 성공적으로 업로드되었음을 확인하는 메시지를 사용자에게 반환합니다.\n썸네일 생성에는 최대 60초가 소요될 수 있지만 회사는 사용자에게 원본 이미지가 수신되었음을 알리기 위해 더 빠른 응답 시간을 제공하고자 합니다. 솔루션 설계자는 서로 다른 애플리케이션 계층에 요청을 비동기식으로 전달하도록 애플리케이션을 설계해야 합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to confirm that the image was uploaded successfully.\nThe thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers.\nWhat should the solutions architect do to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "사용자 지정 AWS Lambda 함수를 작성하여 썸네일을 생성하고 사용자에게 알립니다. 이미지 업로드 프로세스를 이벤트 소스로 사용하여 Lambda 함수를 호출합니다.",
        "B": "AWS Step Functions 워크플로를 생성합니다. 애플리케이션 계층 간의 오케스트레이션을 처리하고 썸네일 생성이 완료되면 사용자에게 알리도록 Step Functions를 구성합니다.",
        "C": "Amazon Simple Queue Service(Amazon SQS) 메시지 대기열을 생성합니다. 이미지가 업로드되면 썸네일 생성을 위해 SQS 대기열에 메시지를 배치합니다. 이미지가 수신되었음을 애플리케이션 메시지를 통해 사용자에게 알립니다.",
        "D": "Amazon Simple Notification Service(Amazon SNS) 알림 주제 및 구독을 생성합니다. 애플리케이션과 함께 하나의 구독을 사용하여 이미지 업로드가 완료된 후 썸네일을 생성하십시오. 섬네일 생성이 완료된 후 푸시 알림을 통해 사용자의 모바일 앱에 메시지를 보내려면 두 번째 구독을 사용하십시오."
      },
      "eng": {
        "A": "Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.",
        "B": "Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.",
        "C": "Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.",
        "D": "Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one subscription with the application to generate the thumbnail after the image upload is complete. Use a second subscription to message the user's mobile app by way of a push notification after thumbnail generation is complete."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/99753-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 232,
    "question": {
      "kor": "회사는 새 문서가 Amazon S3 버킷에 업로드될 때 AWS Lambda 함수를 호출하는 서버리스 애플리케이션을 배포했습니다. 애플리케이션은 Lambda 함수를 사용하여 문서를 처리합니다. 최근 마케팅 캠페인 후 회사는 애플리케이션이 많은 문서를 처리하지 않는다는 사실을 알게 되었습니다.\n솔루션 설계자는 이 애플리케이션의 아키텍처를 개선하기 위해 무엇을 해야 합니까?",
      "eng": "A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a recent marketing campaign, the company noticed that the application did not process many of the documents.\nWhat should a solutions architect do to improve the architecture of this application?"
    },
    "choices": {
      "kor": {
        "A": "Lambda 함수의 런타임 제한 시간 값을 15분으로 설정합니다.",
        "B": "S3 버킷 복제 정책을 구성합니다. 나중에 처리할 수 있도록 S3 버킷에 문서를 준비합니다.",
        "C": "추가 Lambda 함수를 배포합니다. 두 Lambda 함수에서 문서 처리 부하를 분산합니다.",
        "D": "Amazon Simple Queue Service(Amazon SQS) 대기열을 생성합니다. 대기열에 요청을 보냅니다. 대기열을 Lambda에 대한 이벤트 소스로 구성합니다."
      },
      "eng": {
        "A": "Set the Lambda function's runtime timeout value to 15 minutes.",
        "B": "Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.",
        "C": "Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.",
        "D": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/102180-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 405,
    "question": {
      "kor": "빠르게 성장하고 있는 음식 배달 서비스를 제공하는 회사가 있습니다. 성장으로 인해 회사의 주문 처리 시스템은 최대 트래픽 시간 동안 확장 문제를 겪고 있습니다. 현재 아키텍처에는 다음이 포함됩니다.\n* 애플리케이션에서 주문을 수집하기 위해 Amazon EC2 Auto Scaling 그룹에서 실행되는 Amazon EC2 인스턴스 그룹\n* 주문을 이행하기 위해 Amazon EC2 Auto Scaling 그룹에서 실행되는 다른 EC2 인스턴스 그룹\n주문 수집 프로세스는 빠르게 진행되지만 주문 이행 프로세스는 더 오래 걸릴 수 있습니다. 스케일링 이벤트로 인해 데이터가 손실되어서는 안 됩니다.\n솔루션 설계자는 주문 수집 프로세스와 주문 이행 프로세스가 트래픽이 가장 많은 시간에 적절하게 확장될 수 있는지 확인해야 합니다. 솔루션은 회사의 AWS 리소스 활용을 최적화해야 합니다.\n어떤 솔루션이 이러한 요구 사항을 충족합니까?",
      "eng": "A company offers a food delivery service that is growing rapidly. Because of the growth, the company’s order processing system is experiencing scaling problems during peak traffic hours. The current architecture includes the following: * A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the application * Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders\nThe order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event.\nA solutions architect must ensure that the order collection process and the order fulfillment process can both scale properly during peak traffic hours. The solution must optimize utilization of the company’s AWS resources.\nWhich solution meets these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Amazon CloudWatch 지표를 사용하여 Auto Scaling 그룹에 있는 각 인스턴스의 CPU를 모니터링합니다. 최대 워크로드 값에 따라 각 Auto Scaling 그룹의 최소 용량을 구성합니다.",
        "B": "Amazon CloudWatch 지표를 사용하여 Auto Scaling 그룹에 있는 각 인스턴스의 CPU를 모니터링합니다. 요청 시 추가 Auto Scaling 그룹을 생성하는 Amazon SimpleNotificationService(Amazon SNS) 주제를 호출하도록 CloudWatch 경보를 구성합니다.",
        "C": "두 개의 Amazon Simple Queue Service(Amazon SQS) 대기열을 프로비저닝합니다. 하나는 주문 수집용이고 다른 하나는 주문 이행용입니다. 각 대기열을 폴링하도록 EC2 인스턴스를 구성합니다. 대기열이 보내는 알림을 기반으로 Auto Scaling 그룹을 조정합니다.",
        "D": "2개의 Amazon Simple Queue Service(Amazon SQS) 대기열을 프로비저닝합니다. 하나는 주문 수집용이고 다른 하나는 주문 이행용입니다. 각 대기열을 폴링하도록 EC2 인스턴스를 구성합니다. 인스턴스 계산당 백로그를 기반으로 지표를 만듭니다. 이 지표를 기반으로 Auto Scaling 그룹을 조정합니다."
      },
      "eng": {
        "A": "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure each Auto Scaling group’s minimum capacity according to peak workload values.",
        "B": "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure a CloudWatch alarm to invoke an Amazon Simple Notification Service (Amazon SNS) topic that creates additional Auto Scaling groups on demand.",
        "C": "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Scale the Auto Scaling groups based on notifications that the queues send.",
        "D": "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/94992-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 410,
    "question": {
      "kor": "회사에서 Amazon 머신 이미지(AMI)를 관리하려고 합니다. 회사는 현재 AMI가 생성된 동일한 AWS 리전에 AMI를 복사합니다. 회사는 AWS API 호출을 캡처하고 회사 계정 내에서 Amazon EC2 CreateImage API 작업이 호출될 때마다 알림을 보내는 애플리케이션을 설계해야 합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where the AMIs were created.\nThe company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 CreateImage API operation is called within the company’s account.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "AWS CloudTrail 로그를 쿼리하고 CreateImage API 호출이 감지되면 알림을 보내는 AWS Lambda 함수를 생성합니다.",
        "B": "업데이트된 로그가 Amazon S3로 전송될 때 발생하는 Amazon Simple Notification Service(Amazon SNS) 알림으로 AWS CloudTrail을 구성합니다. Amazon Athena를사용하여 새 테이블을 생성하고 API 호출이 감지되면 CreateImage에서 쿼리합니다.",
        "C": "CreateImage API 호출에 대한 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. CreateImage API 호출이 감지되면 알림을 보내도록 대상을 Amazon Simple Notification Service(Amazon SNS) 주제로 구성합니다.",
        "D": "Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 AWS CloudTrail 로그의 대상으로 구성합니다. CreateImage API 호출이 감지되면 Amazon SimpleNotificationService(Amazon SNS) 주제에 알림을 보내는 AWS Lambda 함수를 생성합니다."
      },
      "eng": {
        "A": "Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a CreateImage API call is detected.",
        "B": "Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on CreateImage when an API call is detected.",
        "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateImage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a CreateImage API call is detected.",
        "D": "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when a CreateImage API call is detected."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/89086-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 470,
    "question": {
      "kor": "회사에 고객 주문을 처리하는 애플리케이션이 있습니다. 회사는 주문을 Amazon Aurora 데이터베이스에 저장하는 Amazon EC2 인스턴스에서 애플리케이션을 호스팅합니다. 때때로 트래픽이 많을 때 워크로드가 주문을 충분히 빠르게 처리하지 못합니다.\n가능한 한 빨리 데이터베이스에 주문을 안정적으로 기록하려면 솔루션 설계자가 무엇을 해야 합니까?",
      "eng": "A company has an application that processes customer orders. The company hosts the application on an Amazon EC2 instance that saves the orders to an Amazon Aurora database. Occasionally when traffic is high, the workload does not process orders fast enough.\nWhat should a solutions architect do to write the orders reliably to the database as quickly as possible?"
    },
    "choices": {
      "kor": {
        "A": "트래픽이 많을 때 EC2 인스턴스의 인스턴스 크기를 늘립니다. Amazon Simple Notification Service(Amazon SNS)에 주문을 작성합니다. SNS 주제에 데이터베이스 엔드포인트를 구독합니다.",
        "B": "Amazon Simple Queue Service(Amazon SQS) 대기열에 주문을 씁니다. Application Load Balancer 뒤의 Auto Scaling 그룹에서 EC2 인스턴스를 사용하여 SQS 대기열에서 읽고 주문을 데이터베이스로 처리합니다.",
        "C": "Amazon Simple Notification Service(Amazon SNS)에 주문을 작성합니다. SNS 주제에 데이터베이스 엔드포인트를 구독합니다. Application Load Balancer 뒤의 Auto 그룹에서 Scaling EC2 인스턴스를 사용하여 SNS 주제에서 읽습니다.",
        "D": "EC2 인스턴스가 CPU 임계값 제한에 도달하면 Amazon Simple Queue Service(Amazon SQS) 대기열에 주문을 씁니다. Application Load Balancer 뒤의 Auto Scaling 그룹에서 EC2 인스턴스의 예약된 조정을 사용하여 SQS 대기열에서 읽고 데이터베이스로 주문을 처리합니다."
      },
      "eng": {
        "A": "Increase the instance size of the EC2 instance when traffic is high. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic.",
        "B": "Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.",
        "C": "Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SNS topic.",
        "D": "Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance reaches CPU threshold limits. Use scheduled scaling of EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/109653-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 515,
    "question": {
      "kor": "금융 회사의 고객은 문자 메시지를 보내 재정 고문과의 약속을 요청합니다. Amazon EC2 인스턴스에서 실행되는 웹 애플리케이션은 약속 요청을 수락합니다. 텍스트 메시지는 웹 애플리케이션을 통해 Amazon Simple Queue Service(Amazon SQS) 대기열에 게시됩니다. EC2 인스턴스에서 실행되는 또 다른 애플리케이션은 회의 초대장과 회의 확인 이메일 메시지를 고객에게 보냅니다. 예약에 성공한 후 이 애플리케이션은 회의 정보를 Amazon DynamoDB 데이터베이스에 저장합니다.\n회사가 확장됨에 따라 고객은 회의 초대장이 도착하는 데 시간이 더 오래 걸린다고 보고합니다.\n솔루션 설계자는 이 문제를 해결하기 위해 무엇을 권장해야 합니까?",
      "eng": "The customers of a finance company request appointments with financial advisors by sending text messages. A web application that runs on Amazon EC2 instances accepts the appointment requests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another application that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages to the customers. After successful scheduling, this application stores the meeting information in an Amazon DynamoDB database.\nAs the company expands, customers report that their meeting invitations are taking longer to arrive.\nWhat should a solutions architect recommend to resolve this issue?"
    },
    "choices": {
      "kor": {
        "A": "DynamoDB 데이터베이스 앞에 DynamoDB Accelerator(DAX) 클러스터를 추가합니다.",
        "B": "약속 요청을 수락하는 웹 애플리케이션 앞에 Amazon API Gateway API를 추가합니다.",
        "C": "Amazon CloudFront 배포를 추가합니다. 오리진을 약속 요청을 수락하는 웹 애플리케이션으로 설정합니다.",
        "D": "회의 초대를 보내는 애플리케이션에 대한 Auto Scaling 그룹을 추가합니다. SQS 대기열의 깊이에 따라 확장되도록 Auto Scaling 그룹을 구성합니다."
      },
      "eng": {
        "A": "Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.",
        "B": "Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.",
        "C": "Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the appointment requests.",
        "D": "Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SQS queue."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/89082-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 603,
    "question": {
      "kor": "개발팀이 다른 회사와 협력하여 통합 제품을 만들고 있습니다. 다른 회사는 개발 팀의 계정에 포함된 Amazon Simple Queue Service(Amazon SQS) 대기열에 액세스해야 합니다. 다른 회사는 자신의 계정 권한을 포기하지 않고 대기열을 폴링하려고 합니다.\n솔루션 설계자는 SQS 대기열에 대한 액세스를 어떻게 제공해야 합니까?",
      "eng": "A development team is collaborating with another company to create an integrated product. The other company needs to access an Amazon Simple Queue\nService (Amazon SQS) queue that is contained in the development team's account. The other company wants to poll the queue without giving up its own account permissions to do so.\nHow should a solutions architect provide access to the SQS queue?"
    },
    "choices": {
      "kor": {
        "A": "다른 회사에 SQS 대기열에 대한 액세스를 제공하는 인스턴스 프로필을 생성합니다.",
        "B": "SQS 대기열에 대한 다른 회사 액세스를 제공하는 IAM 정책을 생성합니다.",
        "C": "SQS 대기열에 대한 다른 회사 액세스를 제공하는 SQS 액세스 정책을 만듭니다.",
        "D": "다른 회사에 SQS 대기열에 대한 액세스를 제공하는 Amazon Simple 알림 서비스(Amazon SNS) 액세스 정책을 생성합니다."
      },
      "eng": {
        "A": "Create an instance profile that provides the other company access to the SQS queue.",
        "B": "Create an IAM policy that provides the other company access to the SQS queue.",
        "C": "Create an SQS access policy that provides the other company access to the SQS queue.",
        "D": "Create an Amazon Simple Notification Service (Amazon SNS) access policy that provides the other company access to the SQS queue."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/132956-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 633,
    "question": {
      "kor": "회사는 Amazon EC2 인스턴스에서 분석 소프트웨어를 실행합니다. 소프트웨어는 Amazon S3에 업로드된 데이터를 처리하기 위해 사용자의 작업 요청을 수락합니다. 일부 제출된 데이터가 처리되지 않고 있다고 사용자가 보고합니다. Amazon CloudWatch는 EC2 인스턴스의 일관된 CPU 사용률이 100% 또는 거의 100%에 가깝다고 밝혔습니다. 회사는 시스템 성능을 개선하고 사용자 부하에 따라 시스템을 확장하려고 합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A company runs analytics software on Amazon EC2 instances. The software accepts job requests from users to process data that has been uploaded to Amazon S3. Users report that some submitted data is not being processed Amazon CloudWatch reveals that the EC2 instances have a consistent CPU utilization at or near 100%. The company wants to improve system performance and scale the system based on user load.\nWhat should a solutions architect do to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "인스턴스의 복사본을 만듭니다. Application Load Balancer 뒤에 모든 인스턴스를 배치합니다.",
        "B": "Amazon S3용 S3 VPC 엔드포인트를 생성합니다. 엔드포인트를 참조하도록 소프트웨어를 업데이트합니다.",
        "C": "EC2 인스턴스를 중지합니다. CPU와 메모리가 더 강력한 인스턴스 유형으로 인스턴스 유형을 수정합니다. 인스턴스를 다시 시작하십시오.",
        "D": "들어오는 요청을 Amazon Simple Queue Service(Amazon SQS)로 라우팅합니다. 대기열 크기에 따라 EC2 Auto Scaling 그룹을 구성합니다. 대기열에서 읽을 수 있도록 소프",
        "E": "트웨어를 업데이트합니다."
      },
      "eng": {
        "A": "Create a copy of the instance. Place all instances behind an Application Load Balancer.",
        "B": "Create an S3 VPC endpoint for Amazon S3. Update the software to reference the endpoint.",
        "C": "Stop the EC2 instances. Modify the instance type to one with a more powerful CPU and more memory. Restart the instances.",
        "D": "Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2 Auto Scaling group based on queue size. Update the software to read from the queue."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/95329-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 725,
    "question": {
      "kor": "한 회사는 GPS 추적기를 사용하여 수천 마리의 바다거북의 이동 패턴을 기록합니다. 추적기는 거북이가 100야드(91.4미터) 이상 이동했는지 확인하기 위해 5분마다 확인합니다. 거북이가 움직인 경우 추적기는 하나의 AWS 리전의 여러 가용 영역에 있는 3개의 Amazon EC2 인스턴스에서 실행되는 웹 애플리케이션에 새 좌표를 보냅니다.\n최근 예상치 못한 양의 추적 데이터를 처리하는 동안 웹 애플리케이션이 과부하되었습니다. 이벤트를 재생할 방법이 없어 데이터가 손실되었습니다. 솔루션 설계자는 이 문제가 다시 발생하지 않도록 방지해야 하며 운영 오버헤드가 가장 적은 솔루션이 필요합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A company uses GPS trackers to document the migration patterns of thousands of sea turtles. The trackers check every 5 minutes to see if a turtle has moved more than 100 yards (91.4 meters). If a turtle has moved, its tracker sends the new coordinates to a web application running on three Amazon EC2 instances that are in multiple Availability Zones in one AWS Region.\nRecently, the web application was overwhelmed while processing an unexpected volume of tracker data. Data was lost with no way to replay the events. A solutions architect must prevent this problem from happening again and needs a solution with the least operational overhead.\nWhat should the solutions architect do to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "데이터를 저장할 Amazon S3 버킷을 생성합니다. 처리를 위해 버킷에서 새 데이터를 검색하도록 애플리케이션을 구성합니다.",
        "B": "전송된 위치 좌표를 처리하기 위해 Amazon API Gateway 엔드포인트를 생성합니다. AWS Lambda 함수를 사용하여 각 항목을 동시에 처리합니다.",
        "C": "Amazon Simple Queue Service(Amazon SQS) 대기열을 생성하여 수신 데이터를 저장합니다. 처리할 새 메시지를 폴링하도록 애플리케이션을 구성합니다.",
        "D": "전송된 위치 좌표를 저장할 Amazon DynamoDB 테이블을 생성합니다. 처리할 새 데이터에 대한 테이블을 쿼리하도록 애플리케이션을 구성합니다. TTL을 사용하여 처리된 데이터를 제거합니다."
      },
      "eng": {
        "A": "Create an Amazon S3 bucket to store the data. Configure the application to scan for new data in the bucket for processing.",
        "B": "Create an Amazon API Gateway endpoint to handle transmitted location coordinates. Use an AWS Lambda function to process each item concurrently.",
        "C": "Create an Amazon Simple Queue Service (Amazon SQS) queue to store the incoming data. Configure the application to poll for new messages for processing.",
        "D": "Create an Amazon DynamoDB table to store transmitted location coordinates. Configure the application to query the table for new data for processing. Use TTL to remove data that has been processed."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 728,
    "question": {
      "kor": "솔루션 설계자는 은행에 대한 신용 카드 데이터 유효성 검사 요청을 처리하기 위해 비동기식 애플리케이션을 설계하고 있습니다. 애플리케이션은 안전해야 하며 각 요청을 한 번 이상 처리할 수 있어야 합니다.\n이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?",
      "eng": "A solutions architect is designing an asynchronous application to process credit card data validation requests for a bank. The application must be secure and be able to process each request at least once.\nWhich solution will meet these requirements MOST cost-effectively?"
    },
    "choices": {
      "kor": {
        "A": "AWS Lambda 이벤트 소스 매핑을 사용하십시오. Amazon Simple Queue Service(Amazon SQS) 표준 대기열을 이벤트 소스로 설정합니다. 암호화에 AWS Key Management Service(SSE-KMS)를 사용합니다. Lambda 실행 역할에 대한 kms:Decrypt 권한을 추가합니다.",
        "B": "AWS Lambda 이벤트 소스 매핑을 사용합니다. Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 이벤트 소스로 사용합니다. 암호화에 SQS 관리형 암호화 키(SSE-SQS)를 사용합니다. Lambda 함수에 대한 암호화 키 호출 권한을 추가합니다.",
        "C": "AWS Lambda 이벤트 소스 매핑을 사용합니다. Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 이벤트 소스로 설정합니다. AWS KMS 키(SSE-KMS)를 사용합니다. Lambda 실행 역할에 대한 kms:Decrypt 권한을 추가합니다.",
        "D": "AWS Lambda 이벤트 소스 매핑을 사용합니다. Amazon Simple Queue Service(Amazon SQS) 표준 대기열을 이벤트 소스로 설정합니다. 암호화에 AWS KMS 키(SSE-KMS)를 사용합니다. Lambda 함수에 대한 암호화 키 호출 권한을 추가합니다."
      },
      "eng": {
        "A": "Use AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS Key Management Service (SSE-KMS) for encryption. Add the kms:Decrypt permission for the Lambda execution role.",
        "B": "Use AWS Lambda event source mapping. Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use SQS managed encryption keys (SSE-SQS) for encryption. Add the encryption key invocation permission for the Lambda function.",
        "C": "Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use AWS KMS keys (SSE-KMS). Add the kms:Decrypt permission for the Lambda execution role.",
        "D": "Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS KMS keys (SSE-KMS) for encryption. Add the encryption key invocation permission for the Lambda function."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/109513-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 743,
    "question": {
      "kor": "회사는 퍼블릭 및 프라이빗 서브넷이 있는 VPC에서 애플리케이션을 실행합니다. VPC는 여러 가용 영역에 걸쳐 확장됩니다. 애플리케이션은 프라이빗 서브넷의 Amazon EC2 인스턴스에서 실행됩니다. 애플리케이션은 Amazon Simple Queue Service(Amazon SQS) 대기열을 사용합니다.\n솔루션 설계자는 EC2 인스턴스와 SQS 대기열 간의 연결을 설정하기 위한 보안 솔루션을 설계해야 합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company runs an application in a VPC with public and private subnets. The VPC extends across multiple Availability Zones. The application runs on Amazon EC2 instances in private subnets. The application uses an Amazon Simple Queue Service (Amazon SQS) queue.\nA solutions architect needs to design a secure solution to establish a connection between the EC2 instances and the SQS queue.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Amazon SQS용 인터페이스 VPC 엔드포인트를 구현합니다. 프라이빗 서브넷을 사용하도록 엔드포인트를 구성합니다. 프라이빗 서브넷에 있는 EC2 인스턴스의 트래픽을 허용하는 인바운드 액세스 규칙이 있는 보안 그룹을 엔드포인트에 추가합니다.",
        "B": "Amazon SQS용 인터페이스 VPC 엔드포인트를 구현합니다. 퍼블릭 서브넷을 사용하도록 엔드포인트를 구성합니다. 프라이빗 서브넷에 있는 EC2 인스턴스의 액세스를 허용하는 VPC 엔드포인트 정책을 인터페이스 엔드포인트에 연결합니다.",
        "C": "Amazon SQS용 인터페이스 VPC 엔드포인트를 구현합니다. 퍼블릭 서브넷을 사용하도록 엔드포인트를 구성합니다. 지정된 VPC 엔드포인트의 요청만 허용하는 인터페이스 VPC 엔드포인트에 Amazon SQS 액세스 정책을 연결합니다.",
        "D": "Amazon SQS용 게이트웨이 엔드포인트를 구현합니다. 프라이빗 서브넷에 NAT 게이트웨이를 추가합니다. SQS 대기열에 대한 액세스를 허용하는 EC2 인스턴스에 IAM 역할을 연결 합니다."
      },
      "eng": {
        "A": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the private subnets. Add to the endpoint a security group that has an inbound access rule that allows traffic from the EC2 instances that are in the private subnets.",
        "B": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public subnets. Attach to the interface endpoint a VPC endpoint policy that allows access from the EC2 instances that are in the private subnets.",
        "C": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public subnets. Attach an Amazon SQS access policy to the interface VPC endpoint that allows requests from only a specified VPC endpoint.",
        "D": "Implement a gateway endpoint for Amazon SQS. Add a NAT gateway to the private subnets. Attach an IAM role to the EC2 instances that allows access to the SQS queue."
      }
    },
    "category": [
      "SQS",
      "VPC endpoint"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/116983-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 761,
    "question": {
      "kor": "한 회사가 AWS 클라우드에 웹 애플리케이션을 보유하고 있으며 거래 데이터를 실시간으로 수집하려고 합니다. 회사에서는 데이터 중복을 방지하고 인프라 관리를 원하지 않습니다. 회사는 데이터가 수집된 후 해당 데이터에 대해 추가 처리를 수행하려고 합니다.\n어떤 솔루션이 이러한 요구 사항을 충족합니까?",
      "eng": "A company has a web application in the AWS Cloud and wants to collect transaction data in real time. The company wants to prevent data duplication and does not want to manage infrastructure. The company wants to perform additional processing on the data after the data is collected.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 구성합니다. 데이터를 처리하기 위해 FIFO 대기열에 대한 이벤트 소스 매핑으로 AWS Lambda 함수를 구성 합니다.",
        "B": "Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 구성합니다. AWS Batch 작업을 사용하여 대기열에서 중복 데이터를 제거합니다. 데이터를 처리하도록 AWS Lambda 함수를 구성합니다.",
        "C": "Amazon Kinesis Data Streams를 사용하여 수신 트랜잭션 데이터를 중복 데이터를 제거하는 AWS Batch 작업으로 보냅니다. 데이터를 처리하기 위해 사용자 지정 스크립트를 실행하는 Amazon EC2 인스턴스를 시작합니다.",
        "D": "중복 데이터를 제거하기 위해 들어오는 트랜잭션 데이터를 AWS Lambda 함수로 보내도록 AWS Step Functions 상태 머신을 설정합니다. 데이터를 처리하기 위해 사용자 지정 스크립트를 실행하는 Amazon EC2 인스턴스를 시작합니다."
      },
      "eng": {
        "A": "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Configure an AWS Lambda function with an event source mapping for the FIFO queue to process the data.",
        "B": "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Use an AWS Batch job to remove duplicate data from the queue. Configure an AWS Lambda function to process the data.",
        "C": "Use Amazon Kinesis Data Streams to send the incoming transaction data to an AWS Batch job that removes duplicate data. Launch an Amazon EC2 instance that runs a custom script to process the data.",
        "D": "Set up an AWS Step Functions state machine to send incoming transaction data to an AWS Lambda function to remove duplicate data. Launch an Amazon EC2 instance that runs a custom script to process the data."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 810,
    "question": {
      "kor": "전자상거래 회사는 온프레미스 웨어하우스 솔루션과 통합된 AWS 클라우드에서 애플리케이션을 실행합니다. 이 회사는 Amazon Simple Notification Service(Amazon SNS)를 사용하여 주문 메시지를 온프레미스 HTTPS 엔드포인트로 보내 창고 애플리케이션이 주문을 처리할 수 있도록 합니다. 로컬 데이터 센터 팀에서 일부 주문 메시지가 수신되지 않은 것을 감지했습니다.\n솔루션 설계자는 전달되지 않은 메시지를 보관하고 최대 14일 동안 메시지를 분석해야 합니다.\n최소한의 개발 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "An ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse solution. The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application can process the orders. The local data center team has detected that some of the order messages were not received.\nA solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days.\nWhich solution will meet these requirements with the LEAST development effort?"
    },
    "choices": {
      "kor": {
        "A": "보존 기간이 14일인 Amazon Kinesis Data Stream 대상이 있는 Amazon SNS 배달 못한 편지 대기열을 구성합니다.",
        "B": "애플리케이션과 Amazon SNS 사이에 보존 기간이 14일인 Amazon Simple Queue Service(Amazon SQS) 대기열을 추가합니다.",
        "C": "보존 기간이 14일인 Amazon Simple Queue Service(Amazon SQS) 대상이 있는 Amazon SNS 데드 레터 대기열을 구성합니다.",
        "D": "보존 기간이 14일로 설정된 TTL 속성이 있는 Amazon DynamoDB 대상이 있는 Amazon SNS 데드 레터 대기열을 구성합니다."
      },
      "eng": {
        "A": "Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target with a retention period of 14 days.",
        "B": "Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days between the application and Amazon SNS.",
        "C": "Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.",
        "D": "Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL attribute set for a retention period of 14 days."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [
      "DLQ"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/109637-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 790,
    "question": {
      "kor": "회사에는 Amazon Simple Queue Service(Amazon SQS)를 사용하여 메시지를 구문 분석하는 Java 애플리케이션이 있습니다. 애플리케이션은 크기가 256KB보다 큰 메시지를 구문 분석할 수 없습니다. 회사는 응용 프로그램이 50MB만큼 큰 메시지를 구문 분석할 수 있는 기능을 제공하는 솔루션을 구현하려고 합니다.\n코드를 가장 적게 변경하여 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB.\nWhich solution will meet these requirements with the FEWEST changes to the code?"
    },
    "choices": {
      "kor": {
        "A": "Java용 Amazon SQS 확장 클라이언트 라이브러리를 사용하여 Amazon S3에서 256KB보다 큰 메시지를 호스팅합니다.",
        "B": "Amazon SQS 대신 Amazon EventBridge를 사용하여 애플리케이션에서 큰 메시지를 게시합니다.",
        "C": "256KB보다 큰 메시지를 처리하도록 Amazon SQS의 제한을 변경합니다.",
        "D": "Amazon Elastic File System(Amazon EFS)에 256KB보다 큰 메시지를 저장합니다. 메시지에서 이 위치를 참조하도록 Amazon SQS를 구성합니다."
      },
      "eng": {
        "A": "Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.",
        "B": "Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.",
        "C": "Change the limit in Amazon SQS to handle messages that are larger than 256 KB.",
        "D": "Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [
      "Amazon SQS Extended Client Library for Java"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/100202-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 920,
    "question": {
      "kor": "솔루션 아키텍트는 여러 가용 영역에 걸쳐 프라이빗 서브넷의 AWS Lambda에서 실행되는 결제 처리 애플리케이션을 설계하고 있습니다. 이 애플리케이션은 여러 Lambda 함수를 사용하고 매일 수백만 건의 트랜잭션을 처리합니다.\n아키텍처는 애플리케이션이 중복 결제를 처리하지 않도록 보장해야 합니다.\n어떤 솔루션이 이러한 요구 사항을 충족합니까?",
      "eng": "A solutions architect is designing a payment processing application that runs on AWS Lambda in private subnets across multiple Availability Zones. The application uses multiple Lambda functions and processes millions of transactions each day.\nThe architecture must ensure that the application does not process duplicate payments.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Lambda를 사용하여 모든 지불 금액을 검색하십시오. Amazon S3 버킷에 지불 기한을 게시합니다. 기한 지불을 처리하기 위해 다른 Lambda 함수를 호출하도록 이벤트 알림으로 S3 버킷을 구성합니다.",
        "B": "Lambda를 사용하여 모든 지불 금액을 검색합니다. Amazon Simple Queue Service(Amazon SQS) 대기열에 지불 기한을 게시합니다. SQS 대기열을 폴링하고 기한 지불을 처리하도록 다른 Lambda 함수를 구성합니다.",
        "C": "Lambda를 사용하여 모든 지불 금액을 검색합니다. Amazon Simple Queue Service(Amazon SQS) FIFO 대기열에 지불 기한을 게시합니다. FIFO 대기열을 폴링하고 기한 지불을 처리하도록 다른 Lambda 함수를 구성합니다.",
        "D": "Lambda를 사용하여 모든 지불 금액을 검색합니다. Amazon DynamoDB 테이블에 지불 기한을 저장합니다. 기한 지불을 처리하기 위해 다른 Lambda 함수를 호출하도록 DynamoDB 테이블의 스트림을 구성합니다."
      },
      "eng": {
        "A": "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon S3 bucket. Configure the S3 bucket with an event notification to invoke another Lambda function to process the due payments.",
        "B": "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple Queue Service (Amazon SQS) queue. Configure another Lambda function to poll the SQS queue and to process the due payments.",
        "C": "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Configure another Lambda function to poll the FIFO queue and to process the due payments.",
        "D": "Use Lambda to retrieve all due payments. Store the due payments in an Amazon DynamoDB table. Configure streams on the DynamoDB table to invoke another Lambda function to process the due payments."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/133021-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 957,
    "question": {
      "kor": "한 회사에서 AWS에 배포된 전자상거래 주문 처리 애플리케이션을 개선하려고 합니다. 애플리케이션은 예측할 수 없는 트래픽 급증 중에 고객 경험에 영향을 주지 않고 각 주문을 정확히 한 번만 처리해야 합니다.\n어떤 솔루션이 이러한 요구 사항을 충족합니까?",
      "eng": "A company wants to enhance its ecommerce order-processing application that is deployed on AWS. The application must process each order exactly once without affecting the customer experience during unpredictable traffic surges.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 생성합니다. 모든 주문을 SQS 대기열에 넣습니다. 주문을 처리할 대상으로 AWS Lambda 함수를 구성합니다.",
        "B": "Amazon Simple 알림 서비스(Amazon SNS) 표준 주제를 생성합니다. 모든 주문을 SNS 표준 주제에 게시합니다. 애플리케이션을 알림 대상으로 구성합니다.",
        "C": "Amazon AppFlow를 사용하여 흐름을 생성합니다. 주문을 흐름으로 보냅니다. 주문을 처리할 대상으로 AWS Lambda 함수를 구성합니다.",
        "D": "주문 요청을 추적하도록 애플리케이션에서 AWS X-Ray를 구성합니다. Amazon CloudWatch에서 주문을 가져와 주문을 처리하도록 애플리케이션을 구성합니다."
      },
      "eng": {
        "A": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Put all the orders in the SQS queue. Configure an AWS Lambda function as the target to process the orders.",
        "B": "Create an Amazon Simple Notification Service (Amazon SNS) standard topic. Publish all the orders to the SNS standard topic. Configure the application as a notification target.",
        "C": "Create a flow by using Amazon AppFlow. Send the orders to the flow. Configure an AWS Lambda function as the target to process the orders.",
        "D": "Configure AWS X-Ray in the application to track the order requests. Configure the application to process the orders by pulling the orders from Amazon CloudWatch."
      }
    },
    "category": [
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/138082-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 294,
    "question": {
      "kor": "한 회사에서 AWS를 사용하여 보험 견적을 처리할 웹 애플리케이션을 설계하고 있습니다. 사용자는 애플리케이션에서 견적을 요청합니다. 견적은 견적 유형별로 구분되어야 하며, 24시간 이내에 응답해야 하며 분실해서는 안 됩니다. 솔루션은 운영 효율성을 극대화하고 유지 보수를 최소화해야 합니다.\n어떤 솔루션이 이러한 요구 사항을 충족합니까?",
      "eng": "A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational efficiency and must minimize maintenance.\nWhich solution meets these requirements?"
    },
    "choices": {
      "kor": {
        "A": "견적 유형에 따라 여러 Amazon Kinesis 데이터 스트림을 생성합니다. 적절한 데이터 스트림으로 메시지를 보내도록 웹 애플리케이션을 구성합니다. Kinesis Client Library(KCL)를 사용하여 자체 데이터 스트림에서 메시지를 풀링하도록 애플리케이션 서버의 각 백엔드 그룹을 구성합니다.",
        "B": "각 견적 유형에 대해 AWS Lambda 함수 및 Amazon Simple Notification Service(Amazon SNS) 주제를 생성합니다. 연결된 SNS 주제에 Lambda 함수를 구독합니다. 견적 요청을 적절한 SNS 주제에 게시하도록 애플리케이션을 구성합니다.",
        "C": "단일 Amazon Simple Notification Service(Amazon SNS) 주제를 생성합니다. SNS 주제에 대한 Amazon Simple Queue Service(Amazon SQS) 대기열을 구독합니다. 견적 유형에 따라 적절한 SQS 대기열에 메시지를 게시하도록 SNS 메시지 필터링을 구성합니다. 자체 SQS 대기열을 사용하도록 각 백엔드 애플리케이션 서버를 구성합니다.",
        "D": "데이터 스트림을 Amazon OpenSearch Service 클러스터로 전달하기 위해 견적 유형을 기반으로 여러 Amazon Kinesis Data Firehose 전달 스트림을 생성합니다. 적절한 전송 스트림으로 메시지를 보내도록 애플리케이션을 구성합니다. OpenSearch Service에서 메시지를 검색하고 그에 따라 처리하도록 애플리케이션 서버의 각 백엔드 그룹을 구성합니다."
      },
      "eng": {
        "A": "Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.",
        "B": "Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic for each quote type. Subscribe the Lambda function to its associated SNS topic. Configure the application to publish requests for quotes to the appropriate SNS topic.",
        "C": "Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.",
        "D": "Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an Amazon OpenSearch Service cluster. Configure the application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from OpenSearch Service and process them accordingly."
      }
    },
    "category": [
      "SNS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/99627-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 313,
    "question": {
      "kor": "한 병원에서 환자의 증상을 수집하는 새로운 애플리케이션을 설계하고 있습니다. 병원은 아키텍처에서 Amazon Simple Queue Service(Amazon SQS)와 Amazon Simple Notification Service(Amazon SNS)를 사용하기로 결정했습니다.\n솔루션 설계자가 인프라 디자인을 검토하고 있습니다. 저장 및 전송 중에 데이터를 암호화해야 합니다. 병원의 승인된 직원만 데이터에 액세스할 수 있어야 합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까? (두 가지를 선택하세요.)",
      "eng": "A hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the architecture.\nA solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only authorized personnel of the hospital should be able to access the data.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "SQS 구성 요소에서 서버 측 암호화를 켭니다. 기본 키 정책을 업데이트하여 인증된 보안 주체 집합으로 키 사용을 제한합니다.",
        "B": "AWS Key Management Service(AWS KMS) 고객 관리 키를 사용하여 SNS 구성 요소에서 서버 측 암호화를 켭니다. 키 정책을 적용하여 인증된 보안 주체 집합으로 키 사용을 제한합니다.",
        "C": "SNS 구성 요소에서 암호화를 켭니다. 기본 키 정책을 업데이트하여 인증된 보안 주체 집합으로 키 사용을 제한합니다. TLS를 통한 암호화된 연결만 허용하도록 주제 정책에서 조건을 설정합니다.",
        "D": "AWS Key Management Service(AWS KMS) 고객 관리 키를 사용하여 SQS 구성 요소에서 서버 측 암호화를 켭니다. 키 정책을 적용하여 인증된 보안 주체 집합으로 키 사용을 제한합니다. TLS를 통한 암호화된 연결만 허용하도록 대기열 정책에서 조건을 설정합니다.",
        "E": "AWS Key Management Service(AWS KMS) 고객 관리 키를 사용하여 SQS 구성 요소에서 서버 측 암호화를 켭니다. IAM 정책을 적용하여 인증된 보안 주체 집합으로 키 사용을 제한합니다. TLS를 통한 암호화된 연결만 허용하도록 대기열 정책에서 조건을 설정합니다."
      },
      "eng": {
        "A": "Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage to a set of authorized principals.",
        "B": "Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.",
        "C": "Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of authorized principals. Set a condition in the topic policy to allow only encrypted connections over TLS.",
        "D": "Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.",
        "E": "Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply an IAM policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS."
      }
    },
    "category": [
      "SNS",
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/102125-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B",
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 792,
    "question": {
      "kor": "회사는 고유한 이벤트를 별도의 리더보드, 매치메이킹 및 인증 서비스로 동시에 전송해야 하는 게임 시스템을 구축하고 있습니다. 회사에는 이벤트 순서를 보장하는 AWS 이벤트 기반 시스템이 필요합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company is building a game system that needs to send unique events to separate leaderboard, matchmaking, and authentication services concurrently.\nThe company needs an AWS event-driven system that guarantees the order of the events.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Amazon EventBridge 이벤트 버스",
        "B": "Amazon Simple Notification Service(Amazon SNS) FIFO 주제",
        "C": "Amazon Simple Notification Service(Amazon SNS) 표준 주제",
        "D": "Amazon Simple Queue Service(Amazon SQS) FIFO 대기열"
      },
      "eng": {
        "A": "Amazon EventBridge event bus",
        "B": "Amazon Simple Notification Service (Amazon SNS) FIFO topics",
        "C": "Amazon Simple Notification Service (Amazon SNS) standard topics",
        "D": "Amazon Simple Queue Service (Amazon SQS) FIFO queues"
      }
    },
    "category": [
      "SNS"
    ],
    "subcategory": [
      "FIFO"
    ],
    "rote_memorization": true,
    "reference": "https://www.examtopics.com/discussions/amazon/view/102124-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 860,
    "question": {
      "kor": "개발 팀은 AWS Lambda 기능을 사용하는 이벤트 기반 애플리케이션을 만들고 있습니다. Amazon S3 버킷에 파일이 추가되면 이벤트가 생성됩니다. 개발 팀은 현재 Amazon S3의 이벤\n트 대상으로 구성된 Amazon Simple 알림 서비스(Amazon SNS)를 보유하고 있습니다.\n확장 가능한 방식으로 Amazon S3의 이벤트를 처리하려면 솔루션 아키텍트가 무엇을 해야 합니까?",
      "eng": "A development team is creating an event-based application that uses AWS Lambda functions. Events will be generated when files are added to an Amazon S3 bucket. The development team currently has Amazon Simple Notification Service (Amazon SNS) configured as the event target from Amazon S3.\nWhat should a solutions architect do to process the events from Amazon S3 in a scalable way?"
    },
    "choices": {
      "kor": {
        "A": "이벤트가 Lambda에서 실행되기 전에 Amazon Elastic Container Service(Amazon ECS)에서 이벤트를 처리하는 SNS 구독을 생성하십시오.",
        "B": "이벤트가 Lambda에서 실행되기 전에 Amazon Elastic Kubernetes Service(Amazon EKS)에서 이벤트를 처리하는 SNS 구독을 생성합니다.",
        "C": "이벤트를 Amazon Simple Queue Service(Amazon SQS)로 보내는 SNS 구독을 생성합니다. Lambda 함수를 트리거하도록 SOS 대기열을 구성합니다.",
        "D": "이벤트를 AWS Server Migration Service(AWS SMS)로 보내는 SNS 구독을 생성합니다. SMS 이벤트에서 폴링하도록 Lambda 함수를 구성합니다."
      },
      "eng": {
        "A": "Create an SNS subscription that processes the event in Amazon Elastic Container Service (Amazon ECS) before the event runs in Lambda.",
        "B": "Create an SNS subscription that processes the event in Amazon Elastic Kubernetes Service (Amazon EKS) before the event runs in Lambda",
        "C": "Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon SQS). Configure the SOS queue to trigger a Lambda function.",
        "D": "Create an SNS subscription that sends the event to AWS Server Migration Service (AWS SMS). Configure the Lambda function to poll from the SMS event."
      }
    },
    "category": [
      "SNS",
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/125546-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 924,
    "question": {
      "kor": "한 소셜 미디어 회사가 사용자를 위한 보상 프로그램 웹사이트를 만들고 있습니다. 회사는 이용자가 동영상을 제작하여 홈페이지에 업로드할 때 이용자에게 포인트를 부여합니다. 사용자는 자신의 포인트를 회사 제휴 파트너의 선물이나 할인으로 교환할 수 있습니다. 고유 ID는 사용자를 식별합니다. 파트너는 이 ID를 참조하여 사용자의 보상 자격을 확인합니다.\n파트너는 회사가 사용자에게 포인트를 제공할 때 HTTP 엔드포인트를 통해 사용자 ID에 대한 알림을 받기를 원합니다. 매일 수백 개의 공급업체가 제휴 파트너가 되는 데 관심을 갖고 있습니다. 회사는 확장 가능한 방식으로 신속하게 파트너를 추가할 수 있는 기능을 웹 사이트에 제공하는 아키텍처를 설계하려고 합니다.\n최소한의 구현 노력으로 이러한 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?",
      "eng": "A social media company is creating a rewards program website for its users. The company gives users points when users create and upload videos to the website. Users redeem their points for gifts or discounts from the company's affiliated partners. A unique ID identifies users. The partners refer to this ID to verify user eligibility for rewards.\nThe partners want to receive notification of user IDs through an HTTP endpoint when the company gives users points. Hundreds of vendors are interested in becoming affiliated partners every day. The company wants to design an architecture that gives the website the ability to add partners rapidly in a scalable way.\nWhich solution will meet these requirements with the LEAST implementation effort?"
    },
    "choices": {
      "kor": {
        "A": "제휴 파트너 목록을 보관하려면 Amazon Timestream 데이터베이스를 생성하세요. 목록을 읽으려면 AWS Lambda 함수를 구현하십시오. 회사에서 사용자에게 포인트를 제공할 때 각 파트너에게 사용자 ID를 보내도록 Lambda 기능을 구성합니다.",
        "B": "Amazon Simple 알림 서비스(Amazon SNS) 주제를 생성합니다. 엔드포인트 프로토콜을 선택하세요. 주제에 대한 파트너를 구독하십시오. 회사가 사용자에게 포인트를 제공할 때 해당 주제에 사용자 ID를 게시합니다.",
        "C": "AWS Step Functions 상태 머신을 생성합니다. 모든 제휴 파트너에 대한 작업을 만듭니다. 회사가 사용자에게 포인트를 제공할 때 사용자 ID를 입력으로 사용하여 상태 머신을 호출합니다.",
        "D": "Amazon Kinesis Data Streams에서 데이터 스트림을 생성합니다. 생산자 및 소비자 애플리케이션을 구현합니다. 데이터 스트림에 제휴 파트너 목록을 저장합니다. 회사가 이용자에게 포인트를 지급할 때 이용자 ID를 발송합니다."
      },
      "eng": {
        "A": "Create an Amazon Timestream database to keep a list of affiliated partners. Implement an AWS Lambda function to read the list. Configure the Lambda function to send user IDs to each partner when the company gives users points.",
        "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Choose an endpoint protocol. Subscribe the partners to the topic. Publish user IDs to the topic when the company gives users points.",
        "C": "Create an AWS Step Functions state machine. Create a task for every affiliated partner. Invoke the state machine with user IDs as input when the company gives users points.",
        "D": "Create a data stream in Amazon Kinesis Data Streams. Implement producer and consumer applications. Store a list of affiliated partners in the data stream. Send user IDs when the company gives users points."
      }
    },
    "category": [
      "SNS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/133037-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 81,
    "question": {
      "kor": "이미지 처리 회사에는 사용자가 이미지를 업로드하는 데 사용하는 웹 애플리케이션이 있습니다. 애플리케이션은 이미지를 Amazon S3 버킷에 업로드합니다. 회사는 객체 생성 이벤트를 Amazon Simple Queue Service(Amazon SQS) 표준 대기열에 게시하도록 S3 이벤트 알림을 설정했습니다. SQS 대기열은 이미지를 처리하고 이메일을 통해 사용자에게 결과를 보내는 AWS Lambda 함수의 이벤트 소스 역할을 합니다.\n사용자는 업로드된 모든 이미지에 대해 여러 개의 이메일 메시지를 받고 있다고 보고합니다. 솔루션 아키텍트는 SQS 메시지가 Lambda 함수를 두 번 이상 호출하여 여러 이메일 메시지가 생성되었음을 확인합니다.\n최소한의 운영 오버헤드로 이 문제를 해결하려면 솔루션 설계자가 무엇을 해야 합니까?",
      "eng": "An image-processing company has a web application that users use to upload images. The application uploads the images into an Amazon S3 bucket. The company has set up S3 event notifications to publish the object creation events to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function that processes the images and sends the results to users through email.\nUsers report that they are receiving multiple email messages for every uploaded image. A solutions architect determines that SQS messages are invoking the Lambda function more than once, resulting in multiple email messages.\nWhat should the solutions architect do to resolve this issue with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "ReceiveMessage 대기 시간을 30초로 늘려 SQS 대기열에서 긴 폴링을 설정합니다.",
        "B": "SQS 표준 대기열을 SQS FIFO 대기열로 변경합니다. 중복 메시지를 삭제하려면 메시지 중복 제거 ID를 사용하십시오.",
        "C": "SQS 대기열의 가시성 제한 시간을 기능 제한 시간과 배치 창 제한 시간의 합계보다 큰 값으로 늘립니다.",
        "D": "처리하기 전에 메시지를 읽은 직후 SQS 대기열에서 각 메시지를 삭제하도록 Lambda 함수를 수정합니다."
      },
      "eng": {
        "A": "Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds.",
        "B": "Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard duplicate messages.",
        "C": "Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout.",
        "D": "Modify the Lambda function to delete each message from the SQS queue immediately after the message is read before processing."
      }
    },
    "category": [
      "Messaging"
    ],
    "subcategory": [
      "SQS",
      "Visibility timeout"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85185-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 15,
    "question": {
      "kor": "회사에 수신 메시지를 수집하는 애플리케이션이 있습니다. 그러면 수십 개의 다른 애플리케이션과 마이크로서비스가 이러한 메시지를 빠르게 사용합니다. 메시지 수는 매우 다양하며 때로는 초당 100,000개로 갑자기 증가하기도 합니다. 회사는 솔루션을 분리하고 확장성을 높이고자 합니다.\n어떤 솔루션이 이러한 요구 사항을 충족합니까?",
      "eng": "A company has an application that ingests incoming messages. Dozens of other applications and microservices then quickly consume these messages. The number of messages varies drastically and sometimes increases suddenly to 100,000 each second. The company wants to decouple the solution and increase scalability.\nWhich solution meets these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis Data Analytics에 대한 메시지를 유지합니다. 메시지를 읽고 처리하도록 소비자 애플리케이션을 구성합니다.",
        "B": "Auto Scaling 그룹의 Amazon EC2 인스턴스에 수집 애플리케이션을 배포하여 CPU 지표를 기반으로 EC2 인스턴스 수를 확장합니다.",
        "C": "단일 샤드로 Amazon Kinesis Data Streams에 메시지를 씁니다. AWS Lambda 함수를 사용하여 메시지를 사전 처리하고 Amazon DynamoDB에 저장합니다. 메시지를 처리하기 위해 DynamoDB에서 읽을 소비자 애플리케이션을 구성합니다.",
        "D": "여러 Amazon Simple Queue Service(Amazon SQS) 구독이 있는 Amazon Simple Notification Service(Amazon SNS) 주제에 메시지를 게시합니다. 대기열의 메시지를 처리하도록 소비자 애플리케이션을 구성합니다."
      },
      "eng": {
        "A": "Persist the messages to Amazon Kinesis Data Analytics. Configure the consumer applications to read and process the messages.",
        "B": "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale the number of EC2 instances based on CPU metrics.",
        "C": "Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to preprocess messages and store them in Amazon DynamoDB. Configure the consumer applications to read from DynamoDB to process the messages.",
        "D": "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SQS) subscriptions. Configure the consumer applications to process the messages from the queues."
      }
    },
    "category": [
      "Messaging"
    ],
    "subcategory": [
      "SNS",
      "SQS"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/84721-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 65,
    "question": {
      "kor": "회사에는 다음으로 구성된 데이터 수집 워크플로가 있습니다.\n* 새로운 데이터 전달에 대한 알림을 위한 Amazon Simple Notification Service(Amazon SNS) 주제\n* 데이터를 처리하고 메타데이터를 기록하는 AWS Lambda 함수\n수집 워크플로가 실패하는 것을 회사에서 관찰합니다. 때때로 네트워크 연결 문제로 인해. 이러한 실패가 발생하면 회사에서 수동으로 작업을 다시 실행하지 않는 한 Lambda 함수는 해당 데이터를 수집하지 않습니다.\nLambda 함수가 미래에 모든 데이터를 수집하도록 하기 위해 솔루션 설계자는 어떤 작업 조합을 취해야 합니까? (두 가지를 선택하세요.)",
      "eng": "A company has a data ingestion workflow that consists of the following: * An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data deliveries * An AWS Lambda function to process the data and record metadata\nThe company observes that the ingestion workflow fails occasionally because of network connectivity issues. When such a failure occurs, the Lambda function does not ingest the corresponding data unless the company manually reruns the job.\nWhich combination of actions should a solutions architect take to ensure that the Lambda function ingests all data in the future? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "여러 가용 영역에 Lambda 함수를 배포합니다.",
        "B": "Amazon Simple Queue Service(Amazon SQS) 대기열을 생성하고 SNS 주제를 구독합니다.",
        "C": "Lambda 함수에 할당된 CPU와 메모리를 늘립니다.",
        "D": "Lambda 함수에 대해 프로비저닝된 처리량을 늘립니다.",
        "E": "Amazon Simple Queue Service(Amazon SQS) 대기열에서 읽도록 Lambda 함수를 수정합니다."
      },
      "eng": {
        "A": "Deploy the Lambda function in multiple Availability Zones.",
        "B": "Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.",
        "C": "Increase the CPU and memory that are allocated to the Lambda function.",
        "D": "Increase provisioned throughput for the Lambda function.",
        "E": "Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue."
      }
    },
    "category": [
      "Messaging"
    ],
    "subcategory": [
      "Lambda",
      "SQS"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85408-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B",
      "E"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 72,
    "question": {
      "kor": "회사에서 애플리케이션을 설계하고 있습니다. 애플리케이션은 AWS Lambda 함수를 사용하여 Amazon API Gateway를 통해 정보를 수신하고 Amazon Aurora PostgreSQL 데이터베이스에 정보를 저장합니다.\n개념 증명 단계에서 회사는 데이터베이스에 로드해야 하는 대량의 데이터를 처리하기 위해 Lambda 할당량을 크게 늘려야 합니다. 솔루션 설계자는 확장성을 개선하고 구성 노력을 최소화하기 위해 새로운 설계를 권장해야 합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company is designing an application. The application uses an AWS Lambda function to receive information through Amazon API Gateway and to store the information in an Amazon Aurora PostgreSQL database.\nDuring the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the high volumes of data that the company needs to load into the database. A solutions architect must recommend a new design to improve scalability and minimize the configuration effort.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Lambda 함수 코드를 Amazon EC2 인스턴스에서 실행되는 Apache Tomcat 코드로 리팩터링합니다. 원시 JDBC(Java Database Connectivity) 드라이버를 사용하여 데이터베이스를 연결하십시오.",
        "B": "Aurora에서 Amazon DynamoDB로 플랫폼을 변경합니다. DynamoDB Accelerator(DAX) 클러스터를 프로비저닝합니다. DAX 클라이언트 SDK를 사용하여 DAX 클러스터에서 기존 DynamoDB API 호출을 가리킵니다.",
        "C": "두 개의 Lambda 함수를 설정합니다. 정보를 수신하도록 하나의 기능을 구성합니다. 데이터베이스에 정보를 로드하도록 다른 기능을 구성하십시오. Amazon Simple Notification Service(Amazon SNS)를 사용하여 Lambda 함수를 통합합니다.",
        "D": "두 개의 Lambda 함수를 설정합니다. 정보를 수신하도록 하나의 기능을 구성합니다. 데이터베이스에 정보를 로드하도록 다른 기능을 구성하십시오. Amazon Simple Queue Service(Amazon SQS) 대기열을 사용하여 Lambda 함수를 통합합니다."
      },
      "eng": {
        "A": "Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity (JDBC) drivers.",
        "B": "Change the platform from Aurora to Amazon DynamoDB. Provision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster.",
        "C": "Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using Amazon Simple Notification Service (Amazon SNS).",
        "D": "Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue."
      }
    },
    "category": [
      "Messaging"
    ],
    "subcategory": [
      "Lambda",
      "SQS"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85197-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 331,
    "question": {
      "kor": "회사는 특정 지불 ID에 대한 메시지가 전송된 순서대로 수신되어야 하는 지불 처리 시스템을 사용합니다. 그렇지 않으면 결제가 잘못 처리될 수 있습니다.\n솔루션 설계자는 이 요구 사항을 충족하기 위해 어떤 조치를 취해야 합니까? (두 가지를 선택하세요.)",
      "eng": "A company uses a payment processing system that requires messages for a particular payment ID to be received in the same order that they were sent.\nOtherwise, the payments might be processed incorrectly.\nWhich actions should a solutions architect take to meet this requirement? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "결제 ID를 파티션 키로 사용하여 Amazon DynamoDB 테이블에 메시지를 씁니다.",
        "B": "결제 ID를 파티션 키로 사용하여 Amazon Kinesis 데이터 스트림에 메시지를 씁니다.",
        "C": "결제 ID를 키로 사용하여 Amazon ElastiCache for Memcached 클러스터에 메시지를 씁니다.",
        "D": "Amazon Simple Queue Service(Amazon SQS) 대기열에 메시지를 씁니다. 결제 ID를 사용하도록 메시지 속성을 설정합니다.",
        "E": "Amazon Simple Queue Service(Amazon SQS) FIFO 대기열에 메시지를 씁니다. 결제 ID를 사용할 메시지 그룹을 설정합니다."
      },
      "eng": {
        "A": "Write the messages to an Amazon DynamoDB table with the payment ID as the partition key.",
        "B": "Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.",
        "C": "Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as the key.",
        "D": "Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue. Set the message attribute to use the payment ID.",
        "E": "Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID."
      }
    },
    "category": [
      "Messaging"
    ],
    "subcategory": [
      "Kinesis",
      "SQS"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/102121-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B",
      "E"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 297,
    "question": {
      "kor": "한 회사가 AWS 클라우드에서 3계층 전자상거래 애플리케이션을 호스팅하고 있습니다. 회사는 Amazon S3에서 웹사이트를 호스팅하고 웹사이트를 판매 요청을 처리하는 API와 통합합니다. 이 회사는 ALB(Application Load Balancer) 뒤에 있는 3개의 Amazon EC2 인스턴스에서 API를 호스팅합니다. API는 판매 요청을 비동기식으로 처리하는 백엔드 작업자와 함께 정적 및 동적 프런트 엔드 콘텐츠로 구성됩니다.\n회사는 신제품 출시 이벤트 기간 동안 판매 요청 건수가 급격하게 급증할 것으로 예상하고 있다.\n모든 요청이 성공적으로 처리되도록 하려면 솔루션 설계자가 무엇을 권장해야 합니까?",
      "eng": "A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously.\nThe company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products.\nWhat should a solutions architect recommend to ensure that all the requests are processed successfully?"
    },
    "choices": {
      "kor": {
        "A": "동적 콘텐츠에 대한 Amazon CloudFront 배포를 추가합니다. 트래픽 증가를 처리하기 위해 EC2 인스턴스 수를 늘립니다.",
        "B": "정적 콘텐츠에 대한 Amazon CloudFront 배포를 추가합니다. Auto Scaling 그룹에 EC2 인스턴스를 배치하여 네트워크 트래픽을 기반으로 새 인스턴스를 시작합니다.",
        "C": "동적 콘텐츠에 대한 Amazon CloudFront 배포를 추가합니다. ALB 앞에 Amazon ElastiCache 인스턴스를 추가하여 API가 처리할 트래픽을 줄입니다.",
        "D": "정적 콘텐츠에 대한 Amazon CloudFront 배포를 추가합니다. Amazon Simple Queue Service(Amazon SQS) 대기열을 추가하여 나중에 EC2 인스턴스에서 처리할 수 있도록 웹 사이트에서 요청을 수신합니다."
      },
      "eng": {
        "A": "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.",
        "B": "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.",
        "C": "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce traffic for the API to handle.",
        "D": "Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances."
      }
    },
    "category": [
      "Messaging"
    ],
    "subcategory": [
      "CloudFront",
      "SQS"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/99704-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 499,
    "question": {
      "kor": "회사는 Amazon S3에 데이터를 저장합니다. 규정에 따르면 데이터에는 개인 식별 정보(PII)가 포함되어서는 안 됩니다. 이 회사는 최근 S3 버킷에 PII가 포함된 일부 개체가 있음을 발견했습니다. 회사는 S3 버킷에서 PII를 자동으로 감지하고 회사의 보안 팀에 알려야 합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company stores data in Amazon S3. According to regulations, the data must not contain personally identifiable information (PII). The company recently discovered that S3 buckets have some objects that contain PII. The company needs to automatically detect PII in S3 buckets and to notify the company’s security team.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Macie를 사용하십시오. Amazon EventBridge 규칙을 생성하여 Macie 결과에서 SensitiveData 이벤트 유형을 필터링하고 보안 팀에 Amazon Simple Notification Service(Amazon SNS) 알림을 보냅니다.",
        "B": "Amazon GuardDuty를 사용합니다. GuardDuty 결과에서 중요한 이벤트 유형을 필터링하고 보안 팀에 Amazon Simple Notification Service(Amazon SNS) 알림을 보내는 Amazon EventBridge 규칙을 생성합니다.",
        "C": "Amazon Macie를 사용합니다. Amazon EventBridge 규칙을 생성하여 Macie 결과에서 SensitiveData:S3Object/Personal 이벤트 유형을 필터링하고 보안 팀에 Amazon Simple Queue Service(Amazon SQS) 알림을 보냅니다.",
        "D": "Amazon GuardDuty를 사용합니다. GuardDuty 결과에서 중요한 이벤트 유형을 필터링하고 보안 팀에 Amazon Simple Queue Service(Amazon SQS) 알림을 보내는 Amazon EventBridge 규칙을 생성합니다."
      },
      "eng": {
        "A": "Use Amazon Macie. Create an Amazon EventBridge rule to filter the SensitiveData event type from Macie findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
        "B": "Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type from GuardDuty findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
        "C": "Use Amazon Macie. Create an Amazon EventBridge rule to filter the SensitiveData:S3Object/Personal event type from Macie findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team.",
        "D": "Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type from GuardDuty findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team."
      }
    },
    "category": [
      "Macie",
      "SNS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/111432-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 146,
    "question": {
      "kor": "회사에서 Amazon 머신 이미지(AMI)를 관리하려고 합니다. 회사는 현재 AMI가 생성된 동일한 AWS 리전에 AMI를 복사합니다. 회사는 AWS API 호출을 캡처하고 회사 계정 내에서 Amazon EC2 CreateImage API 작업이 호출될 때마다 알림을 보내는 애플리케이션을 설계해야 합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where the AMIs were created.\nThe company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 CreateImage API operation is called within the company’s account.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "AWS CloudTrail 로그를 쿼리하고 CreateImage API 호출이 감지되면 알림을 보내는 AWS Lambda 함수를 생성합니다.",
        "B": "업데이트된 로그가 Amazon S3로 전송될 때 발생하는 Amazon Simple Notification Service(Amazon SNS) 알림으로 AWS CloudTrail을 구성합니다. Amazon Athena를 사용하여 새 테이블을 생성하고 API 호출이 감지되면 CreateImage에서 쿼리합니다.",
        "C": "CreateImage API 호출에 대한 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. CreateImage API 호출이 감지되면 알림을 보내도록 대상을 Amazon Simple Notification Service(Amazon SNS) 주제로 구성합니다.",
        "D": "Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 AWS CloudTrail 로그의 대상으로 구성합니다. CreateImage API 호출이 감지되면 Amazon Simple Notification Service(Amazon SNS) 주제에 알림을 보내는 AWS Lambda 함수를 생성합니다."
      },
      "eng": {
        "A": "Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a CreateImage API call is detected.",
        "B": "Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on CreateImage when an API call is detected.",
        "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateImage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a CreateImage API call is detected.",
        "D": "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when a CreateImage API call is detected."
      }
    },
    "category": [
      "EventBridge",
      "SQS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/89086-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 284,
    "question": {
      "kor": "회사에서 계층적 구조 관계로 직원 데이터를 저장하는 애플리케이션을 만들고자 합니다. 회사는 직원 데이터에 대한 트래픽이 많은 쿼리에 대한 최소 대기 시간 응답이 필요하며 민감한 데이터를 보호해야 합니다. 회사는 또한 직원 데이터에 재무 정보가 있는 경우 월별 이메일 메시지를 받아야 합니다.\n이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 단계 조합을 수행해야 합니까? (두 가지를 선택하세요.)",
      "eng": "A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-traffic queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any financial information is present in the employee data.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "Amazon Redshift를 사용하여 직원 데이터를 계층에 저장하십시오. 매월 Amazon S3에 데이터를 언로드합니다.",
        "B": "Amazon DynamoDB를 사용하여 직원 데이터를 계층에 저장합니다. 매월 데이터를 Amazon S3로 내보냅니다.",
        "C": "AWS 계정에 대해 Amazon Macie를 구성합니다. Macie를 Amazon EventBridge와 통합하여 월별 이벤트를 AWS Lambda로 전송합니다.",
        "D": "Amazon Athena를 사용하여 Amazon S3에서 직원 데이터를 분석합니다. Athena를 Amazon QuickSight와 통합하여 분석 대시보드를 게시하고 사용자와 대시보드를 공유합니다.",
        "E": "AWS 계정에 대해 Amazon Macie를 구성합니다. Macie를 Amazon EventBridge와 통합하여 Amazon Simple Notification Service(Amazon SNS) 구독을 통해 월별 알",
        "F": "림을 보냅니다."
      },
      "eng": {
        "A": "Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.",
        "B": "Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.",
        "C": "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.",
        "D": "Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.",
        "E": "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription."
      }
    },
    "category": [
      "Database",
      "SNS"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/99940-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B",
      "E"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 38,
    "question": {
      "kor": "회사는 AWS에서 인프라를 실행하고 문서 관리 애플리케이션에 대해 700,000명의 등록된 사용자 기반을 가지고 있습니다. 회사는 대용량 .pdf 파일을 .jpg 이미지 파일로 변환하는 제품을 만들려고 합니다. .pdf 파일의 평균 크기는 5MB입니다. 회사는 원본 파일과 변환된 파일을 보관해야 합니다. 솔루션 설계자는 시간이 지남에 따라 빠르게 증가할 수요를 수용할 수 있는 확장\n가능한 솔루션을 설계해야 합니다.\n이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?",
      "eng": "A company runs its infrastructure on AWS and has a registered base of 700,000 users for its document management application. The company intends to create a product that converts large .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files. A solutions architect must design a scalable solution to accommodate demand that will grow rapidly over time.\nWhich solution meets these requirements MOST cost-effectively?"
    },
    "choices": {
      "kor": {
        "A": ".pdf 파일을 Amazon S3에 저장합니다. 파일을 .jpg 형식으로 변환하고 Amazon S3에 다시 저장하는 AWS Lambda 함수를 호출하도록 S3 PUT 이벤트를 구성합니다.",
        "B": ".pdf 파일을 Amazon Dynamo에 저장합니다DynamoDB Streams 기능을 사용하여 AWS Lambda 함수를 호출하여 파일을 .jpg 형식으로 변환하고 다시 DynamoDB에 저장합니다.",
        "C": "Amazon EC2 인스턴스, Amazon Elastic Block Store(Amazon EBS) 스토리지 및 Auto Scaling 그룹을 포함하는 AWS Elastic Beanstalk 애플리케이션에 .pdf 파일을 업로드합니다. EC2 인스턴스의 프로그램을 사용하여 파일을 .jpg 형식으로 변환합니다. .pdf 파일과 .jpg 파일을 EBS 스토어에 저장합니다.",
        "D": "Amazon EC2 인스턴스, Amazon Elastic File System(Amazon EFS) 스토리지 및 Auto Scaling 그룹이 포함된 AWS Elastic Beanstalk 애플리케이션에 .pdf 파일을 업로드합니다. EC2 인스턴스의 프로그램을 사용하여 파일을 .jpg 형식으로 변환합니다. .pdf 파일과 .jpg 파일을 EBS 스토어에 저장합니다."
      },
      "eng": {
        "A": "Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Amazon S3.",
        "B": "Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS Lambda function to convert the files to .jpg format and store them back in DynamoDB.",
        "C": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .jpg format. Save the .pdf files and the .jpg files in the EBS store.",
        "D": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the file to .jpg format. Save the .pdf files and the .jpg files in the EBS store."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Lambda",
      "S3"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85795-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 53,
    "question": {
      "kor": "소셜 미디어 회사는 사용자가 웹 사이트에 이미지를 업로드할 수 있도록 합니다. 웹 사이트는 Amazon EC2 인스턴스에서 실행됩니다. 업로드 요청 중에 웹 사이트는 이미지 크기를 표준 크기로 조정하고 크기 조정된 이미지를 Amazon S3에 저장합니다. 사용자가 웹 사이트에 대한 느린 업로드 요청을 경험하고 있습니다.\n회사는 애플리케이션 내 결합을 줄이고 웹 사이트 성능을 개선해야 합니다. 솔루션 설계자는 이미지 업로드를 위해 운영상 가장 효율적인 프로세스를 설계해야 합니다.\n이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 작업 조합을 수행해야 합니까? (두 가지를 선택하세요.)",
      "eng": "A social media company allows users to upload images to its website. The website runs on Amazon EC2 instances. During upload requests, the website resizes the images to a standard size and stores the resized images in Amazon S3. Users are experiencing slow upload requests to the website.\nThe company needs to reduce coupling within the application and improve website performance. A solutions architect must design the most operationally efficient process for image uploads.\nWhich combination of actions should the solutions architect take to meet these requirements? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "S3 Glacier에 이미지를 업로드하도록 애플리케이션을 구성합니다.",
        "B": "원본 이미지를 Amazon S3에 업로드하도록 웹 서버를 구성합니다.",
        "C": "미리 서명된 URL을 사용하여 각 사용자의 브라우저에서 Amazon S3로 직접 이미지를 업로드하도록 애플리케이션을 구성합니다.",
        "D": "이미지가 업로드될 때 AWS Lambda 함수를 호출하도록 S3 이벤트 알림을 구성합니다. 기능을 사용하여 이미지 크기를 조정하십시오.",
        "E": "업로드된 이미지의 크기를 조정하기 위해 일정에 따라 AWS Lambda 함수를 호출하는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다."
      },
      "eng": {
        "A": "Configure the application to upload images to S3 Glacier.",
        "B": "Configure the web server to upload the original images to Amazon S3.",
        "C": "Configure the application to upload images directly from each user's browser to Amazon S3 through the use of a presigned URL",
        "D": "Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the function to resize the image.",
        "E": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS Lambda function on a schedule to resize uploaded images."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Lambda",
      "S3"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/86471-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B",
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 75,
    "question": {
      "kor": "회사는 AWS에서 온라인 마켓플레이스 웹 애플리케이션을 실행합니다. 이 응용 프로그램은 피크 시간 동안 수십만 명의 사용자에게 서비스를 제공합니다. 이 회사는 수백만 건의 금융 거래 세부 정보를 다른 여러 내부 애플리케이션과 공유하기 위해 확장 가능한 실시간에 가까운 솔루션이 필요합니다. 짧은 대기 시간 검색을 위해 문서 데이터베이스에 저장되기 전에 민감한 데이터를 제거하기 위해 트랜잭션을 처리해야 합니다.\n이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 권장해야 합니까?",
      "eng": "A company runs an online marketplace web application on AWS. The application serves hundreds of thousands of users during peak hours. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval.\nWhat should a solutions architect recommend to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "트랜잭션 데이터를 Amazon DynamoDB에 저장합니다. 쓰기 시 모든 트랜잭션에서 중요한 데이터를 제거하도록 DynamoDB에서 규칙을 설정합니다. DynamoDB Streams를 사용하여 트랜잭션 데이터를 다른 애플리케이션과 공유합니다.",
        "B": "트랜잭션 데이터를 Amazon Kinesis Data Firehose로 스트리밍하여 Amazon DynamoDB 및 Amazon S3에 데이터를 저장합니다. Kinesis Data Firehose와 AWS Lambda 통합을 사용하여 민감한 데이터를 제거합니다. 다른 애플리케이션은 Amazon S3에 저장된 데이터를 사용할 수 있습니다.",
        "C": "트랜잭션 데이터를 Amazon Kinesis Data Streams로 스트리밍합니다. AWS Lambda 통합을 사용하여 모든 트랜잭션에서 민감한 데이터를 제거한 다음 트랜잭션 데이터를 Amazon DynamoDB에 저장합니다. 다른 애플리케이션은 Kinesis 데이터 스트림 외부에서 트랜잭션 데이터를 사용할 수 있습니다.",
        "D": "배치 트랜잭션 데이터를 Amazon S3에 파일로 저장합니다. Amazon S3에서 파일을 업데이트하기 전에 AWS Lambda를 사용하여 모든 파일을 처리하고 민감한 데이터를 제거하십시오. 그런 다음 Lambda 함수는 Amazon DynamoDB에 데이터를 저장합니다. 다른 애플리케이션은 Amazon S3에 저장된 트랜잭션 파일을 사용할 수 있습니다."
      },
      "eng": {
        "A": "Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon write. Use DynamoDB Streams to share the transactions data with other applications.",
        "B": "Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.",
        "C": "Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.",
        "D": "Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Kinesis",
      "Lambda"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85201-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 238,
    "question": {
      "kor": "회사는 애플리케이션에서 생성하는 대량의 스트리밍 데이터를 수집하고 처리해야 합니다. 이 애플리케이션은 Amazon EC2 인스턴스에서 실행되며 기본 설정으로 구성된 Amazon Kinesis Data Streams로 데이터를 전송합니다. 격일로 애플리케이션은 데이터를 소비하고 비즈니스 인텔리전스(BI) 처리를 위해 데이터를 Amazon S3 버킷에 기록합니다. 회사는 Amazon S3가 애플리케이션이 Kinesis Data Streams로 보내는 모든 데이터를 수신하지 못하는 것을 관찰합니다.\n솔루션 설계자는 이 문제를 해결하기 위해 무엇을 해야 합니까?",
      "eng": "A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams.\nWhat should a solutions architect do to resolve this issue?"
    },
    "choices": {
      "kor": {
        "A": "데이터 보존 기간을 수정하여 Kinesis Data Streams 기본 설정을 업데이트합니다.",
        "B": "Kinesis Producer Library(KPL)를 사용하여 Kinesis Data Streams로 데이터를 전송하도록 애플리케이션을 업데이트합니다.",
        "C": "Kinesis Data Streams로 전송되는 데이터의 처리량을 처리하도록 Kinesis 샤드 수를 업데이트합니다.",
        "D": "S3 버킷 내에서 S3 버전 관리를 켜서 S3 버킷에 수집된 모든 객체의 모든 버전을 보존합니다."
      },
      "eng": {
        "A": "Update the Kinesis Data Streams default settings by modifying the data retention period.",
        "B": "Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.",
        "C": "Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.",
        "D": "Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Kinesis"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/102175-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 135,
    "question": {
      "kor": "한 온라인 소매 회사는 5천만 명 이상의 활성 고객을 보유하고 있으며 매일 25,000건 이상의 주문을 받습니다. 회사는 고객의 구매 데이터를 수집하고 이 데이터를 Amazon S3에 저장합니다. 추가 고객 데이터는 Amazon RDS에 저장됩니다.\n회사는 팀이 분석을 수행할 수 있도록 다양한 팀에서 모든 데이터를 사용할 수 있도록 하려고 합니다. 솔루션은 데이터에 대한 세분화된 권한을 관리하는 기능을 제공하고 운영 오버헤드를 최소화해야 합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "An online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers and stores this data in Amazon S3. Additional customer data is stored in Amazon RDS.\nThe company wants to make all the data available to various teams so that the teams can perform analytics. The solution must provide the ability to manage fine-grained permissions for the data and must minimize operational overhead.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "구매 데이터를 마이그레이션하여 Amazon RDS에 직접 씁니다. RDS 액세스 제어를 사용하여 액세스를 제한하십시오.",
        "B": "Amazon RDS에서 Amazon S3로 데이터를 주기적으로 복사하도록 AWS Lambda 함수를 예약합니다. AWS Glue 크롤러를 생성합니다. Amazon Athena를 사용하여 데이터를 쿼리합니다. S3 정책을 사용하여 액세스를 제한하십시오.",
        "C": "AWS Lake Formation을 사용하여 데이터 레이크를 생성합니다. Amazon RDS에 대한 AWS Glue JDBC 연결을 생성합니다. Lake Formation에 S3 버킷을 등록합니다. Lake Formation 액세스 제어를 사용하여 액세스를 제한하십시오.",
        "D": "Amazon Redshift 클러스터를 생성합니다. Amazon S3 및 Amazon RDS에서 Amazon Redshift로 데이터를 주기적으로 복사하도록 AWS Lambda 함수를 예약합니다. Amazon Redshift 액세스 제어를 사용하여 액세스를 제한하십시오."
      },
      "eng": {
        "A": "Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.",
        "B": "Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access.",
        "C": "Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.",
        "D": "Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from Amazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit access."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Glue",
      "Lake Formation",
      "S3"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/89083-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 203,
    "question": {
      "kor": "회사에는 매시간 수백 개의 .csv 파일을 Amazon S3 버킷에 배치하는 애플리케이션이 있습니다. 파일 크기는 1GB입니다. 파일이 업로드될 때마다 회사는 파일을 Apache Parquet 형식으로 변환하고 출력 파일을 S3 버킷에 배치해야 합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": ".csv 파일을 다운로드하고 파일을 Parquet 형식으로 변환하고 출력 파일을 S3 버킷에 배치하는 AWS Lambda 함수를 생성합니다. 각 S3 PUT 이벤트에 대해 Lambda 함수를 호출 합니다.",
        "B": "Apache Spark 작업을 생성하여 .csv 파일을 읽고, 파일을 Parquet 형식으로 변환하고, 출력 파일을 S3 버킷에 배치합니다. Spark 작업을 호출하기 위해 각 S3 PUT 이벤트에 대한 AWS Lambda 함수를 생성합니다.",
        "C": "애플리케이션이 .csv 파일을 배치하는 S3 버킷에 대한 AWS Glue 테이블과 AWS Glue 크롤러를 생성합니다. Amazon Athena를 주기적으로 사용하여 AWS Glue 테이블을 쿼리하고, 쿼리 결과를 Parquet 형식으로 변환하고, 출력 파일을 S3 버킷에 배치하도록 AWS Lambda 함수를 예약합니다.",
        "D": "AWS Glue 추출, 변환 및 로드(ETL) 작업을 생성하여 .csv 파일을 Parquet 형식으로 변환하고 출력 파일을 S3 버킷에 배치합니다. 각 S3 PUT 이벤트에 대한 AWS Lambda 함수를 생성하여 ETL 작업을 호출합니다."
      },
      "eng": {
        "A": "Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.",
        "B": "Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.",
        "C": "Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.",
        "D": "Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Glue",
      "Lambda",
      "S3"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/95028-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 299,
    "question": {
      "kor": "회사는 Amazon EC2 인스턴스에서 실행되는 RESTful 웹 서비스 애플리케이션을 사용하여 수천 개의 원격 장치에서 데이터를 수집합니다. EC2 인스턴스는 원시 데이터를 수신하고 원시 데이터를 변환하며 모든 데이터를 Amazon S3 버킷에 저장합니다. 원격 장치의 수는 곧 수백만 개로 증가할 것입니다. 이 회사는 운영 오버헤드를 최소화하는 확장성이 뛰어난 솔루션이 필요합니다.\n이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 단계 조합을 수행해야 합니까? (두 가지를 선택하세요.)",
      "eng": "A company collects data from thousands of remote devices by using a RESTful web services application that runs on an Amazon EC2 instance. The EC2 instance receives the raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket. The number of remote devices will increase into the millions soon. The company needs a highly scalable solution that minimizes operational overhead.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "AWS Glue를 사용하여 Amazon S3에서 원시 데이터를 처리합니다.",
        "B": "Amazon Route 53을 사용하여 트래픽을 다른 EC2 인스턴스로 라우팅합니다.",
        "C": "들어오는 데이터의 양을 수용하기 위해 더 많은 EC2 인스턴스를 추가합니다.",
        "D": "원시 데이터를 Amazon Simple Queue Service(Amazon SQS)로 보냅니다. EC2 인스턴스를 사용하여 데이터를 처리합니다.",
        "E": "Amazon API Gateway를 사용하여 원시 데이터를 Amazon Kinesis 데이터 스트림으로 보냅니다. 데이터 스트림을 소스로 사용하여 데이터를 Amazon S3에 전달하도록 Amazon Kinesis Data Firehose를 구성합니다."
      },
      "eng": {
        "A": "Use AWS Glue to process the raw data in Amazon S3.",
        "B": "Use Amazon Route 53 to route traffic to different EC2 instances.",
        "C": "Add more EC2 instances to accommodate the increasing amount of incoming data.",
        "D": "Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to process the data.",
        "E": "Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Glue",
      "Kinesis"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/95312-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A",
      "E"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 26,
    "question": {
      "kor": "한 회사에서 사용자가 작은 파일을 Amazon S3에 업로드하는 애플리케이션을 설계하고 있습니다. 사용자가 파일을 업로드한 후 파일은 나중에 분석할 수 있도록 데이터를 변환하고 데이터를 JSON 형식으로 저장하는 일회성 간단한 처리가 필요합니다.\n각 파일은 업로드된 후 가능한 한 빨리 처리되어야 합니다. 수요는 다양할 것입니다. 어떤 날에는 사용자가 많은 수의 파일을 업로드합니다. 다른 날에는 사용자가 몇 개의 파일을 업로드하거나 파일을 전혀 업로드하지 않습니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company is designing an application where users upload small files into Amazon S3. After a user uploads a file, the file requires one-time simple processing to transform the data and save the data in JSON format for later analysis.\nEach file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users will upload a high number of files. On other days, users will upload a few files or no files.\nWhich solution meets these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "Amazon S3에서 텍스트 파일을 읽도록 Amazon EMR을 구성합니다. 처리 스크립트를 실행하여 데이터를 변환합니다. 결과 JSON 파일을 Amazon Aurora DB 클러스터에 저장합니다.",
        "B": "이벤트 알림을 Amazon Simple Queue Service(Amazon SQS) 대기열로 보내도록 Amazon S3를 구성합니다. Amazon EC2 인스턴스를 사용하여 대기열에서 읽고 데이터를 처리합니다. 결과 JSON 파일을 Amazon DynamoDB에 저장합니다.",
        "C": "이벤트 알림을 Amazon Simple Queue Service(Amazon SQS) 대기열로 보내도록 Amazon S3를 구성합니다. AWS Lambda 함수를 사용하여 대기열에서 읽고 데이터를 처리합니다. 결과 JSON 파일을 Amazon DynamoDB에 저장합니다.",
        "D": "새 파일이 업로드되면 Amazon Kinesis Data Streams로 이벤트를 보내도록 Amazon EventBridge(Amazon CloudWatch Events)를 구성합니다. AWS Lambda 함수를 사용하여 스트림에서 이벤트를 소비하고 데이터를 처리합니다. 결과 JSON 파일을 Amazon Aurora DB 클러스터에 저장합니다."
      },
      "eng": {
        "A": "Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the data. Store the resulting JSON file in an Amazon Aurora DB cluster.",
        "B": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.",
        "C": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.",
        "D": "Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new file is uploaded. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON file in an Amazon Aurora DB cluster."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "DynamoDB",
      "Lambda",
      "S3",
      "SQS",
      "event notification",
      "operational overhead"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/86676-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 71,
    "question": {
      "kor": "한 회사에서 사용자가 사진을 업로드하고 이미지에 사진 프레임을 추가할 수 있는 이미지 분석 애플리케이션을 만들었습니다. 사용자는 이미지에 추가할 사진 프레임을 나타내기 위해 이미지와 메타데이터를 업로드합니다. 애플리케이션은 단일 Amazon EC2 인스턴스와 Amazon DynamoDB를 사용하여 메타데이터를 저장합니다. 응용 프로그램이 점점 대중화되고 사용자 수가 증가하고 있습니다. 회사는 시간대와 요일에 따라 동시접속자 수가 크게 달라질 것으로 예상하고 있다. 회사는 애플리케이션이 증가하는 사용자 기반의 요구 사항을 충족하도록 확장할 수 있는지 확인해야 합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company has created an image analysis application in which users can upload photos and add photo frames to their images. The users upload images and metadata to indicate which photo frames they want to add to their images. The application uses a single Amazon EC2 instance and Amazon DynamoDB to store the metadata.\nThe application is becoming more popular, and the number of users is increasing. The company expects the number of concurrent users to vary significantly depending on the time of day and day of week. The company must ensure that the application can scale to meet the needs of the growing user base.\nWhich solution meats these requirements?"
    },
    "choices": {
      "kor": {
        "A": "AWS Lambda를 사용하여 사진을 처리하십시오. 사진과 메타데이터를 DynamoDB에 저장합니다.",
        "B": "Amazon Kinesis Data Firehose를 사용하여 사진을 처리하고 사진과 메타데이터를 저장합니다.",
        "C": "AWS Lambda를 사용하여 사진을 처리합니다. 사진을 Amazon S3에 저장합니다. 메타데이터를 저장하기 위해 DynamoDB를 보관합니다.",
        "D": "EC2 인스턴스 수를 3개로 늘립니다. 프로비저닝된 IOPS SSD(io2) Amazon Elastic Block Store(Amazon EBS) 볼륨을 사용하여 사진과 메타데이터를 저장합니다."
      },
      "eng": {
        "A": "Use AWS Lambda to process the photos. Store the photos and metadata in DynamoDB.",
        "B": "Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata.",
        "C": "Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the metadata.",
        "D": "Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumes to store the photos and metadata."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "DynamoDB",
      "Lambda",
      "S3"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85189-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 1,
    "question": {
      "kor": "한 회사에서 300개 이상의 글로벌 웹사이트와 애플리케이션을 호스팅합니다. 이 회사는 매일 이상의 클릭스트림 30TB 데이터를 분석할 플랫폼이 필요합니다.\n솔루션 설계자는 클릭 스트림 데이터를 전송하고 처리하기 위해 무엇을 해야 합니까?",
      "eng": "A company hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day.\nWhat should a solutions architect do to transmit and process the clickstream data?"
    },
    "choices": {
      "kor": {
        "A": "데이터를 Amazon S3 버킷에 보관하고 데이터로 Amazon EMR 클러스터를 실행하여 분석을 생성하도록 AWS Data Pipeline을 설계합니다.",
        "B": "Amazon EC2 인스턴스의 Auto Scaling 그룹을 생성하여 데이터를 처리하고 Amazon Redshift가 분석에 사용할 수 있도록 Amazon S3 데이터 레이크로 보냅니다.",
        "C": "데이터를 Amazon CloudFront에 캐시합니다. 데이터를 Amazon S3 버킷에 저장합니다. 객체가 S3 버킷에 추가될 때. AWS Lambda 함수를 실행하여 분석을 위해 데이터를 처리합니다.",
        "D": "Amazon Kinesis Data Streams에서 데이터를 수집합니다. Amazon Kinesis Data Firehose를 사용하여 데이터를 Amazon S3 데이터 레이크로 전송합니다. 분석을 위해 Amazon Redshift에 데이터를 로드합니다."
      },
      "eng": {
        "A": "Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster with the data to generate analytics.",
        "B": "Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 data lake for Amazon Redshift to use for analysis.",
        "C": "Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is added to the S3 bucket. run an AWS Lambda function to process the data for analysis.",
        "D": "Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Data Streaming",
      "Kinesis Data Firehose",
      "Kinesis Data Streams",
      "clickstream data"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85793-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 101,
    "question": {
      "kor": "회사의 애플리케이션은 데이터 수집을 위해 여러 SaaS(Software-as-a-Service) 소스와 통합됩니다. 회사는 Amazon EC2 인스턴스를 실행하여 데이터를 수신하고 분석을 위해 데이터를 Amazon S3 버킷에 업로드합니다. 데이터를 수신하고 업로드하는 동일한 EC2 인스턴스도 업로드가 완료되면 사용자에게 알림을 보냅니다. 회사는 느린 응용 프로그램 성능을 발견했으며 성능을 최대한 개선하고자 합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company's application integrates with multiple software-as-a-service (SaaS) sources for data collection. The company runs Amazon EC2 instances to receive the data and to upload the data to an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when an upload is complete. The company has noticed slow application performance and wants to improve the performance as much as possible.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "EC2 인스턴스가 확장될 수 있도록 Auto Scaling 그룹을 생성합니다. S3 버킷에 대한 업로드가 완료되면 Amazon Simple Notification Service(Amazon SNS) 주제로 이벤트를 보내도록 S3 이벤트 알림을 구성합니다.",
        "B": "각 SaaS 소스와 S3 버킷 간에 데이터를 전송하는 Amazon AppFlow 흐름을 생성합니다. S3 버킷에 대한 업로드가 완료되면 Amazon Simple Notification Service(Amazon SNS) 주제로 이벤트를 보내도록 S3 이벤트 알림을 구성합니다.",
        "C": "각 SaaS 소스에 대한 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성하여 출력 데이터를 보냅니다. S3 버킷을 규칙의 대상으로 구성합니다. S3 버킷에 대한 업로드가 완료되면 이벤트를 전송하는 두 번째 EventBridge(Cloud Watch Events) 규칙을 생성합니다. Amazon Simple Notification Service(Amazon SNS) 주제를 두 번째 규칙의 대상으로 구성합니다.",
        "D": "EC2 인스턴스 대신 사용할 Docker 컨테이너를 생성합니다. Amazon Elastic Container Service(Amazon ECS)에서 컨테이너화된 애플리케이션을 호스팅합니다. S3 버킷에 대한 업로드가 완료되면 Amazon Simple Notification Service(Amazon SNS) 주제로 이벤트를 보내도록 Amazon CloudWatch Container Insights를 구성합니다."
      },
      "eng": {
        "A": "Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.",
        "B": "Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.",
        "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to send output data. Configure the S3 bucket as the rule's target. Create a second EventBridge (Cloud Watch Events) rule to send events when the upload to the S3 bucket is complete. Configure an Amazon Simple Notification Service (Amazon SNS) topic as the second rule's target.",
        "D": "Create a Docker container to use instead of an EC2 instance. Host the containerized application on Amazon Elastic Container Service (Amazon ECS). Configure Amazon CloudWatch Container Insights to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "AppFlow",
      "S3",
      "SNS",
      "SaaS"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85446-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 35,
    "question": {
      "kor": "회사는 애플리케이션에 대한 실시간 데이터 수집 아키텍처를 구성해야 합니다. 회사는 API, 데이터가 스트리밍될 때 데이터를 변환하는 프로세스, 데이터를 위한 스토리지 솔루션이 필요합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is streamed, and a storage solution for the data.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis 데이터 스트림으로 데이터를 전송하는 API를 호스팅하기 위해 Amazon EC2 인스턴스를 배포합니다. Kinesis 데이터 스트림을 데이터 소스로 사용하는 Amazon Kinesis Data Firehose 전송 스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. Kinesis Data Firehose 전송 스트림을 사용하여 데이터를 Amazon S3로 보냅니다.",
        "B": "AWS Glue로 데이터를 전송하는 API를 호스팅하기 위해 Amazon EC2 인스턴스를 배포합니다. EC2 인스턴스에서 소스/대상 확인을 중지합니다. AWS Glue를 사용하여 데이터를 변환하고 데이터를 Amazon S3로 보냅니다.",
        "C": "Amazon Kinesis 데이터 스트림으로 데이터를 전송하도록 Amazon API Gateway API를 구성합니다. Kinesis 데이터 스트림을 데이터 소스로 사용하는 Amazon Kinesis Data Firehose 전송 스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. Kinesis Data Firehose 전송 스트림을 사용하여 데이터를 Amazon S3로 보냅니다.",
        "D": "AWS Glue로 데이터를 전송하도록 Amazon API Gateway API를 구성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. AWS Glue를 사용하여 데이터를 Amazon S3로 보냅니다."
      },
      "eng": {
        "A": "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.",
        "B": "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination checking on the EC2 instance. Use AWS Glue to transform the data and to send the data to Amazon S3.",
        "C": "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.",
        "D": "Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform the data. Use AWS Glue to send the data to Amazon S3."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "API Gateway",
      "Data Ingestion",
      "Data Streaming",
      "Kinesis",
      "Lambda"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85740-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 337,
    "question": {
      "kor": "회사에서 Amazon EC2 인스턴스 플릿을 사용하여 온프레미스 데이터 소스에서 데이터를 수집하고 있습니다. 데이터는 JSON 형식이며 수집 속도는 최대 1MB/s입니다. EC2 인스턴스가 재부팅되면 진행 중인 데이터가 손실됩니다. 회사의 데이터 과학 팀은 거의 실시간으로 수집된 데이터를 쿼리하려고 합니다.\n데이터 손실을 최소화하면서 확장 가능한 거의 실시간 데이터 쿼리를 제공하는 솔루션은 무엇입니까?",
      "eng": "A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-flight is lost. The company’s data science team wants to query ingested data in near-real time.\nWhich solution provides near-real-time data querying that is scalable with minimal data loss?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis Data Streams에 데이터를 게시하고 Kinesis Data Analytics를 사용하여 데이터를 쿼리합니다.",
        "B": "Amazon Redshift를 대상으로 사용하여 Amazon Kinesis Data Firehose에 데이터를 게시합니다. Amazon Redshift를 사용하여 데이터를 쿼리합니다.",
        "C": "수집된 데이터를 EC2 인스턴스 스토어에 저장합니다. Amazon S3를 대상으로 Amazon Kinesis Data Firehose에 데이터를 게시합니다. Amazon Athena를 사용하여 데이터를 쿼리합니다.",
        "D": "수집된 데이터를 Amazon Elastic Block Store(Amazon EBS) 볼륨에 저장합니다. Redis용 Amazon ElastiCache에 데이터를 게시합니다. Redis 채널을 구독하여 데이터를 쿼리합니다."
      },
      "eng": {
        "A": "Publish data to Amazon Kinesis Data Streams. Use Kinesis Data Analytics to query the data.",
        "B": "Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.",
        "C": "Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.",
        "D": "Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/99752-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 695,
    "question": {
      "kor": "회사에는 고객이 Amazon S3 버킷에 이미지를 업로드하는 데 사용하는 애플리케이션이 있습니다. 매일 밤 회사는 그날 받은 모든 이미지를 처리하는 Amazon EC2 스팟 집합을 시작합니다. 각 이미지를 처리하는 데는 2분이 걸리며 512MB의 메모리가 필요합니다.\n솔루션 설계자는 이미지가 업로드될 때 이미지를 처리하도록 애플리케이션을 변경해야 합니다.\n이러한 요구 사항을 가장 비용 효율적으로 충족하는 변경 사항은 무엇입니까?",
      "eng": "A company has an application that customers use to upload images to an Amazon S3 bucket. Each night, the company launches an Amazon EC2 Spot Fleet that processes all the images that the company received that day. The processing for each image takes 2 minutes and requires 512 MB of memory.\nA solutions architect needs to change the application to process the images when the images are uploaded.\nWhich change will meet these requirements MOST cost-effectively?"
    },
    "choices": {
      "kor": {
        "A": "S3 이벤트 알림을 사용하여 Amazon Simple Queue Service(Amazon SQS) 대기열에 이미지 세부 정보가 포함된 메시지를 씁니다. 대기열에서 메시지를 읽고 이미지를 처리하도록 AWS Lambda 함수를 구성합니다.",
        "B": "S3 이벤트 알림을 사용하여 Amazon Simple Queue Service(Amazon SQS) 대기열에 이미지 세부 정보가 포함된 메시지를 씁니다. 대기열에서 메시지를 읽고 이미지를 처리하도록 EC2 예약 인스턴스를 구성합니다.",
        "C": "S3 이벤트 알림을 사용하여 이미지 세부 정보가 포함된 메시지를 Amazon SNS(Amazon SNS) 주제에 게시합니다. 주제를 구독하고 이미지를 처리하도록 Amazon Elastic Container Service(Amazon ECS)에서 컨테이너 인스턴스를 구성합니다.",
        "D": "S3 이벤트 알림을 사용하여 이미지 세부 정보가 포함된 메시지를 Amazon SNS(Amazon SNS) 주제에 게시합니다. 주제를 구독하고 이미지를 처리하도록 AWS Elastic Beanstalk 애플리케이션을 구성합니다."
      },
      "eng": {
        "A": "Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue Service (Amazon SQS) queue. Configure an AWS Lambda function to read the messages from the queue and to process the images.",
        "B": "Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue Service (Amazon SQS) queue. Configure an EC2 Reserved Instance to read the messages from the queue and to process the images.",
        "C": "Use S3 Event Notifications to publish a message with image details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure a container instance in Amazon Elastic Container Service (Amazon ECS) to subscribe to the topic and to process the images.",
        "D": "Use S3 Event Notifications to publish a message with image details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Elastic Beanstalk application to subscribe to the topic and to process the images."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Lambda",
      "S3",
      "S3 Event Notifications",
      "SQS"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/139858-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 383,
    "question": {
      "kor": "한 회사가 여러 소스에서 실시간 스트리밍 데이터를 수집할 새로운 데이터 플랫폼을 준비하고 있습니다. 회사는 Amazon S3에 데이터를 쓰기 전에 데이터를 변환해야 합니다. 회사는 SQL을 사용하여 변환된 데이터를 쿼리할 수 있는 기능이 필요합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까? (두 가지를 선택하세요.)",
      "eng": "A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data.\nWhich solutions will meet these requirements? (Choose two.)"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis Data Streams를 사용하여 데이터를 스트리밍합니다. Amazon Kinesis Data Analytics를 사용하여 데이터를 변환합니다. Amazon Kinesis Data Firehose를 사용하여 Amazon S3에 데이터를 씁니다. Amazon Athena를 사용하여 Amazon S3에서 변환된 데이터를 쿼리합니다.",
        "B": "Amazon Managed Streaming for Apache Kafka(Amazon MSK)를 사용하여 데이터를 스트리밍합니다. AWS Glue를 사용하여 데이터를 변환하고 데이터를 Amazon S3에 씁니다. Amazon Athena를 사용하여 Amazon S3에서 변환된 데이터를 쿼리합니다.",
        "C": "AWS Database Migration Service(AWS DMS)를 사용하여 데이터를 수집합니다. Amazon EMR을 사용하여 데이터를 변환하고 Amazon S3에 데이터를 씁니다. Amazon Athena를 사용하여 Amazon S3에서 변환된 데이터를 쿼리합니다.",
        "D": "Amazon Managed Streaming for Apache Kafka(Amazon MSK)를 사용하여 데이터를 스트리밍합니다. Amazon Kinesis Data Analytics를 사용하여 데이터를 변환하고 데이터를 Amazon S3에 씁니다. Amazon RDS 쿼리 편집기를 사용하여 Amazon S3에서 변환된 데이터를 쿼리합니다.",
        "E": "Amazon Kinesis Data Streams를 사용하여 데이터를 스트리밍합니다. AWS Glue를 사용하여 데이터를 변환합니다. Amazon Kinesis Data Firehose를 사용하여 Amazon S3에 데이터를 씁니다. Amazon RDS 쿼리 편집기를 사용하여 Amazon S3에서 변환된 데이터를 쿼리합니다."
      },
      "eng": {
        "A": "Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
        "B": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
        "C": "Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
        "D": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.",
        "E": "Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Kinesis",
      "MSK"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/99834-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A",
      "B"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 428,
    "question": {
      "kor": "회사는 서로 다른 위치에 데이터 수집 센서를 가지고 있습니다. 데이터 수집 센서는 대량의 데이터를 회사로 스트리밍합니다. 이 회사는 대용량 스트리밍 데이터를 수집하고 처리하기 위해\nAWS에서 플랫폼을 설계하려고 합니다. 솔루션은 확장 가능해야 하며 거의 실시간으로 데이터 수집을 지원해야 합니다. 회사는 향후 보고를 위해 데이터를 Amazon S3에 저장해야 합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company has data collection sensors at different locations. The data collection sensors stream a high volume of data to the company. The company wants to design a platform on AWS to ingest and process high-volume streaming data. The solution must be scalable and support data collection in near real time.\nThe company must store the data in Amazon S3 for future reporting.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis Data Firehose를 사용하여 스트리밍 데이터를 Amazon S3에 전달합니다.",
        "B": "AWS Glue를 사용하여 스트리밍 데이터를 Amazon S3에 전달합니다.",
        "C": "AWS Lambda를 사용하여 스트리밍 데이터를 전달하고 데이터를 Amazon S3에 저장합니다.",
        "D": "AWS DMS(AWS Database Migration Service)를 사용하여 스트리밍 데이터를 Amazon S3에 전달합니다."
      },
      "eng": {
        "A": "Use Amazon Kinesis Data Firehose to deliver streaming data to Amazon S3.",
        "B": "Use AWS Glue to deliver streaming data to Amazon S3.",
        "C": "Use AWS Lambda to deliver streaming data and store the data to Amazon S3.",
        "D": "Use AWS Database Migration Service (AWS DMS) to deliver streaming data to Amazon S3."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Kinesis",
      "near real time"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/116976-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 429,
    "question": {
      "kor": "회사에 모바일 앱을 사용하는 백만 명의 사용자가 있습니다. 회사는 거의 실시간으로 데이터 사용량을 분석해야 합니다. 회사는 또한 거의 실시간으로 데이터를 암호화하고 추가 처리를 위해 데이터를 Apache Parquet 형식의 중앙 위치에 저장해야 합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis 데이터 스트림을 생성하여 Amazon S3에 데이터를 저장합니다. 데이터를 분석할 Amazon Kinesis Data Analytics 애플리케이션을 생성합니다. AWS Lambda 함수를 호출하여 데이터를 Kinesis Data Analytics 애플리케이션으로 보냅니다.",
        "B": "Amazon Kinesis 데이터 스트림을 생성하여 Amazon S3에 데이터를 저장합니다. 데이터를 분석할 Amazon EMR 클러스터를 생성합니다. AWS Lambda 함수를 호출하여 데이터를 EMR 클러스터로 보냅니다.",
        "C": "Amazon Kinesis Data Firehose 전송 스트림을 생성하여 Amazon S3에 데이터를 저장합니다. 데이터를 분석할 Amazon EMR 클러스터를 생성합니다.",
        "D": "Amazon Kinesis Data Firehose 전송 스트림을 생성하여 Amazon S3에 데이터를 저장합니다. 데이터를 분석할 Amazon Kinesis Data Analytics 애플리케이션을 생성합니다."
      },
      "eng": {
        "A": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWS Lambda function to send the data to the Kinesis Data Analytics application.",
        "B": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.",
        "C": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.",
        "D": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Kinesis",
      "near real time analysis"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/95347-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "D"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 555,
    "question": {
      "kor": "미디어 회사는 온프레미스에서 사용자 활동 데이터를 수집하고 분석합니다. 회사는 이 기능을 AWS로 마이그레이션하려고 합니다. 사용자 활동 데이터 저장소는 계속해서 성장하여 크기가 페타바이트가 될 것입니다. 회사는 SQL을 사용하여 기존 데이터 및 새 데이터의 온디맨드 분석을 용이하게 하는 고가용성 데이터 수집 솔루션을 구축해야 합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to grow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new data with SQL.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "활동 데이터를 Amazon Kinesis 데이터 스트림으로 보냅니다. 데이터를 Amazon S3 버킷으로 전달하도록 스트림을 구성합니다.",
        "B": "활동 데이터를 Amazon Kinesis Data Firehose 전송 스트림으로 보냅니다. 데이터를 Amazon Redshift 클러스터로 전달하도록 스트림을 구성합니다.",
        "C": "활동 데이터를 Amazon S3 버킷에 배치합니다. 데이터가 S3 버킷에 도착하면 데이터에서 AWS Lambda 함수를 실행하도록 Amazon S3를 구성합니다.",
        "D": "여러 가용 영역에 분산된 Amazon EC2 인스턴스에서 수집 서비스를 생성합니다. 데이터를 Amazon RDS 다중 AZ 데이터베이스로 전달하도록 서비스를 구성합니다."
      },
      "eng": {
        "A": "Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon S3 bucket.",
        "B": "Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.",
        "C": "Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as the data arrives in the S3 bucket.",
        "D": "Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones. Configure the service to forward data to an Amazon RDS Multi-AZ database."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Kinesis",
      "Redshift"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/94985-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "B"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 596,
    "question": {
      "kor": "회사의 애플리케이션은 여러 가용 영역에 있는 Amazon EC2 인스턴스에서 실행됩니다. 애플리케이션은 타사 애플리케이션에서 실시간 데이터를 수집해야 합니다.\n회사에는 수집된 원시 데이터를 Amazon S3 버킷에 배치하는 데이터 수집 솔루션이 필요합니다.\n어떤 솔루션이 이러한 요구 사항을 충족합니까?",
      "eng": "A company's application runs on Amazon EC2 instances that are in multiple Availability Zones. The application needs to ingest real-time data from third-party applications.\nThe company needs a data ingestion solution that places the ingested raw data in an Amazon S3 bucket.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "데이터 수집을 위해 Amazon Kinesis 데이터 스트림을 생성합니다. Kinesis 데이터 스트림을 사용하기 위해 Amazon Kinesis Data Firehose 전송 스트림을 생성합니다. S3 버킷을 전송 스트림의 대상으로 지정합니다.",
        "B": "AWS Database Migration Service(AWS DMS)에서 데이터베이스 마이그레이션 작업을 생성합니다. EC2 인스턴스의 복제 인스턴스를 소스 엔드포인트로 지정합니다. S3 버킷을 대상 엔드포인트로 지정합니다. 기존 데이터를 마이그레이션하고 지속적인 변경 사항을 복제하도록 마이그레이션 유형을 설정합니다.",
        "C": "EC2 인스턴스에서 AWS DataSync 에이전트를 생성하고 구성합니다. EC2 인스턴스에서 S3 버킷으로 데이터를 전송하도록 DataSync 작업을 구성합니다.",
        "D": "데이터 수집을 위해 애플리케이션에 대한 AWS Direct Connect 연결을 생성합니다. Amazon Kinesis Data Firehose 전송 스트림을 생성하여 애플리케이션에서 직접 PUT 작업을 사용합니다. S3 버킷을 전송 스트림의 대상으로 지정합니다."
      },
      "eng": {
        "A": "Create Amazon Kinesis data streams for data ingestion. Create Amazon Kinesis Data Firehose delivery streams to consume the Kinesis data streams. Specify the S3 bucket as the destination of the delivery streams.",
        "B": "Create database migration tasks in AWS Database Migration Service (AWS DMS). Specify replication instances of the EC2 instances as the source endpoints. Specify the S3 bucket as the target endpoint. Set the migration type to migrate existing data and replicate ongoing changes.",
        "C": "Create and configure AWS DataSync agents on the EC2 instances. Configure DataSync tasks to transfer data from the EC2 instances to the S3 bucket.",
        "D": "Create an AWS Direct Connect connection to the application for data ingestion. Create Amazon Kinesis Data Firehose delivery streams to consume direct PUT operations from the application. Specify the S3 bucket as the destination of the delivery streams."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Kinesis"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/135270-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 811,
    "question": {
      "kor": "회사는 고객 결제 데이터를 Amazon S3의 회사 데이터 레이크로 수집하려고 합니다. 회사는 평균적으로 1분마다 결제 데이터를 수신합니다. 회사는 결제 데이터를 실시간으로 분석하기를 원 합니다. 그런 다음 회사는 데이터를 데이터 레이크로 수집하려고 합니다.\n이러한 요구 사항을 가장 효율적으로 충족하는 솔루션은 무엇입니까?",
      "eng": "A company wants to ingest customer payment data into the company's data lake in Amazon S3. The company receives payment data every minute on average. The company wants to analyze the payment data in real time. Then the company wants to ingest the data into the data lake.\nWhich solution will meet these requirements with the MOST operational efficiency?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis Data Streams를 사용하여 데이터를 수집하십시오. AWS Lambda를 사용하여 실시간으로 데이터를 분석합니다.",
        "B": "AWS Glue를 사용하여 데이터를 수집합니다. Amazon Kinesis Data Analytics를 사용하여 데이터를 실시간으로 분석하십시오.",
        "C": "Amazon Kinesis Data Firehose를 사용하여 데이터를 수집합니다. Amazon Kinesis Data Analytics를 사용하여 데이터를 실시간으로 분석하십시오.",
        "D": "Amazon API Gateway를 사용하여 데이터를 수집합니다. AWS Lambda를 사용하여 실시간으로 데이터를 분석합니다."
      },
      "eng": {
        "A": "Use Amazon Kinesis Data Streams to ingest data. Use AWS Lambda to analyze the data in real time.",
        "B": "Use AWS Glue to ingest data. Use Amazon Kinesis Data Analytics to analyze the data in real time.",
        "C": "Use Amazon Kinesis Data Firehose to ingest data. Use Amazon Kinesis Data Analytics to analyze the data in real time.",
        "D": "Use Amazon API Gateway to ingest data. Use AWS Lambda to analyze the data in real time."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Kinesis"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/109421-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 845,
    "question": {
      "kor": "회사에는 제3자 공급업체로부터 거의 실시간으로 데이터를 수신할 수 있는 REST 기반 인터페이스가 있는 애플리케이션이 있습니다. 일단 수신되면 애플리케이션은 추가 분석을 위해 데이터를 처리하고 저장합니다. 애플리케이션이 Amazon EC2 인스턴스에서 실행 중입니다.\n타사 공급업체에서 애플리케이션에 데이터를 보낼 때 503 서비스를 사용할 수 없음 오류가 많이 발생했습니다. 데이터 볼륨이 급증하면 컴퓨팅 용량이 최대 한도에 도달하고 애플리케이션이\n모든 요청을 처리할 수 없게 됩니다.\n보다 확장 가능한 솔루션을 제공하기 위해 솔루션 설계자는 어떤 디자인을 권장해야 합니까?",
      "eng": "A company has an application with a REST-based interface that allows data to be received in near-real time from a third-party vendor. Once received, the application processes and stores the data for further analysis. The application is running on Amazon EC2 instances.\nThe third-party vendor has received many 503 Service Unavailable Errors when sending data to the application. When the data volume spikes, the compute capacity reaches its maximum limit and the application is unable to process all requests.\nWhich design should a solutions architect recommend to provide a more scalable solution?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis Data Streams를 사용하여 데이터를 수집하십시오. AWS Lambda 함수를 사용하여 데이터를 처리합니다.",
        "B": "기존 애플리케이션 위에 Amazon API Gateway를 사용하십시오. 타사 공급업체에 대한 할당량 제한이 있는 사용량 계획을 만듭니다.",
        "C": "Amazon Simple 알림 서비스(Amazon SNS)를 사용하여 데이터를 수집합니다. Application Load Balancer 뒤의 Auto Scaling 그룹에 EC2 인스턴스를 배치합니다.",
        "D": "애플리케이션을 컨테이너로 다시 패키징합니다. Auto Scaling 그룹과 함께 EC2 시작 유형을 사용하는 Amazon Elastic Container Service(Amazon ECS)를 사용하여 애플리케이션을 배포합니다."
      },
      "eng": {
        "A": "Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.",
        "B": "Use Amazon API Gateway on top of the existing application. Create a usage plan with a quota limit for the third-party vendor.",
        "C": "Use Amazon Simple Notification Service (Amazon SNS) to ingest the data. Put the EC2 instances in an Auto Scaling group behind an Application Load Balancer.",
        "D": "Repackage the application as a container. Deploy the application using Amazon Elastic Container Service (Amazon ECS) using the EC2 launch type with an Auto Scaling group."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Kinesis"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/121218-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 966,
    "question": {
      "kor": "전자상거래 회사는 실시간 분석을 위해 회사 웹사이트에서 사용자 클릭스트림 데이터를 수집하려고 합니다. 웹사이트는 하루 종일 변동하는 트래픽 패턴을 경험합니다. 회사에는 다양한 수준의 트래픽에 적응할 수 있는 확장 가능한 솔루션이 필요합니다.\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "An ecommerce company wants to collect user clickstream data from the company's website for real-time analysis. The website experiences fluctuating traffic patterns throughout the day. The company needs a scalable solution that can adapt to varying levels of traffic.\nWhich solution will meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "온디맨드 모드에서 Amazon Kinesis Data Streams의 데이터 스트림을 사용하여 클릭스트림 데이터를 캡처합니다. AWS Lambda를 사용하여 실시간으로 데이터를 처리합니다.",
        "B": "Amazon Kinesis Data Firehose를 사용하여 클릭스트림 데이터를 캡처합니다. AWS Glue를 사용하여 실시간으로 데이터를 처리합니다.",
        "C": "Amazon Kinesis Video Streams를 사용하여 클릭스트림 데이터를 캡처합니다. AWS Glue를 사용하여 실시간으로 데이터를 처리합니다.",
        "D": "Apache Flink용 Amazon Managed Service(이전의 Amazon Kinesis Data Analytics)를 사용하여 클릭스트림 데이터를 캡처합니다. AWS Lambda를 사용하여 실시간으로",
        "E": "데이터를 처리합니다."
      },
      "eng": {
        "A": "Use a data stream in Amazon Kinesis Data Streams in on-demand mode to capture the clickstream data. Use AWS Lambda to process the data in real time.",
        "B": "Use Amazon Kinesis Data Firehose to capture the clickstream data. Use AWS Glue to process the data in real time.",
        "C": "Use Amazon Kinesis Video Streams to capture the clickstream data. Use AWS Glue to process the data in real time.",
        "D": "Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to capture the clickstream data. Use AWS Lambda to process the data in real time."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "Kinesis"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/139803-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 386,
    "question": {
      "kor": "회사는 계정과 간에 데이터를 안전하게 교환하고자 Salesforce Amazon S3 합니다. 회사는 AWS Key Management Service (AWS KMS) 고객 관리 키 (CMK)를 사용하여 데이터를 휴식 상태에서 암호화해야 합니다. 또한 데이터를 전송 중에도 암호화해야 합니다. 회사는 Salesforce 계정에 대한 API 액세스를 활성화했습니다.\n가장 적은 개발 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company wants to securely exchange data between its Software as a Service (SaaS) application, Salesforce account, and Amazon S3. The company must encrypt the data at rest using AWS Key Management Service (AWS KMS) customer-managed keys (CMKs) and also encrypt the data in transit. The company has enabled API access for the Salesforce account.\nWhich solution will meet these requirements with the LEAST development effort?"
    },
    "choices": {
      "kor": {
        "A": "Salesforce에서 Amazon S3로 데이터를 안전하게 전송하기 위해 AWS Lambda 함수를 생성합니다.",
        "B": "AWS Step Functions 워크플로우를 생성합니다. Salesforce에서 Amazon S3로 데이터를 안전하게 전송하는 작업을 정의합니다.",
        "C": "Amazon AppFlow 플로우를 생성합니다. Salesforce에서 Amazon S3로 데이터를 안전하게 전송합니다.",
        "D": "Salesforce에서 Amazon S3로 데이터를 안전하게 전송하기 위해 사용자 정의 커넥터를 생성합니다."
      },
      "eng": {
        "A": "Create AWS Lambda functions to transfer the data securely from Salesforce to Amazon S3.",
        "B": "Create an AWS Step Functions workflow. Define the task to transfer the data securely from Salesforce to Amazon S3.",
        "C": "Create Amazon AppFlow flows to transfer the data securely from Salesforce to Amazon S3.",
        "D": "Create a custom connector for Salesforce to transfer the data securely from Salesforce to Amazon S3."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "AppFlow",
      "S3",
      "SaaS"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/109525-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 488,
    "question": {
      "kor": "회사는 애플리케이션에 대한 실시간 데이터 수집 아키텍처를 구성해야 합니다. 회사는 API, 데이터가 스트리밍될 때 데이터를 변환하는 프로세스, 데이터를 위한 스토리지 솔루션이 필요합니다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
      "eng": "A company needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is streamed, and a storage solution for the data.\nWhich solution will meet these requirements with the LEAST operational overhead?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis 데이터 스트림으로 데이터를 전송하는 API를 호스팅하기 위해 Amazon EC2 인스턴스를 배포합니다. Kinesis 데이터 스트림을 데이터 소스로 사용하는 Amazon Kinesis Data Firehose 전송 스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. Kinesis Data Firehose 전송 스트림을 사용하여 데이터를 Amazon S3로 보냅니다.",
        "B": "AWS Glue로 데이터를 전송하는 API를 호스팅하기 위해 Amazon EC2 인스턴스를 배포합니다. EC2 인스턴스에서 소스/대상 확인을 중지합니다. AWS Glue를 사용하여 데이터를 변환하고 데이터를 Amazon S3로 보냅니다.",
        "C": "Amazon Kinesis 데이터 스트림으로 데이터를 전송하도록 Amazon API Gateway API를 구성합니다. Kinesis 데이터 스트림을 데이터 소스로 사용하는 Amazon Kinesis Data Firehose 전송 스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. Kinesis Data Firehose 전송 스트림을 사용하여 데이터를 Amazon S3로 보냅니다.",
        "D": "AWS Glue로 데이터를 전송하도록 Amazon API Gateway API를 구성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. AWS Glue를 사용하여 데이터를 Amazon S3로 보냅니다."
      },
      "eng": {
        "A": "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.",
        "B": "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination checking on the EC2 instance. Use AWS Glue to transform the data and to send the data to Amazon S3.",
        "C": "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.",
        "D": "Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform the data. Use AWS Glue to send the data to Amazon S3."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [
      "API Gateway",
      "Kinesis",
      "Lambda",
      "S3"
    ],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/85740-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "C"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  },
  {
    "idx": 620,
    "question": {
      "kor": "한 회사에서 점수 업데이트를 백엔드 프로세서로 스트리밍한 다음 결과를 리더보드에 게시하는 모바일 게임을 개발하고 있습니다. 솔루션 설계자는 대규모 트래픽 급증을 처리하고, 모바일 게임 업데이트를 수신 순서대로 처리하고, 처리된 업데이트를 고가용성 데이터베이스에 저장할 수 있는 솔루션을 설계해야 합니다. 또한 회사는 솔루션을 유지하는 데 필요한 관리 오버헤드를 최소화하려고 합니다.\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?",
      "eng": "A company is developing a mobile game that streams score updates to a backend processor and then posts results on a leaderboard. A solutions architect needs to design a solution that can handle large traffic spikes, process the mobile game updates in order of receipt, and store the processed updates in a highly available database. The company also wants to minimize the management overhead required to maintain the solution.\nWhat should the solutions architect do to meet these requirements?"
    },
    "choices": {
      "kor": {
        "A": "Amazon Kinesis Data Streams에 점수 업데이트를 푸시합니다. AWS Lambda를 사용하여 Kinesis Data Streams의 업데이트를 처리합니다. 처리된 업데이트를 Amazon DynamoDB에 저장합니다.",
        "B": "Amazon Kinesis Data Streams에 점수 업데이트를 푸시합니다. Auto Scaling용으로 설정된 Amazon EC2 인스턴스 집합으로 업데이트를 처리합니다. 처리된 업데이트를 Amazon Redshift에 저장합니다.",
        "C": "Amazon Simple 알림 서비스(Amazon SNS) 주제에 점수 업데이트를 푸시합니다. 업데이트를 처리하려면 SNS 주제에 대한 AWS Lambda 함수를 구독하세요. Amazon EC2에서 실행되는 SQL 데이터베이스에 처리된 업데이트를 저장합니다.",
        "D": "Amazon Simple Queue Service(Amazon SQS) 대기열에 점수 업데이트를 푸시합니다. Auto Scaling이 포함된 Amazon EC2 인스턴스 집합을 사용하여 SQS 대기열의 업데이트를 처리합니다. 처리된 업데이트를 Amazon RDS 다중 AZ DB 인스턴스에 저장합니다."
      },
      "eng": {
        "A": "Push score updates to Amazon Kinesis Data Streams. Process the updates in Kinesis Data Streams with AWS Lambda. Store the processed updates in Amazon DynamoDB.",
        "B": "Push score updates to Amazon Kinesis Data Streams. Process the updates with a fleet of Amazon EC2 instances set up for Auto Scaling. Store the processed updates in Amazon Redshift.",
        "C": "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to process the updates. Store the processed updates in a SQL database running on Amazon EC2.",
        "D": "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue. Use a fleet of Amazon EC2 instances with Auto Scaling to process the updates in the SQS queue. Store the processed updates in an Amazon RDS Multi-AZ DB instance."
      }
    },
    "category": [
      "Data Processing"
    ],
    "subcategory": [],
    "rote_memorization": false,
    "reference": "https://www.examtopics.com/discussions/amazon/view/132922-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "answer": [
      "A"
    ],
    "correct": 0,
    "incorrect": 0,
    "correct_students": []
  }
]